<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>GazeEstimationfromMultimodalKinectData</title>
      <link href="/2022/01/16/GazeEstimationfromMultimodalKinectData/"/>
      <url>/2022/01/16/GazeEstimationfromMultimodalKinectData/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 视线估计 </category>
          
          <category> 试验记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文献阅读 </tag>
            
            <tag> 试验记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data</title>
      <link href="/2022/01/16/Data/"/>
      <url>/2022/01/16/Data/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DailtedNet</title>
      <link href="/2022/01/16/DailtedNet/"/>
      <url>/2022/01/16/DailtedNet/</url>
      
        <content type="html"><![CDATA[<h1 id="Dilated-convolution"><a href="#Dilated-convolution" class="headerlink" title="Dilated convolution"></a>Dilated convolution</h1><p><img src="/2022/01/16/DailtedNet/dilated_conv.png" alt="image-20220111115104315"></p><p>在语义分割中使用分类网络作为backbone，之后会将图片进行一系列的下采样。再通过上采样还原回原来的大小。一般分类网络会下采样为原来的32倍。如果下采样倍率太大，对还原回原图有影响。比如vgg，通过最大池化进行下采样，会丢失一些细节信息和小目标，后续无法通过上采样进行还原，会导致语义分割效果不理想。</p><p>如果去掉最大池化下采样，特征图对于原图的感受野变小了。</p><p><img src="/2022/01/16/DailtedNet/grid_effect.png" alt="image-20220111190102930"></p><p>在上图中使用膨胀卷积，会有一些像素用不上。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gpu并行训练</title>
      <link href="/2022/01/12/gpu%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/"/>
      <url>/2022/01/12/gpu%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/</url>
      
        <content type="html"><![CDATA[<h1 id="nn-DataParallel"><a href="#nn-DataParallel" class="headerlink" title="nn.DataParallel"></a>nn.DataParallel</h1><p>若只考虑单机多卡并行和数据平行训练时，最简单的方法是用 <strong>nn.DataParallel</strong>。</p><p>先设置所使用的GPU卡，然后用 <strong>nn.DataParallel</strong> 包装模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] =<span class="string">&quot;0, 2&quot;</span><span class="comment"># 使用0和2号卡</span></span><br><span class="line">model=Classifier()</span><br><span class="line">model =nn.DataParallel(model)</span><br></pre></td></tr></table></figure><p><a href="https://zhuanlan.zhihu.com/p/102697821">Pytorch的nn.DataParallel - 知乎 (zhihu.com)</a></p><p>前向过程：</p><ol><li>分发mini-batch到每个GPU上，</li><li>将模型复制到每一个GPU上，</li><li>模型对mini-batch进行forward计算结果，</li><li>将每个GPU上的计算结果汇总到第一个GPU上。</li></ol><p>反向传播过程：</p><ol><li>在第一个GPU上根据计算的loss计算梯度，</li><li>将第一个GPU上计算的梯度值分发到每个GPU上，</li><li>每个GPU上进行梯度更新，</li><li>将每个GPU上更新的梯度值汇总到第一个GPU上。<br>其中foward过程都会把第一个GPU上的模型重新分发到每个每个GPU上。各个卡只计算到loss，然后0号卡做loss平均，最后分发到各个卡上求梯度并进行参数更新。</li></ol><p>文章有错误：应该是<br>1：把每张卡的输出gather到GPU0，<br>2：只在GPU0在算loss，<br>3：把loss scatter到其他GPU上，<br>4：每个GPU算各自的梯度，再把梯度汇总到GPU0上</p><blockquote><p>关于这个流程不同的文章有不同的说法。</p></blockquote><h1 id="nn-parallel-DistributedDataParallel"><a href="#nn-parallel-DistributedDataParallel" class="headerlink" title="nn.parallel.DistributedDataParallel"></a><strong>nn.parallel.DistributedDataParallel</strong></h1><p><a href="https://zhuanlan.zhihu.com/p/361314953">PyTorch 源码解读之分布式训练了解一下？ - 知乎 (zhihu.com)</a></p><p><img src="/2022/01/12/gpu%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220112162455330.png" alt="image-20220112162455330"></p><p><a href="https://www.bilibili.com/video/BV1yt4y1e7sZ?from=search&amp;seid=5934700972016238189&amp;spm_id_from=333.337.0.0">pytorch多GPU并行训练教程_哔哩哔哩_bilibili</a></p><p><img src="/2022/01/12/gpu%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220112162623305.png" alt="image-20220112162623305"></p><p><img src="/2022/01/12/gpu%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220112163216347.png" alt="image-20220112163216347"></p><p><img src="/2022/01/12/gpu%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220112163334093.png" alt="image-20220112163334093"></p><p>同步bn</p><h1 id="nn-DataParallel-1"><a href="#nn-DataParallel-1" class="headerlink" title="nn.DataParallel"></a>nn.DataParallel</h1><p>单机多卡。单进程多线程。工作与单机。</p><h1 id="nn-parallel-DistributedDataParallel-1"><a href="#nn-parallel-DistributedDataParallel-1" class="headerlink" title="nn.parallel.DistributedDataParallel"></a>nn.parallel.DistributedDataParallel</h1><p>可以多级多卡，速度快于DataParallel。</p><p><img src="/2022/01/12/gpu%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220112170831371.png" alt="image-20220112170831371"></p><p><img src="/2022/01/12/gpu%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220112171028243.png" alt="image-20220112171028243"></p><p>多gpu训练，每块gpu上初始模型需要一摸一样。</p><p>loss_reduce,多块gpu loss做平均。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习知识 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>GAN</title>
      <link href="/2022/01/11/GAN/"/>
      <url>/2022/01/11/GAN/</url>
      
        <content type="html"><![CDATA[<h1 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h1><p>分辨模型，分到属于哪一个类别。生成模型生成数据本身。</p><p>主要思路：</p><p>生成模型G，生成数据本身的分布。辨别模型D，估计数据是从真正的训练数据分布当中得来的还是从G生成得来的。</p><p>生成模型G是尽量让判别模型D犯错。</p><p>深度学习不仅仅是深度网络，而且也是学习整个深度数据分布的一个特征表示。深度学习对于辨别模型有比较好的效果，但对于生成模型的研究比较少，要去近似那个分布计算似然函数。</p><p>G：造假者。提高造假币的能力。</p><p>D：警察。提高判断假币的能力。</p><p>希望最后造假者的能力很强，而警察分不清真币还是假币。这样就能生成跟真实一样的数据了。</p><p>生成模型G的框架是MLP，它的输入是随机的噪声，MLP可以把噪声拟合到任意的分布。判别模型也是MLP，两个网络都可以反向传播学习参数。</p><p>不是计算出一个模型的参数，而是学一个模型近似结果即可。坏处是不知道真正的分布长啥样，好处是计算简单。</p><p>adversarial example生成一些假的样本。能够互动到分类器，从而测试整个算法的稳定性。\</p><h2 id="对抗网络模型"><a href="#对抗网络模型" class="headerlink" title="对抗网络模型"></a>对抗网络模型</h2><p>生成器生成对于数据$x$上的分布$p_g$，一开始 初始化模型为$p_z(z)$,生成模型$G(z,θ_g)$把$z$映射成$x$,判别器$D(x,θ_d)$输出一个标量，用于判别是生成的数据还是真实数据。</p><p>对抗损失</p><script type="math/tex; mode=display">\mathop{min} \limits_{G} \mathop{max} \limits_{D} \mathbb{E}_{x \sim p_{data}(x)}[log(D(x))]+ \mathbb{E}_{z \sim p_{data}(z)}[log(1-D(G(z)))]</script><p>假如$D$是完美的，对于一张真图片$x$,$D(x)=1$。</p><p>$G(z)$是生成的，假如$D$是完美的，$D(G(z))=0$。</p><p>在这种情况下，两项都为0。</p><p>$D$尽量使数据分开，$G$使得生成的数据使得$D$分不开。第一项使得$D$尽可能正确,第二项使$D$犯错。</p><p>希望最后生成数据和真实数据在分布上一样，判别器无能为力，对于生成数据和真实数据都输出0.5。</p><p>每次迭代先更新判别器，再更新生成器，糊弄判别器。$D$每次不能更新的太小，也不能更新的太完美。</p><p>GAN网络不容易收敛。</p><p>GAN是无监督学习，没有用到label。GAN使用有监督学习的损失来做无监督学习，label来自于数据，数据是采样的还是生成的。</p><h1 id="最大似然估计的原理"><a href="#最大似然估计的原理" class="headerlink" title="最大似然估计的原理"></a>最大似然估计的原理</h1><p>存在即合理。</p><p>抽出一组样本的概率会随着概率模型参数变化而变化的。</p><p>一个小概率事件可以认为他不发生，但是现在发生了，这种概率达到了最大，就认为它是最大的。认为抽样样本出现时候的概率达到最大值时的模型参数就是分布的参数。于是就变成了求函数最大值。</p><p>求导数，求导数值为0的参数值。</p><h1 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h1><p>在知道p的情况下至少需要多少个比特把q给描述出来。</p><h1 id="cycle-gan"><a href="#cycle-gan" class="headerlink" title="cycle gan"></a>cycle gan</h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/45394148">CycleGAN论文的阅读与翻译，无监督风格迁移 - 知乎 (zhihu.com)</a></p></blockquote><p>图像到<strong>图像</strong>的<strong>翻译 (Image-to-Image translation)</strong> 是一种视觉上和图像上的问题，它的目标是使用成对的图像作为训练集，（让机器）学习从输入图像到输出图像的映射。然而，在很多任务中，成对的训练数据无法得到。</p><p>我们提出一种在缺少成对数据的情况下，（让机器）学习从源数据域X到目标数据域Y 的方法。我们的目标是使用一个对抗损失函数，学习映射G：X → Y ，使得判别器难以区分图片 G(X) 与 图片Y。因为这样子的映射受到巨大的限制，所以我们为映射G 添加了一个相反的映射F：Y → X，使他们成对，同时加入一个循环一致性损失函数 (cycle consistency loss)，以确保 F(G(X)) ≈ X（反之亦然）。</p><p>在缺少成对训练数据的情况下，我们比较了风格迁移、物体变形、季节转换、照片增强等任务下的定性结果。经过定性比较，我们的方法表现得比先前的方法更好。</p><p><img src="https://pic2.zhimg.com/v2-6bd4d69c9c31a79b949af61755f1cebd_r.jpg" alt="preview"></p><p>我们的目标是学习两个数据域 X 与 Y 之间的映射函数，定义数据集合与数据分布，与模型的两个映射，其中：<img src="https://www.zhihu.com/equation?tex=%5C%7B+x_%7Bi%7D+%5C%7D+%5EN+_%7Bi%3D1%7D++%5Cquad++x_i+%5Cin+X+++%2C%5Cquad+x%5Csim+p_%7Bdata%7D%28x%29%5C%5C+%5C%7B+y_%7Bi%7D+%5C%7D+%5EM+_%7Bi%3D1%7D++%5Cquad++y_i+%5Cin+Y+++%2C%5Cquad+y%5Csim+p_%7Bdata%7D%28y%29+%5C%5C+%5C+%5C%5C+G%3A+X+%5Crightarrow+Y+%5C%5C+F%3A+Y+%5Crightarrow+X" alt="[公式]"></p><p>另外，我们引入了两个判别函数：</p><ul><li>用于区分{x} 与 {F(y)} 的 <img src="https://www.zhihu.com/equation?tex=D_X" alt="[公式]"></li><li>用于区分{y} 与 {G(x)} 的 <img src="https://www.zhihu.com/equation?tex=D_Y" alt="[公式]"> 。</li></ul><p>我们的构建的模型包含两类组件(Our objective contains two types of terms)：</p><ul><li>对抗损失(adversarial losses)，使生成的图片在分布上更接近于目标图片；</li><li>循环一致性损失(cycle consistency losses)，防止学习到的映射 G与F 相互矛盾。</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>MobileNet</title>
      <link href="/2022/01/10/MobileNet/"/>
      <url>/2022/01/10/MobileNet/</url>
      
        <content type="html"><![CDATA[<h1 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h1><p>传统卷积，内存需求大，运算量需求大。vgg16权重大小约490M，resnet152层权重参数约644M。</p><p>mobileNet是google17年提出的，专注于移动端或嵌入式设备中的轻量级CNN网络。相比于传统神经网络，<strong>准确率小幅降低的前提下大大减少模型参数和运算量</strong>。（在imageNet上，相比于VGG准确率减少了0.9%，但模型参数只有vgg的1/32)。</p><h2 id="depth-wise-separable-convolutions"><a href="#depth-wise-separable-convolutions" class="headerlink" title="depth-wise separable convolutions"></a>depth-wise separable convolutions</h2><p>DW卷积：每个卷积核只和特定的一个channel进行卷积。</p><ul><li>每个卷积核只有一个channel，负责输入特征矩阵的特定channel。</li><li>输出channel个数=卷积核个数=输入特征矩阵channel数。</li></ul><p><img src="/2022/01/10/MobileNet/dw_network_bili.png" alt="image-20220110174042293"></p><p>PW卷积是普通的卷积，卷积核的大小是1。</p><p><img src="/2022/01/10/MobileNet/pw_conv.png" alt="image-20220110201847646"></p><p>深度可分离卷积是吧DW和PW放在一起使用的。</p><p>普通卷积输出特征图为4个channel，使用DW+PW得到输出特征也为4个channel。</p><p><img src="/2022/01/10/MobileNet/compute_times.png" alt="image-20220110202132545"></p><p>α为卷积核个数的倍率，控制卷积过程中卷积核的个数，当宽度乘子α时，输入通道数为αM，输出通道数为αN。β为分辨率参数，对于不同的网络输入图片大小，加法次数不同，网络计算速度不同。</p><blockquote><p>α因子是在训练之前就选定好的</p></blockquote><p><img src="/2022/01/10/MobileNet/result_ablation.png" alt="image-20220110203944575"></p><p><img src="/2022/01/10/MobileNet/dw_conv_structure.png" alt="image-20220110211508226"></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>我们为移动和嵌入式视觉应用程序提出了一类名为mobilenet的高效模型。mobilenet基于一种流线型的架构，使用<strong>depth-wise separable convolutions深度可分离卷积来构建轻量级的深度神经网络</strong>。我们引入了两个简单的<strong>全局超参数，可以有效地在延迟和准确性之间进行权衡</strong>。这些超参数允许模型构建者根据问题的约束为他们的应用程序选择合适大小的模型。我们在资源和准确性的权衡上进行了大量的实验，并在ImageNet分类中显示了与其他流行模型相比的强大性能。然后，我们在广泛的应用和用例中展示了mobilenet的有效性，包括对象检测、细粒度分类、人脸属性和大规模地理定位。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>自从AlexNet[19]赢得ImageNet挑战(ILSVRC 2012[24])使深度卷积神经网络普及以来，卷积神经网络已经在计算机视觉中无处不在。为了达到更高的精度，总的趋势是制作更深、更复杂的网络[27,31,29,8]。然而，这些提高准确性的进步并不一定会使网络在规模和速度方面更高效。在许多现实世界的应用中，如机器人、自动驾驶汽车和增强现实，识别任务需要在计算有限的平台上及时执行。</p><p>本文描述了一种高效的网络架构和一组两个超参数，以构建非常小的、低延迟的模型，可以很容易地满足移动和嵌入式视觉应用的设计需求。第二节回顾了之前建立小模型的工作。第3节描述了MobileNet架构和两个超参数宽度乘法器和分辨率乘法器width multiplier and resolution multiplier，以定义更小和更有效的MobileNet。第4节描述了ImageNet的实验以及各种不同的应用程序和用例。第5节以总结和结论结束。</p><h1 id="MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks"><a href="#MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks" class="headerlink" title="MobileNetV2: Inverted Residuals and Linear Bottlenecks"></a>MobileNetV2: Inverted Residuals and Linear Bottlenecks</h1><p><img src="/2022/01/10/MobileNet/mobilenet_v2.png" alt="image-20220110214925153"></p><p><img src="/2022/01/10/MobileNet/MobileNet_compare_resnet.png" alt="image-20220110214543637"></p><p>对于倒残差结构的最后一个卷积层，使用了线性激活函数，而不是relu。因为通过对relu函数的分析可以看出，对于低维特征relu的损失大，对高维损失小。倒残差两边细，中间粗，mobileNet输出就是低维向量。所以使用线性激活函数替代relu。</p><p><img src="/2022/01/10/MobileNet/relu_dim.png" alt="x"></p><p>步距=1，且输入输出H,W,C都相等，才使用残差。</p><p><img src="/2022/01/10/MobileNet/mobile_netv2_structrue.png" alt="image-20220110215807021"></p><p><img src="/2022/01/10/MobileNet/invert_residual_io.png" alt="image-20220110220131093"></p><p><img src="/2022/01/10/MobileNet/mobilev2_classification.png" alt="image-20220110220504613"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人脸关键点检测的轻量化网络</title>
      <link href="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/"/>
      <url>/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<p>本篇文章文章主要介绍PFLD: A Practical Facial Landmark Detector这篇文章。</p><h2 id="2019CVPR-PFLD-A-Practical-Facial-Landmark-Detector"><a href="#2019CVPR-PFLD-A-Practical-Facial-Landmark-Detector" class="headerlink" title="[2019CVPR]PFLD: A Practical Facial Landmark Detector"></a>[2019CVPR]PFLD: A Practical Facial Landmark Detector</h2><blockquote><p>2019CVPR Xiaojie Guo1, Siyuan Li1, Jinke Y u1, Jiawan Zhang1, Jiayi Ma2, Lin Ma3, Wei Liu3, and Haibin Ling4</p><p>1Tianjin University2Wuhan University3Tencent AI Lab4Temple University</p></blockquote><h2 id="关键点检测四大挑战"><a href="#关键点检测四大挑战" class="headerlink" title="关键点检测四大挑战"></a>关键点检测四大挑战</h2><p>•<strong>挑战1</strong> ——<strong>局部变化。</strong> <strong>表情、局部极端光照(如高光和阴影)和遮挡</strong>会给人脸图像带来部分变化和干扰。一些区域的关键点可能会偏离它们的正常位置，甚至消失。</p><p>•<strong>挑战2</strong>——<strong>全局变化。</strong> <strong>姿态和成像质量</strong>是全局影响图像中人脸外观的两个主要因素，当人脸的全局结构被错误估计时，会导致很大一部分关键点定位出现偏差。</p><p>•<strong>挑战3 </strong>—— <strong>数据不平衡。</strong>在浅层学习和深层学习中，一个<strong>可用的数据集在其类/属性之间呈现不平衡的分布</strong>是很常见的。这种不平衡很可能使算法/模型不能正确地表示数据的特征，从而在不同的属性上提供不令人满意的精度。</p><p>原因：在现实生活中，获得完美的脸几乎不可能。换句话说，人脸经常暴露在不受控制甚至不受约束的环境中。在不同的光照条件下，外观有很大的姿态、表情和形状变化，有时伴有部分遮挡。</p><p><img src="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/PFLD_challenge.png" alt="image-20220103114028361"></p><p>上图提供了几个这样的示例。此外，为数据驱动方法提供足够的训练数据也是模型性能的关键。假设出于数据平衡的考虑，在不同的条件下捕捉几个人的面孔可能是可行的，但这种收集方式不现实。</p><p>上述挑战大大增加了精确检测的难度，要求检测器具有鲁棒性。</p><p>•<strong>挑战4</strong> ——<strong>模型效率</strong>。模型大小和计算要求。机器人、AR和视频聊天等任务需要在配备有限计算和内存资源的平台(如智能手机或嵌入式产品)上实时执行。这一点特别要求关键点检测器具有小的模型尺寸和快速的处理速度。</p><h2 id="方法简介"><a href="#方法简介" class="headerlink" title="方法简介"></a>方法简介</h2><p><img src="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/two_stream_network.png" alt="image-20220103170252735"></p><p>•与局部变化相比，全局变化极大地影响整个地标集。为了增强鲁棒性，使用网络上分支来估计每个人脸样本的几何信息，并随后正则化地标定位。辅助网络(上分支)可以输出目标角度，估计三维旋转信息，包括偏航yaw、俯仰pitch和滚转角roll。有了这三个欧拉角，就可以确定头部的姿态。</p><p>•预测地关键点的骨干网(下分支)中使用了MobileNet代替了传统卷积，骨干网的负荷大大减少，从而加快了速度。此外，网络可以通过根据用户的需求调整mobilenet的宽度参数来压缩，使模型更小、更快。</p><p>上下分支网络设计有效的应对了挑战2和挑战4。</p><p>扩大感受野，更好地捕捉人脸的全局结构，增加了一个多尺度全连通层，用于精确定位图像中的关键点。</p><h2 id="针对于几何变化和数据不平衡问题设计的loss函数"><a href="#针对于几何变化和数据不平衡问题设计的loss函数" class="headerlink" title="针对于几何变化和数据不平衡问题设计的loss函数"></a>针对于几何变化和数据不平衡问题设计的loss函数</h2><p><img src="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/landmark_loss.png" alt="image-20220103174622027"></p><p>第m个输入的第n个地标，N是每个检测人脸预定义的标定点数量，M表示每个过程中训练图片的个数。$d$代表估计值与真值之间的偏差。</p><p><img src="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/unbalanced_data_angle_loss.png" alt="image-20220103174654579"></p><p>公式(2)表示地面实况和估计的偏航、俯仰和滚转角之间的偏离角。显然，随着偏离角度的增加，惩罚力度会增加。此外，将样本分为一个或多个属性类，包括侧面、正面、抬头、低头、表情和遮挡。C表示类。</p><p>加权参数$ω_c^n$根据属于c类的样本的分数进行调整(论文中采用倒数)。这样设计loss，对于数量少的样本惩罚系数会增大，对于数量多的样本惩罚系数会减小。</p><p>很容易得到式(2)中的$\sum_{c=1}^C \sum_{k=1}^K(1−cosθ^k_n)$作为式(1)中的$\gamma_n$。式中$θ^1、θ^2、θ^3$(K=3)表示地真值与估计的偏航角、俯仰角、滚转角的偏差角，当误差大时惩罚系数大，当误差小时惩罚系数小。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/PFLD_compared_with_sota.png" alt="image-20220103200847245"></p><p><img src="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/loss_ablation.png" alt="image-20220103200931893"></p>]]></content>
      
      
      <categories>
          
          <category> 人脸关键点检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人脸关键点检测简介</title>
      <link href="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%AE%80%E4%BB%8B/"/>
      <url>/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>人脸关键点检测也被称为人脸关键点定位，旨在定位人脸上预定义的关键点，关键点位于五官和人脸边界上。关键点能够反映各个部位的脸部特征，随着技术的发展和对精度要求的增加，人脸关键点的数量经历了从最初的5个点到如今超过200个点的发展历程。下图分别是68点和98点关键点的标注形式。</p><p><img src="https://img-blog.csdnimg.cn/20190103210914208.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><img src="https://wywu.github.io/projects/LAB/support/WFLW_annotation.png" alt="img"></p><p>提取人脸关键点可以获得五官的定位，可以对人脸获得五官patch，可以辅助人脸识别、活体检测等任务。或者是可以对人脸进行对齐，所以人脸关键点定位的相关论文也可以使用face alignment进行检索，计算检测到的人脸关键点到目标平均脸的形变参数，可以用于3维人脸重建。</p><h1 id="人脸关键点检测的评价指标："><a href="#人脸关键点检测的评价指标：" class="headerlink" title="人脸关键点检测的评价指标："></a>人脸关键点检测的评价指标：</h1><h2 id="Inter-Ocular-Normalization"><a href="#Inter-Ocular-Normalization" class="headerlink" title="Inter-Ocular Normalization"></a>Inter-Ocular Normalization</h2><p>使用左右眼角的距离对于关键点误差进行归一化，其中$d$为左右眼角的距离，$x$和$x^*$分别为关键点和关键点真值。</p><script type="math/tex; mode=display">e_i = \frac{∣∣x_i−x_i^∗∣∣_2}{d}</script><h2 id="Inter-Pupil-Normalization"><a href="#Inter-Pupil-Normalization" class="headerlink" title="Inter-Pupil Normalization"></a>Inter-Pupil Normalization</h2><p>使用瞳间距对于关键点误差进行归一化的公式于使用左右眼角的距离对于关键点误差进行归一化的公式形式一样，不过$d$换成了瞳间距。如果在真值中瞳仁没有标注，就使用眼周关键点求平均值获得眼睛中心的坐标值，计算瞳间距。</p><h2 id="Normalization-Mean-Error"><a href="#Normalization-Mean-Error" class="headerlink" title="Normalization Mean Error"></a>Normalization Mean Error</h2><p>归一化平均错误率计算关键点检测平均误差，$N$为整张图片上关键点的个数。</p><script type="math/tex; mode=display">e = \frac{∑ _{i=1}^N ∣∣x_i−x_i^∗∣∣ _2}{N∗d}</script><h2 id="Failure-Rate-FR"><a href="#Failure-Rate-FR" class="headerlink" title="Failure Rate (FR)"></a>Failure Rate (FR)</h2><p>是评估定位质量的另一个指标。对于一幅图像，如果NME大于阈值，则认为预测失败。通常使用0.1作为阈值。</p>]]></content>
      
      
      <categories>
          
          <category> 人脸关键点检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人脸关键点检测对heatmap的loss设计</title>
      <link href="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/"/>
      <url>/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<p>本文主要介绍人脸关键点检测中使用的heatmap。并且会介绍Adaptive Wing Loss这篇文章中针对于heatmap损失函数的设计，以及它的实验结果。</p><h2 id="heatmap介绍"><a href="#heatmap介绍" class="headerlink" title="heatmap介绍"></a>heatmap介绍</h2><p><em>参考了[2016CVPRW]Deep Alignment Network: A convolutional neural network for robust face alignment 3.3. Landmark heatmap</em></p><p>landmark heatmap 是landmark位置的强度最高，强度随着距离最近的地标的距离增大而减小的图像。</p><script type="math/tex; mode=display">H(x,y)=\frac{1}{1+min_{s_i}}||(x,y)-s_i||</script><p>$H$是热图，$s_i$是第i个landmark，为了提高性能，可以只在每个地标周围以16为半径的圆圈内计算热图值。</p><p>上面的公式中使用了范数，也可在的每个关键点处绘制一个高斯分布来生成的热图。</p><h2 id="2019ICCV-Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression"><a href="#2019ICCV-Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression" class="headerlink" title="[2019ICCV]Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression"></a>[2019ICCV]Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression</h2><blockquote><p>Xinyao Wang1,2Liefeng Bo2Li Fuxin1</p><p>1Oregon State University 2JD Digits<br>{wangxiny, lif}@oregonstate.edu,{xinyao.wang3, liefeng.bo}@jd.com</p></blockquote><ol><li><p>评价：提出了一种新的loss函数，称为Adaptive Wing loss，它具有适应性，这种适应性对前景像素损失的惩罚更大，而对背景像素损失惩罚较小。提出了加权损失图(Weighted Loss Map)，为前景像素和“困难的背景像素”分配较高的权重，以帮助训练过程更多地关注对地标定位至关重要的像素。为了进一步提高面对齐精度，引入了边界预测和基于边界坐标的CoordConv。</p></li><li><p>针对问题：基于深度网络的热图回归已成为人脸关键点定位的主流方法之一。然而，对于热图回归中损失函数的研究却很少。</p></li><li><p>本文的目的：分析了人脸关键点检测问题热图回归的理想损失函数性质，并根据此设计了Adaptive Wing loss。该模型在像素级对真值热图进行更加准确的回归，然后利用预测的热图来推断关键点的位置。</p></li><li><p>热图分析：</p><p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/heatmap_pixel_class.png" alt="image-20220101171407947"></p><p>在热图中前景像素(具有正值的像素)，特别是每一个高斯分附近的像素，对于准确定位关键点至关重要。这些像素上即使很小的预测误差也会导致预测偏离正确的模式。这些像素的预测误差即使很小，也会导致预测偏离正确的模式。</p><p>相反，准确预测背景像素(零值像素)的值就不那么重要了，因为这些像素上的小误差在大多数情况下不会影响地标的预测。</p><p>然而，困难背景像素(图1背景像素接近前景像素区域的像素)的预测精度也很重要，因为它们经常被错误地回归为前景像素，可能导致预测不准确。</p></li><li><p>heatmap regression中的常用的MSE loss</p><p>a. 均方误差对小误差不敏感，影响了对高斯分布模式的正确定位。</p><p>b. 在训练过程中，MSE loss对于所有的所有的像素都具有相同的损失函数和相等的权值，但是在热图上，背景像素绝对优于前景像素。</p><p>由于a)和b)，使用MSE损失训练的模型所得的结果与ground truth热图相比，前景像素强度会被降低，会获得一张模糊和膨胀的热图。这种低质量的热图可能会导致对面部关键点的错误估计。</p><p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/MSE_blur.png" alt="image-20220101175929474"></p></li><li><p>网络结构</p><p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/hourglass_network.png" alt="image-20220101200537435"></p></li></ol><ol><li><p>损失函数的基本原理</p><p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/loss_influence_balance.png" alt="image-20220101201422715"></p><p>•其中，N是训练样本的总数，H、W和C分别是热图的高度、宽度和通道。</p><p>•从稳健统计中引入一个概念。影响力是一个启发式工具，在稳健统计中用来研究估计量的性质。</p><p>•在收敛时，在收敛时，所有误差的影响必须相互平衡。因此，具有较大梯度幅值的像素的正误差(影响较大)需要用影响较小的许多像素上的负误差来平衡。与梯度较小的错误相比，梯度较大的错误也会在训练中更被关注。如何对小误差有一个相对较高的关注是设计的出发点之一。</p></li><li><p>不同loss对于大梯度和小梯度误差影响力分析以及连续性分析</p><p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/loss_grad.png" alt="image-20220101204052130"></p><p>a. 常用于热图回归的均方误差损失函数。均方误差损失的梯度是线性的，因此误差小的像素影响小。该属性可能导致训练收敛，而许多像素仍有小误差。因此，用均方误差损失训练的模型倾向于预测模糊和扩大的热图。</p><p>b. L1loss具有恒定的梯度，因此误差小的像素与误差大的像素具有相同的影响。然而，L1损失的梯度在零点不是连续的，具有正误差的像素的数量必须恰好等于具有负误差的像素的数量。</p><p>c.当误差较大时具有恒定梯度，当误差较小时具有大梯度的wing损失。因此，误差小的像素将被放大。</p><p>仍然不能克服其梯度在零点时的不连续性。不适用于热图回归。</p><p>因为在所有背景像素上计算wing损失，背景像素上的小误差具有不成比例的影响。训练一个在这些像素上输出零或小梯度的神经网络是非常困难的。</p><p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/wingloss.png" alt="image-20220101204617486"></p></li><li><p>理想的loss设计 </p><p>•误差较大</p><p>•loss函数具有恒定的影响，这样它将对不准确的注释和遮挡具有鲁棒性。</p></li></ol><p>   •误差小</p><p>   •对于前景像素，影响应该开始增加，以便训练能够集中于减少这些误差。当误差非常接近零时，这种影响应该会迅速减小，这样这些“足够好”的像素将不再被关注。减少正确估计的影响有助于网络保持收敛，而不是像L1和wing loss那样振荡。</p><p>   •对于背景像素，梯度应该表现得更类似于均方误差损失，即随着训练误差的减小，梯度将逐渐减小到零，因此当误差较小时，影响将相对较小。该属性减少了背景像素上的训练的关注，稳定了训练过程。</p><ol><li>Adaptive Wing loss设计</li></ol><p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/AdaptiveWingLoss.png" alt="image-20220104205005454"></p><p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/keep_continus_const.png" alt="image-20220101210329410"></p><p>​    损耗函数在零点平滑。</p><ol><li><p>区分前后景像素</p><p>在具有64 × 64热图和7×7高斯大小的面部标志定位的典型设置中，前景像素仅构成所有像素的1.2%。为这种不平衡的数据分配相同的权重可能会使训练过程收敛缓慢，从而导致较差的性能。为了进一步建立网络聚焦前景像素和困难背景像素(接近前景像素的背景像素)的能力，引入加权损失图来平衡不同类型像素的损失。前景像素和困难背景像素1，其他像素0。</p><p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/back_front_distinguish.png" alt="image-20220101210634320"></p></li></ol><h2 id="adapt-wing-loss论文测试效果"><a href="#adapt-wing-loss论文测试效果" class="headerlink" title="adapt_wing_loss论文测试效果"></a>adapt_wing_loss论文测试效果</h2><p><em>此论文只给了测试代码以及预训练好的模型</em></p><p>真值绿色，红色为预测值。</p><p>下图为nme &gt; 0.1的失败案例。失败的原因是图片本身有遮挡，或者图片清晰度不高，或者是人脸为大角度姿势。</p><p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/47_Matador_Bullfighter_Matador_Bullfighter_47_772_2485.jpg" alt></p><p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/59_peopledrivingcar_peopledrivingcar_59_202_2488.jpg" alt="59_peopledrivingcar_peopledrivingcar_59_202_2488"></p><p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/61_Street_Battle_streetfight_61_708_2489.jpg" alt="61_Street_Battle_streetfight_61_708_2489"></p><p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/20_Family_Group_Family_Group_20_667_2486.jpg" alt></p><p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/32_Worker_Laborer_Worker_Laborer_32_600_2499.jpg" alt="32_Worker_Laborer_Worker_Laborer_32_600_2499"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Step 10 Time: 0.194774 Input Mean: 0.005637 Output Mean: 0.009510</span><br><span class="line">Step 20 Time: 0.196732 Input Mean: 0.005678 Output Mean: 0.009235</span><br><span class="line">Step 30 Time: 0.195847 Input Mean: 0.005609 Output Mean: 0.009110</span><br><span class="line">Step 40 Time: 0.195520 Input Mean: 0.005573 Output Mean: 0.008047</span><br><span class="line">Step 50 Time: 0.212965 Input Mean: 0.005603 Output Mean: 0.008757</span><br><span class="line">Step 60 Time: 0.195021 Input Mean: 0.005605 Output Mean: 0.009210</span><br><span class="line">Step 70 Time: 0.194650 Input Mean: 0.005525 Output Mean: 0.008658</span><br><span class="line">Step 80 Time: 0.194495 Input Mean: 0.005669 Output Mean: 0.008297</span><br><span class="line">Step 90 Time: 0.195685 Input Mean: 0.005590 Output Mean: 0.008072</span><br><span class="line">Step 100 Time: 0.194614 Input Mean: 0.005628 Output Mean: 0.008212</span><br><span class="line">NME: 0.082181 Failure Rate: 0.208400 Total Count: 2500.000000 Fail Count: 521.000000</span><br><span class="line">Evaluation done! Average NME: 0.082181</span><br><span class="line">Everage runtime for a single batch: 0.183916</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="数据集与处理方式"><a href="#数据集与处理方式" class="headerlink" title="数据集与处理方式"></a>数据集与处理方式</h2><p>300w的数据集除了private外，没有官方边界。</p><p>WFLW提供的边界框不是很准确。</p><p>因此在两个维度上都将边界框放大了10%，300W直接裁剪人脸。</p><h2 id="数据集WFLW-7500训练-2500测试。"><a href="#数据集WFLW-7500训练-2500测试。" class="headerlink" title="数据集WFLW#7500训练 2500测试。"></a>数据集WFLW#7500训练 2500测试。</h2><p>人脸多种属性、关键点标注数据集，包含了10000张脸，其中7500用于训练，2500张用于测试，共98个关键点。除了关键点之外，还有姿态，表情，光照，妆容，遮挡，模糊。</p><p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bheatmap_loss%E8%AE%BE%E8%AE%A1/WFLWdataset.png" alt="image-20220102110938051"></p><p>list_98pt_rect_attr_train_test.txt是98X2关键点+4左上右下+6属性+图片名称 = 196 +4+6+1 = 207 list_98pt_test.txt196 + 名字 =197。</p>]]></content>
      
      
      <categories>
          
          <category> 人脸关键点检测 </category>
          
          <category> 试验记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>风格造成的人脸关键点检测误差</title>
      <link href="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/"/>
      <url>/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/</url>
      
        <content type="html"><![CDATA[<p>由于图片拍摄时候外界环境,比如光线变化等会造成图片”风格”的不同,不同风格的同一张人脸经过landmark detector 可能会获得不同的地标,这并不是由于人脸结构本身造成。对于此问题的研究目前有两篇具有代表性的文章,主要思路都是通过数据增强,对图片进行某种风格变换,再去训练landmark detector,使得检测器对于风格变化具有鲁棒性。从而提高总体landmark检测准确度。</p><h2 id="2018CVPR-Style-Aggregated-Network-for-Facial-Landmark-Detection"><a href="#2018CVPR-Style-Aggregated-Network-for-Facial-Landmark-Detection" class="headerlink" title="[2018CVPR]Style Aggregated Network for Facial Landmark Detection"></a>[2018CVPR]Style Aggregated Network for Facial Landmark Detection</h2><blockquote><p>Xuanyi Dong1, Yan Yan1, Wanli Ouyang2, Yi Yang1∗<br>1University of Technology Sydney,2The University of Sydney<br>{xuanyi.dong,yan.yan-3}@student.uts.edu.au;<br>wanli.ouyang@sydney.edu.au; yi.yang@uts.edu.au</p></blockquote><ol><li><p>评价：提出了一种对图像风格差异不敏感的风格聚合网络(style - aggregation Network, SAN)人脸地标检测方法。在face landmark detection领域第一个明确了图像风格变化问题造成可能会带来检测失误。</p></li><li><p>针对问题：不同图像风格差异较大的问题。除了人脸本身的方差外，图像风格的内在方差，如灰度与彩色图像、亮与暗、强烈与暗淡等。landmark检测器对于不同风格的同一张人脸图片有不同的输出。例：三张图片内容完全相同。唯一不同的是形象风格。使用训练好的面部标志检测器来定位面部标志。zoom部分显示不同风格图像上相同面部标志的预测位置之间的偏差。</p><p><img src="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/different_style.png" alt="image-20211229210439162"></p></li><li><p>本文的目的：提高对图像风格的大方差的鲁棒性。</p></li><li><p>实现的方法：</p><p><img src="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/structure.png" alt="image-20211229212141043"></p></li></ol><p>SAN架构概述。网络由两部分组成。第一个是样式聚合的脸生成模块，该模块将输入图像转换成不同的样式，然后将它们组合成样式聚合的脸。</p><p>二是人脸地标预测模块。该模块将原始图像和样式聚合图像作为输入，得到两个互补的特征，然后将两个特征进行融合，级联生成热图预测。“fc”意味着fully-convolution。</p><p>5.方法简介</p><p>数据增强（风格变化）：通过将300-W和AFLW转换成不同的风格，发布了两个新的人脸标志物检测数据集:300W-Styles(≈12000 images)和AFLW- styles(≈80000 images)。</p><p>通过生成式对抗模块将原始人脸图像转换为风格聚合图像，使用风格聚合图像使得人脸图像对环境变化更具鲁棒性。</p><p>风格互补训练：将原始人脸图像与风格聚合的人脸图像作为双流输入landmark 检测器，二流输入可以互补。</p><blockquote><p>假如测试阶段移除gan呢，相当于多模态训练，单模态测试？</p></blockquote><p>在基准数据集AFLW方法表现良好。代码可在GitHub上公开获取:<a href="https://github.com/D-X-Y/SAN">https://github.com/D-X-Y/SAN</a></p><p>6.个人的思考</p><p>相当于做了数据增强，增加了数据量。</p><h2 id="2019ICCV-Aggregation-via-Separation-Boosting-Facial-Landmark-Detector-with-Semi-Supervised-Style-Translation"><a href="#2019ICCV-Aggregation-via-Separation-Boosting-Facial-Landmark-Detector-with-Semi-Supervised-Style-Translation" class="headerlink" title="[2019ICCV]Aggregation via Separation: Boosting Facial Landmark Detector with Semi-Supervised Style Translation"></a>[2019ICCV]Aggregation via Separation: Boosting Facial Landmark Detector with Semi-Supervised Style Translation</h2><blockquote><p>Shengju Qian1, Keqiang Sun2, Wayne Wu2,3, Chen Qian3, Jiaya Jia1,4<br>1The Chinese University of Hong Kong2Tsinghua University<br>3SenseTime Research4Y ouTu Lab, Tencent<br>{sjqian, leojia}@cse.cuhk.edu.hk, skq17@mails.tsinghua.edu.cn,{wuwenyan, qianchen}@sensetime.com</p></blockquote><ol><li><p>评价：</p></li><li><p>针对问题：鉴于任何人脸图像都可以被分解成光线、纹理和图像环境的风格空间，以及一个风格不变的结构空间，本文的关键想法是利用每个个体的风格和形状空间。[2018CVPR]Style Aggregated Network for Facial Landmark Detection中显式地研究了图像风格带来的畸变现象。</p></li><li><p>思路：</p><p>在实践中，图像内容是指对象、语义和边缘特征，而风格可以是颜色和纹理。</p><p>基于人脸地标检测的目的，即通过过滤不受约束的“风格”，回归“人脸内容”，即人脸几何的主成分。定义“style”是指的是图像背景、光线、质量、是否存在眼镜等阻碍探测器识别人脸几何形状的因素。</p></li><li><p>实现的方法：</p><p>利用风格迁移和解纠缠表征学习disentangled representation learning来处理人脸对齐问题，因为风格迁移的目的是在保留内容的同时改变风格。在不使用额外知识的情况下，增加人脸地标检测的训练。</p></li><li><p>方法简介：</p><p>a. 不是直接生成图像来做数据增强，而是首先将人脸图像映射到结构和风格的空间中。</p><p>b. 为了保证这两个空间的解纠缠，设计了一个条件变分自编码器模型，该模型将Kullback-Leiber (KL)散度损失和skip连接分别用于风格和结构的紧凑表示。通过分解这些特征，在现有的面部几何图形之间执行视觉风格转换。根据现有的人脸结构，将戴眼镜、质量较差、在模糊或强光下的人脸进行相应的风格，用于进一步训练人脸地标探测器，形成一个较为通用和健壮的人脸几何识别系统。</p></li></ol><p><img src="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/feature_en-decoder.png" alt="image-20211231173635145"></p><p>框架由两部分组成。一个是学习面部外观和结构的解离化表示，而另一个可以是任何面部标志检测器。</p><p>在第一阶段，提出了条件变化自动编码器，用于学习风格和结构之间的分离表示。</p><p>在第二阶段，在从其他人脸转换到风格后，带有结构的“风格化”图像可用于提高训练性能和风格不变检测器。</p><ol><li><p>个人的思考</p><p>与第一篇文章中类似，相当于做了数据增强，增加了数据量。不同的是不是显示地改变光线或是rgb图转换成灰度，而是隐式地对于风格特征和结构特征进行了分离，在把一张图片风格迁移到其他风格。</p></li><li><p>实验效果</p><p><img src="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/test_00075000.png" alt></p></li></ol><p>   <img src="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/test_01000000.png" alt></p><p>   上面两张图 给出了在batch_size=8的情况下，第75个epoch和1000000个 epoch数据增强 的效果。</p><p>   最上面第一行是第一列各个图像地关键点，也就是人脸地结构特征。在第二行到第九行中，第一列为原始图像，之后地第二列到第九列为各个原始图像保持原始landmark结构的情况下，生成的具有风格迁移地人脸。</p>]]></content>
      
      
      <categories>
          
          <category> 人脸关键点检测 </category>
          
          <category> 试验记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>web3.0</title>
      <link href="/2021/11/19/web3-0/"/>
      <url>/2021/11/19/web3-0/</url>
      
        <content type="html"><![CDATA[<p>1.什么是web3.0？</p><p>web1.0类似于博客，有专门的人边界、pgc发布信息，用户消费信息。</p><p>web2.0类似于博客、微博，用户可以read和write，user generate content。</p><p>web3.0不仅仅能write，还可以own。现阶段比如微博网红相比于平台只能拥有价值网络的一小部分。web3.0用户类似于股东。</p><p>公司以前创始人所有-&gt;创始人+股东-&gt;创始人+股东+员工持股，相当于一个范式的转变。</p><p>web3.0user就是investor。</p><blockquote><p>追星需要投入时间、感情 和金钱，用爱发电。但最后收益的是经济公司。web3.0更多的是付出的人获得收益。</p></blockquote><p>用户、创始人、员工。</p><blockquote><p>初始的用户可能对于一个公司的作用很大，但享受到的收益大。</p></blockquote><p>基于protocol web3.0公司核心团队拿15%-30%已经很多了。</p><p>2.web3.0公司治理结构如何保持稳定？</p><p>在相当长一个时间段需要一个领导人强势引导。民主会影响效率。这一点需要摸索迭代。</p><p>deo decentralized autonomous organization。</p><p>3.什么时候对于web3.0感兴趣？</p><p>16、17九四时间，比特币冲入2万刀。</p><p>看人才净流入、净流出。</p><p>杰克·多西、Fred Wilson。</p><p>但巴菲特、芒格的反对意见。</p><p>defi、普惠式金融。</p><p>4.怎么学习web3.0</p><p>如果写到教科书里，那就不用学了？</p><p>a.钱这东西到底是啥？我们为什么要投资？为什么要攒钱？</p><p>b.虚拟货币的发展?为啥中本聪要提出比特币？eth解决了啥？defi解决了啥？有什么新变化？解决了华尔街解决的哪些问题？defi1？defi2。</p><blockquote><p>了解每个人创新的脉络</p></blockquote><p><a href="https://digitalnative.substack.com/p/chain-reactions-how-creators-web3">https://digitalnative.substack.com/p/chain-reactions-how-creators-web3</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Few-Shot Adaptive Gaze Estimation</title>
      <link href="/2021/11/18/Few-Shot-Adaptive-Gaze-Estimation/"/>
      <url>/2021/11/18/Few-Shot-Adaptive-Gaze-Estimation/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Seonwook Park12<em>, Shalini De Mello1</em>, Pavlo Molchanov1, Umar Iqbal1, Otmar Hilliges2, Jan Kautz1<br>1NVIDIA,2ETH Zürich</p><p>2019ICCV</p></blockquote><p>(1) 评价：少样本自适应注视估计。</p><p>(2)针对问题：</p><p>a.由于个体解剖学之间的差异，基于大量数据训练出的注视估计网络限制了对于个体注视估计的准确性。</p><blockquote><p>每个人眼睛的生理结构不同，即使处于同一位置的人盯着同一物体，所获得的视线也可能有差异。眼睛类似于相机，每一双眼睛的内参都不一眼。</p></blockquote><p>b.过度参数化的神经网络并不适合从少数例子中学习，因为它们会很快过度拟合。</p><p>(3) 本文的目的：使用小样本自适应凝视估计网络，再少量校准样本情况下处理个性化的凝视估计。</p><p>(4)实现的方法：通过一个解耦的编码-解码器架构，以及使用元学习训练的高度适应性的凝视估计器，获得<strong>旋转（rotation aware）感知的凝视潜在表征</strong>。</p><p><img src="/2021/11/18/Few-Shot-Adaptive-Gaze-Estimation/faze.png" alt="image-20211118144150719"></p><blockquote><p>FAZE framework在给定一组具有地面真实注视方向信息的训练图像的基础上，首先学习一种为注视估计任务量身定制的潜在特征表示。考虑到这些特征，然后学习一个适应性强的注视估计网络adaptable gaze estimation network AdaGEN。使用元学习可以很容易地适应一个强大的个人特定的注视估计网络Person-specific gaze estimation network(PS-GEN)，只需很少的校准数据。</p></blockquote><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p><a href="https://github.com/NVlabs/few_shot_gaze">https://github.com/NVlabs/few_shot_gaze</a></p><p>The bash script should be self-explanatory and can be edited to replicate the final FAZE model evaluation procedure, given that hardware requirements are satisfied (8x GPUs, where each are Tesla V100 GPUs with 32GB of memory)</p><p>需要显卡资源多Orz</p><p>We also provide a realtime demo that runs with live input from a webcam in the <code>demo/</code> folder. Please check the separate <a href="https://github.com/NVlabs/few_shot_gaze/blob/master/demo/README.md">demo instructions</a> for details of how to setup and run it.、</p><p>给了实时demo。</p><hr>]]></content>
      
      
      <categories>
          
          <category> 视线估计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Appearance-Based Gaze Estimation Using Dilated-Convolutions</title>
      <link href="/2021/11/08/Appearance-Based-Gaze-Estimation-Using-Dilated-Convolutions/"/>
      <url>/2021/11/08/Appearance-Based-Gaze-Estimation-Using-Dilated-Convolutions/</url>
      
        <content type="html"><![CDATA[<blockquote><p>2018 AACV</p><p>Zhaokang Chen[0000−0003−0237−0358]and Bertram E. Shi[0000−0001−9167−7495]<br>The Hong Kong University of Science and Technology, Hong Kong SAR<br>{zchenbc,eebert}@ust.hk</p></blockquote><hr><p> \1)  评价：贡献创新点。</p><p>\2)    针对问题：啥情况啥场景。</p><p>\3)    本文的目的：可以做到啥。</p><p>\4)    实现的方法：</p><p>\5)    方法简介</p><p>\6)    方法优化</p><p>\7)    方法总结‘</p><p>\8)    文章存在的问题</p><p>\9)    个人的思考</p>]]></content>
      
      
      <categories>
          
          <category> 视线估计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multiview Multitask Gaze Estimation With Deep Convolutional Neural Networks</title>
      <link href="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/"/>
      <url>/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/</url>
      
        <content type="html"><![CDATA[<blockquote><p>2018 TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</p><p>Dongze Lian, Lina Hu, Weixin Luo, Y anyu Xu, Lixin Duan, Jingyi Y u,Member , IEEE, and Shenghua Gao.</p></blockquote><p>(1) 评价：基于深度卷积神经网络的<strong>多视图多任务</strong>注视估计</p><p>(2)  针对问题：现有的许多方法都是基于单个摄像机的，大多数方法只关注注视点估计或注视方向估计。</p><p>(3) 本文的方法：</p><p>a.分析了注视点估计和注视方向估计之间的密切关系，并采用<strong>部分共享卷积神经网络结构</strong>来同时估计注视方向和注视点。</p><p>b.引入了一种新的<strong>多视角注视跟踪数据集</strong>，该数据集由<strong>不同被试的多视角注视图像组成</strong>。</p><p>c.对于注视方向的预测，提出在左右眼注视方向上引入共面约束。</p><p>对于注视点的估计，提出引入一个跨视图池模块。</p><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/multiview_structrue.png" alt="image-20211107213710618"></p><blockquote><p>四流输入、四流输出？怎么共享参数?</p></blockquote><p><a href="https://datasets.d2.mpi-inf.mpg.de/MPIIGaze/MPIIGaze.tar.gz">https://datasets.d2.mpi-inf.mpg.de/MPIIGaze/MPIIGaze.tar.gz</a></p><h1 id="数据集ShanghaiTechGaze"><a href="#数据集ShanghaiTechGaze" class="headerlink" title="数据集ShanghaiTechGaze"></a>数据集ShanghaiTechGaze</h1><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/data_collect.png" alt="image-20211112160847091"></p><p>设备：使用27英寸的苹果iMac机器作为显示设备。屏幕的宽/高分别为59.77厘米和33.62厘米。然后，屏幕底部部署了三台GoPro Hero 4相机，以捕捉参与者的图像。两台相邻摄像机之间的距离为18.22厘米。</p><p>环境：然后，在正常照明条件下，将系统固定在房间的书桌上。为了避免其他移动的物体/人或噪音造成的干扰，房间内保持安静和空无一人，只有一名待命助手除外。</p><p>使用大型iMac的第一个原因是，希望在水平和垂直方向上预测更大范围的注视点。相比之下，GazeCapture只预测了手机或平板电脑屏幕内的点。因此，数据集比GazeCapture更具挑战性。</p><p>使用iMac的第二个原因是，它的视网膜屏幕有更高的分辨率，以保证像素精度的地面真实;同时，减少了数据采集过程中的眼睛疲劳。</p><p>采集过程：要求参与者自由地坐在屏幕前。然后，在灰色背景的屏幕上随机显示一个半径为8像素的白点(作为ground truth)，让参与者用鼠标点击。这样的点击动作可以帮助参与者将注意力吸引到这个点上。然后，记录光标的坐标和动作的时间戳，之后显示光标位置上的蓝点。同时，我们计算白点和蓝点之间的距离。如果距离超过一定的阈值(在我们的设置中是8像素)，则有可能参与者没有盯着白点，因此该数据样本将被丢弃。在采集过程中，GoPro相机被设置为视频模式。根据点击动作的时间戳，我们可以从视频中提取参与者的图像帧。对于每个参与者，在记录数据之前，要求他/她先点击9个点，熟悉数据采集系统。接下来，50个点将在每个环节依次显示给参与者，以进行数据采集。当参与者成功点击上一个点后，屏幕会闪烁，下一个点会显示出来。每个参与者被要求点击12期(总共点击600个点)。在两个周期之间，我们设置了1分钟的休息时间，以避免眼睛疲劳。</p><p>我们总共招募了137名学生参与者进行数据收集(年龄在20 — 24岁之间，男性98名，女性39名)。所有参与者视力正常或矫正至正常。在去除白点与蓝点之间距离大于阈值的数据样本后，为每个参与者保留约450-600个点及其对应的眼睛和面孔图像。最后ShanghaiTechGaze数据集由233 796张图像组成。我们进一步使用100个参与者对应的图像作为训练集，其余37个参与者对应的图像作为测试集。</p><h2 id="数据组织格式"><a href="#数据组织格式" class="headerlink" title="数据组织格式"></a>数据组织格式</h2><p>—|dataset</p><p>——|annotations</p><p>———|txtfile</p><p>————|test_txt</p><p>—————|leftcamera</p><p>——————|eyelocation.txt    (images/face_landmarks/leftcamera/00109/00000.mat-images/face_landmarks/leftcamera/00147/00599.mat)21301rows</p><p>——————|lefteye.txt    (images/Single_eyes/leftcamera/00109/00000_left.jpg-images/Single_eyes/leftcamera/00147/00599_left.jpg)</p><p>——————|righteye.txt    (images/Single_eyes/leftcamera/00109/00000_right.jpg)</p><p>—————|middlecamera</p><p>——————|eyelocation.txt</p><p>——————|lefteye.txt</p><p>——————|righteye.txt</p><p>—————|rightcamera</p><p>——————|eyelocation.txt</p><p>——————|lefteye.txt</p><p>——————|righteye.txt</p><p>—————|gt.txt    (images/coordinate/00109/00000.mat-images/coordinate/00147/00599.mat)</p><p>————|train_txt</p><p>(images/coordinate/00004/00000.mat-images/coordinate/00108/00599.mat)56631rows</p><p>——|images</p><p>———|coordinate </p><p>————|candidate_index</p><p>—————|00000.mat-00599.mat 存放2维数据，真值。</p><p>———|face_landmarks（eyelocation，candidate_index，00000.mat-00599.mat）</p><p>————|leftcamera存放24维数据</p><p>————|middlecamera</p><p>————|rightcamera</p><p>———|Single_eyes（eye_patch，candidate_index，00000_left.jpg-00599_left.jpg，00000_right.jpg-00599_right.jpg）</p><p>————|leftcamera</p><p>————|middlecamera</p><p>————|rightcamera</p><blockquote><p>眼睛的landmart位置信息和gt使用.mat格式存储。</p></blockquote><h1 id="实验code"><a href="#实验code" class="headerlink" title="实验code"></a>实验code</h1><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/mmcodeStruct.png" alt="image-20211111170143842"></p><blockquote><p>查看mat格式。能不能清晰的读出来，是否需要下载matlab?</p><p>dataloader的<strong>getitem</strong>函数在enumerate部分</p><p>123行 eyelocation = sio.loadmat(eyelocation_name)[‘eyelocation’]是个24个数字</p><p>eyelocation_name = ’/data/ShanghaiTechGaze/images/face_landmarks/leftcamera/00061/00157.mat’</p><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/eye_location.png" alt="image-20211116170548765"></p><p>124行gt = sio.loadmat(gt_name)[‘xy_gt’]是两个数字</p><p>gt_name = ‘/data/ShanghaiTechGaze/images/coordinate/00061/00157.mat’</p><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/eye_location_in_screen.png" alt="image-20211116170844442"></p><p>数据Normalization 是因为是左眼吗所以这么处理。   </p><p>gt[0] -= W_screen / 2 宽</p><p>​    gt[1] -= H_screen / 2 高</p><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/sub-half-w.png" alt="image-20211116171147547"></p><p>第151行 data, target = (input[‘le’], input[‘re’], input[‘eyelocation’]), input[‘gt’]</p><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/le_re_usage.png" alt="image-20211116172803224"></p></blockquote><h2 id="multi-view-gaze-master-code-Train-Single-View-ST-py"><a href="#multi-view-gaze-master-code-Train-Single-View-ST-py" class="headerlink" title="multi-view-gaze-master/code/Train_Single_View_ST.py"></a>multi-view-gaze-master/code/Train_Single_View_ST.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GazeImageDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, txt_file, txt_dir, transform=<span class="literal">None</span></span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">train_loader, model, criterion, optimizer, epoch</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">test_loader, model, criterion, epoch, minimal_error</span>):</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AverageMeter</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, val, n=<span class="number">1</span></span>):</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_error</span>(<span class="params">output, target</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_checkpoint</span>(<span class="params">state, filename=<span class="string">&#x27;checkpoint.pth.tar&#x27;</span></span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adjust_learning_rate</span>(<span class="params">optimizer, epoch</span>):</span></span><br></pre></td></tr></table></figure><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/实验参数.png" alt="image-20211115131555168"></p><h2 id="multi-view-gaze-master-code-network-gazenet-py"><a href="#multi-view-gaze-master-code-network-gazenet-py" class="headerlink" title="multi-view-gaze-master/code/network/gazenet.py"></a>multi-view-gaze-master/code/network/gazenet.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选对应的resnet</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">single_view</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line"><span class="comment">#在resnet.py中设置好resnet</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#训练过程，向前传递参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, *<span class="built_in">input</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> self.view == <span class="string">&#x27;single&#x27;</span>:</span><br><span class="line">        out = self.single_view(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>128,512.  128,512. 128,128. 128,1152.</p></blockquote><h2 id="single-eye-训练思路"><a href="#single-eye-训练思路" class="headerlink" title="single eye 训练思路"></a>single eye 训练思路</h2><p>在每一个epoch里分别训练和测试，每完成一个epoch训练后，保存权重。</p><p>每一次训练，每一个batch（即每一个iteration）中，左右眼分别过Resnet34，得到长度为512输出，24维landmark过线性层得到长度为128的输出，之后这3个输出concatenate成1152的输出。再过fc（两次linear），输出。</p><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/train0.png" alt="image-20211116212707500"></p><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/train1.png" alt="image-20211117164429587"></p><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h2 id="路径出错"><a href="#路径出错" class="headerlink" title="路径出错"></a>路径出错</h2><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/STpy_101th_row.png" alt="image-20211112150649867"></p><h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/reslut.png" alt="image-20211119155759306"></p><blockquote><p>眼睛的位置是瞳仁的中心？还是眼周做平均</p></blockquote><hr><p><a href="https://github.com/dongzelian/multi-view-gaze">dongzelian/multi-view-gaze: Multi-view gaze estimation (github.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> 视线估计 </category>
          
          <category> 试验记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实验记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gaze大佬主页</title>
      <link href="/2021/10/31/gaze%E5%A4%A7%E4%BD%AC%E4%B8%BB%E9%A1%B5/"/>
      <url>/2021/10/31/gaze%E5%A4%A7%E4%BD%AC%E4%B8%BB%E9%A1%B5/</url>
      
        <content type="html"><![CDATA[<p>Xucong Zhang<a href="https://scholar.google.de/citations?user=lDfmDk4AAAAJ&amp;hl=en">https://scholar.google.de/citations?user=lDfmDk4AAAAJ&amp;hl=en</a></p><p><a href="https://github.com/cvlab-uob/Awesome-Gaze-Estimation">cvlab-uob/Awesome-Gaze-Estimation: Awesome Curated List of Eye Gaze Estimation Paper (github.com)</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>对于视线估计的脆弱性分析</title>
      <link href="/2021/10/29/%E5%AF%B9%E4%BA%8E%E8%A7%86%E7%BA%BF%E4%BC%B0%E8%AE%A1%E7%9A%84%E8%84%86%E5%BC%B1%E6%80%A7%E5%88%86%E6%9E%90/"/>
      <url>/2021/10/29/%E5%AF%B9%E4%BA%8E%E8%A7%86%E7%BA%BF%E4%BC%B0%E8%AE%A1%E7%9A%84%E8%84%86%E5%BC%B1%E6%80%A7%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="Vulnerability-of-Appearance-based-Gaze-Estimation"><a href="#Vulnerability-of-Appearance-based-Gaze-Estimation" class="headerlink" title="Vulnerability of Appearance-based Gaze Estimation"></a>Vulnerability of Appearance-based Gaze Estimation</h1><blockquote><p>2021预印本</p><p>Mingjie Xu1 Haofei Wang2 Yunfei Liu1 Feng Lu1, 2, *<br>1State Key Laboratory of VR Technology and Systems, School of CSE, Beihang University<br>2Peng Cheng Laboratory, Shenzhen, China</p></blockquote><p>评价：基于外观的凝视估计的脆弱性</p><p>针对问题：利用噪声对原始图像进行干扰会混淆凝视估计模型，基于机器学习的方法存在脆弱性。</p><p>本文的目的：尽管扰动后的图像在视觉上与原始图像相似，但凝视估计模型输出的凝视方向是错误的。本文研究了基于外观的注视估计的脆弱性。</p><p>从多个方面系统地描述了该漏洞的特性。</p><ul><li>基于像素的对抗攻击pixel-based adversarial attack、</li><li>基于补丁的对抗攻击patch-based adversarial attack</li><li>防御策略 defense strategy等。</li></ul><p>实现的方法：通过在原始输入中加入对抗性扰动，研究了是否有可能改变预测的注视方向，甚至输出一个特定的注视方向。</p><p><img src="/2021/10/29/%E5%AF%B9%E4%BA%8E%E8%A7%86%E7%BA%BF%E4%BC%B0%E8%AE%A1%E7%9A%84%E8%84%86%E5%BC%B1%E6%80%A7%E5%88%86%E6%9E%90/eyeloc.png" alt="image-20211029144448019"></p><p>refer</p><blockquote><p>眼神凝视是人类交流的重要渠道之一。它表示人眼分型[17,24]、认证[13,19]、显著性预测[31]、目标检测[7]等过程中感兴趣的区域 the region of interest during eye typing<br>[17,24], authentication [13,19], saliency prediction [31],<br>object detection 。</p></blockquote><p>观察到，当只攻击“鼻子”或“嘴”时，系统会产生很大的角度误差。图4(b)和图4(c)显示，发作后注意区域仅为“鼻子”或“嘴”。如果同时攻击“鼻子”和“嘴”，则角度误差会大大降低，攻击后注意区域也会同时转移到“鼻子”和“嘴”。</p><p>但是这个角误差还是比通过攻击其他部分来完成那个大。注意，不是所有的注意力区域都在“眼睛”上，这导致了这样的结果。如果我们同时攻击“眼睛”和“鼻子”，或者同时攻击“眼睛”和“嘴巴”，平均角度误差就会比只攻击“鼻子”和“嘴巴”低得多。在这种情况下，注意力区域在“眼睛”和其他部分。</p><p>Yiwei Bao, Yihua Cheng, Y unfei Liu, and Feng Lu. Adaptive<br>feature fusion network for gaze tracking in mobile tablets. In<br>25th International Conference on Pattern Recognition (ICPR),<br>2020.</p><p>Yihua Cheng, Shiyao Huang, Fei Wang, Chen Qian, and Feng<br>Lu. A coarse-to-fine adaptive network for appearance-based<br>gaze estimation. InProceedings of the AAAI Conference on<br>Artificial Intelligence, volume 34, pages 10623–10630, 2020.</p><p>Seonwook Park, Shalini De Mello, Pavlo Molchanov, Umar<br>Iqbal, Otmar Hilliges, and Jan Kautz. Few-shot adaptive gaze<br>estimation. InProceedings of the IEEE/CVF International<br>Conference on Computer Vision, pages 9368–9377, 2019.</p><p>Tobias Fischer, Hyung Jin Chang, and Yiannis Demiris. RT-<br>GENE: Real-Time Eye Gaze Estimation in Natural Environ-<br>ments. InEuropean Conference on Computer Vision, pages<br>339–357, September 2018.</p><p>Yihua Cheng, Xucong Zhang, Feng Lu, and Y oichi Sato.<br>Gaze estimation by exploring two-eye asymmetry.IEEE<br>Transactions on Image Processing, 29:5259–5272, 2020.</p><hr>]]></content>
      
      
      <categories>
          
          <category> 视线估计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习trick</title>
      <link href="/2021/10/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B0%8F%E7%9F%A5%E8%AF%86/"/>
      <url>/2021/10/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B0%8F%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<h1 id="Batch-Normalization加速训练"><a href="#Batch-Normalization加速训练" class="headerlink" title="Batch Normalization加速训练"></a>Batch Normalization加速训练</h1><p><a href="https://blog.csdn.net/gg_18826075157/article/details/78019622">(6条消息) Batch Normalization：加速神经网络训练的通用手段_hyman.lu-CSDN博客</a></p><h2 id="1-基本的一些防止梯度消失、梯度爆炸的方法包括："><a href="#1-基本的一些防止梯度消失、梯度爆炸的方法包括：" class="headerlink" title="1.基本的一些防止梯度消失、梯度爆炸的方法包括："></a>1.基本的一些防止梯度消失、梯度爆炸的方法包括：</h2><ol><li>换用其他激活函数（比如ReLU）</li><li>预学习得到神经网络的初始参数</li><li>降低学习速率</li><li>限制神经网络的学习自由度（比如Dropout）</li></ol><p>更进一步地，Batch Normalization的普适性要更强一些，能使整个学习过程平稳化，从而达到加快学习的效果。</p><h2 id="2-内部协变量位移（Internal-Covariate-Shift）"><a href="#2-内部协变量位移（Internal-Covariate-Shift）" class="headerlink" title="2.内部协变量位移（Internal Covariate Shift）"></a>2.内部协变量位移（Internal Covariate Shift）</h2><p>首先我要先理解协变量位移（Covariate Shift），它一般是指在机器学习和模式识别领域，采样得到的训练集数据的特征值分布通常跟最终的预测数据的特征值分布存在一定的偏差，因此导致算法模型的最终预测效果会产生或多或少的下降。</p><p>然而，我们今天要讲解的重点是内部协变量位移（Internal Covariate Shift），则是指深度学习领域进行神经网络模型学习时，常使用随机小批量梯度下降算法，此时每一批从训练集采样得到的数据它们的特征值分布也会存在差异。而这些差异会随着神经网络层数不断的深入而不断增大，这就使得整个神经网络（尤其是饱和非线性的部分）的参数难以收敛。</p><h2 id="3-白化（whitening）"><a href="#3-白化（whitening）" class="headerlink" title="3.白化（whitening）"></a>3.白化（whitening）</h2><p>所谓白化，其实就是先进行PCA，求出新特征空间中X的新坐标，然后再对新的坐标进行方差归一化操作。</p><p>对于输入数据集X，经过白化处理后，新的数据X’满足两个性质：<br>(1)特征之间相关性较低；<br>(2)所有特征具有相同的方差。</p><p>该算法常用于图像处理当中，由于图像中相邻像素之间具有很强的相关性（一个像素的颜色值往往跟它周围的像素的颜色值十分接近），因此常使用白化对输入的像素特征向量进行白化，从而降低输入的冗余性。</p><p>同样地，白化也能起到抑制内部协变量位移（Internal Covariate Shift）的作用。我们接下来要详细介绍的Batch Normalization跟白化有点异曲同工之妙。</p><h2 id="5-Batch-Normalization的好处"><a href="#5-Batch-Normalization的好处" class="headerlink" title="5.Batch Normalization的好处"></a>5.Batch Normalization的好处</h2><h3 id="①可以使用较大的学习速率"><a href="#①可以使用较大的学习速率" class="headerlink" title="①可以使用较大的学习速率"></a>①可以使用较大的学习速率</h3><p>在一般的深度学习中，如果强行提高学习速率，会因为每层网络都会对参数的梯度进行缩放，从而导致梯度消失/梯度爆炸。而使用了Batch Normalization之后，每一次的放缩将不会相互叠加，从而可以大胆地使用更大的学习速率，而不用担心引起梯度消失/梯度爆炸。</p><h3 id="②带有正则化的效果"><a href="#②带有正则化的效果" class="headerlink" title="②带有正则化的效果"></a>②带有正则化的效果</h3><p>之前，深度学习防止过拟合的最常用的两种方法包括L2正则化和Dropout，但它们都会使模型的训练所需时间增加，而且往往会增加一定的调参工作。而Batch Normalization则可以很好地规避掉这个问题</p><h3 id="③不受网络参数初始值的影响"><a href="#③不受网络参数初始值的影响" class="headerlink" title="③不受网络参数初始值的影响"></a>③不受网络参数初始值的影响</h3><p>对于一些特定分布的数值（比如像素一般位0-255），模型的训练时间和最终训练效果将十分依赖于网络参数初始值。但是经过Batch Normalization归一化处理后，我们就不需要针对某一维度的数据进行网络参数初始值进行精心调优，统一使用标准正态分布随机初始化即可。</p><h1 id="inchannel-outchannel"><a href="#inchannel-outchannel" class="headerlink" title="inchannel outchannel"></a>inchannel outchannel</h1><p>卷积核的层数和卷积核的个数分别对应in channel 和 out channel的大小，stride决定对于特征图H，W大小的改变。设计kernel size，与感受也有关。</p><h1 id="decconvolution"><a href="#decconvolution" class="headerlink" title="decconvolution"></a>decconvolution</h1><p><a href="https://www.youtube.com/watch?v=Tk5B4seA-AU&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&amp;index=26">ML Lecture 16: Unsupervised Learning - Auto-encoder - YouTube</a>32分钟之后</p><p><img src="/2021/10/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B0%8F%E7%9F%A5%E8%AF%86/deconv.png" alt="image-20211025130057963"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习知识 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>RGBD based gaze试验记录</title>
      <link href="/2021/10/18/RGBD-based-gaze%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95/"/>
      <url>/2021/10/18/RGBD-based-gaze%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h1><h2 id="1-ImportError-cannot-import-name-‘DtypeArg’-from-‘pandas-typing’"><a href="#1-ImportError-cannot-import-name-‘DtypeArg’-from-‘pandas-typing’" class="headerlink" title="1.ImportError: cannot import name ‘DtypeArg’ from ‘pandas._typing’"></a>1.ImportError: cannot import name ‘DtypeArg’ from ‘pandas._typing’</h2><p><a href="https://www.5axxw.com/questions/content/m7rq6b">ImportError:无法从’pandas导入名称“DtypeArg” - 我爱学习网 (5axxw.com)</a></p><p>使用pip list查看</p><p>卸载重装即可。</p><h2 id="2-File-“-home-workspace-feifeizhang-anconda-lib-python3-7-site-packages-pandas-core-indexes-base-py”-line-3363-in-get-loc-raise-KeyError-key-from-err-KeyError-‘left-eye-coord’"><a href="#2-File-“-home-workspace-feifeizhang-anconda-lib-python3-7-site-packages-pandas-core-indexes-base-py”-line-3363-in-get-loc-raise-KeyError-key-from-err-KeyError-‘left-eye-coord’" class="headerlink" title="2.  File “/home/workspace/feifeizhang/anconda/lib/python3.7/site-packages/pandas/core/indexes/base.py”, line 3363, in get_loc raise KeyError(key) from err KeyError: ‘left_eye_coord’"></a>2.  File “/home/workspace/feifeizhang/anconda/lib/python3.7/site-packages/pandas/core/indexes/base.py”, line 3363, in get_loc raise KeyError(key) from err KeyError: ‘left_eye_coord’</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.le_coord_list = (root_dir + <span class="string">&quot;/&quot;</span> + self.anno[<span class="string">&quot;left_eye_coord&quot;</span>]).tolist()</span><br><span class="line">self.re_coord_list = (root_dir + <span class="string">&quot;/&quot;</span> + self.anno[<span class="string">&quot;right_eye_coord&quot;</span>]).tolist()</span><br></pre></td></tr></table></figure><p>源代码中这两行被注释掉了，但是如果注释掉会出现</p><pre><code>le_coor = np.load(self.le_coord_list[idx])</code></pre><p>AttributeError: ‘GazePointAllDataset’ object has no attribute ‘le_coord_list’</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>已经发现了数据组织结构的问题。看代码gaze_dataset.py发现数据组织格式有问题，运行GAZE/RG-BD-Gaze-master/code/data/data_check.py有</p><blockquote><p>Traceback (most recent call last):<br>File “/home/workspace/feifeizhang/GAZE/RGBD-Gaze-master/code/data/data_check.py”, line 1, in <module><br>from data.gaze_dataset import GazePointAllDataset<br>ModuleNotFoundError: No module named ‘data’</module></p><p>看一下cord的.npy文件，读出来。或者发邮件吧。</p></blockquote><h2 id="3-RuntimeError-expand-torch-cuda-FloatTensor-2-1-size-the-number-of-sizes-provided-0-must-be-greater-or-equal-to-the-number-of-dimensions-in-the-tensor-2"><a href="#3-RuntimeError-expand-torch-cuda-FloatTensor-2-1-size-the-number-of-sizes-provided-0-must-be-greater-or-equal-to-the-number-of-dimensions-in-the-tensor-2" class="headerlink" title="3.RuntimeError: expand(torch.cuda.FloatTensor{[2, 1]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (2)"></a>3.RuntimeError: expand(torch.cuda.FloatTensor{[2, 1]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (2)</h2><p>Traceback (most recent call last):<br>  File “/home/workspace/feifeizhang/RGBDgaze/RGBD-Gaze-master/code/trainer_aaai.py”, line 666, in <module><br>    trainer.train_base(epochs= total_epochs, lr=learning_rate,use_refined_depth=True)<br>  File “/home/workspace/feifeizhang/RGBDgaze/RGBD-Gaze-master/code/trainer_aaai.py”, line 118, in train_base<br>    self._train_base_epoch()<br>  File “/home/workspace/feifeizhang/RGBDgaze/RGBD-Gaze-master/code/trainer_aaai.py”, line 411, in _train_base_epoch<br>    left_eye_info[j, 2] = th.median(cur_depth).item() * face_factor<br>RuntimeError: expand(torch.cuda.FloatTensor{[2, 1]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (2)</module></p><blockquote><p>非常奇怪的是，当我把batchsize设置为1时，此问题消失了orz</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.temps.use_refined_depth:</span><br><span class="line">    <span class="keyword">with</span> th.no_grad():</span><br><span class="line">        left_eye_bbox[:, :<span class="number">2</span>] -= face_bbox[:, :<span class="number">2</span>]</span><br><span class="line">        left_eye_bbox[:, <span class="number">2</span>:] -= face_bbox[:, :<span class="number">2</span>]</span><br><span class="line">        right_eye_bbox[:, :<span class="number">2</span>] -= face_bbox[:, :<span class="number">2</span>]</span><br><span class="line">        right_eye_bbox[:, <span class="number">2</span>:] -= face_bbox[:, :<span class="number">2</span>]<span class="comment">#减去坐上坐标，相当于是得到在face中眼睛的位置</span></span><br><span class="line">        left_eye_bbox = th.clamp(face_factor * left_eye_bbox, <span class="built_in">min</span>=<span class="number">0</span>, <span class="built_in">max</span>=<span class="number">223</span>)<span class="comment">#用斜坡函数</span></span><br><span class="line">        right_eye_bbox = th.clamp(face_factor * right_eye_bbox, <span class="built_in">min</span>=<span class="number">0</span>, <span class="built_in">max</span>=<span class="number">223</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j, lb <span class="keyword">in</span> <span class="built_in">enumerate</span>(left_eye_bbox):</span><br><span class="line">        <span class="comment">#left_eye_bbox torchsize[2,4]</span></span><br><span class="line">        <span class="comment">#refined_depth torchsize[2,1,224,224]</span></span><br><span class="line">        cur_depth = refined_depth[j, :, <span class="built_in">int</span>(lb[<span class="number">1</span>]):<span class="built_in">int</span>(lb[<span class="number">3</span>]), <span class="built_in">int</span>(lb[<span class="number">0</span>]):<span class="built_in">int</span>(lb[<span class="number">2</span>])]</span><br><span class="line">        <span class="comment">#cur_depth[1,86,86]</span></span><br><span class="line">        left_eye_info[j, <span class="number">2</span>] = th.median(cur_depth).item() * face_factor</span><br><span class="line">        <span class="comment">#left_eye_info  torchsize[2,3]</span></span><br><span class="line">    <span class="keyword">for</span> j, rb <span class="keyword">in</span> <span class="built_in">enumerate</span>(right_eye_bbox):</span><br><span class="line">        cur_depth = refined_depth[j, :, <span class="built_in">int</span>(rb[<span class="number">1</span>]):<span class="built_in">int</span>(rb[<span class="number">3</span>]), <span class="built_in">int</span>(rb[<span class="number">0</span>]):<span class="built_in">int</span>(rb[<span class="number">2</span>])]</span><br><span class="line">        right_eye_info[j, <span class="number">2</span>] = th.median(cur_depth).item() * face_factor</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">        <span class="comment">#打印出维度，查看结果</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;the shape of depth:&quot;</span>,cur_depth.shape)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;the shape of left_eye_info :&quot;</span>,left_eye_info.shape)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;left_eye_info[j, 2]&quot;</span>,left_eye_info[j, <span class="number">2</span>])</span><br><span class="line">        a= th.median(cur_depth).item() * face_factor</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;median depth:&quot;</span>,a)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;median shape&quot;</span>,a.shape)</span><br><span class="line">&gt;&gt;the shape of depth: torch.Size([<span class="number">1</span>, <span class="number">89</span>, <span class="number">90</span>])</span><br><span class="line">&gt;&gt;the shape of left_eye_info : torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">&gt;&gt;left_eye_info[j, <span class="number">2</span>] tensor(<span class="number">0.4084</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">&gt;&gt;median depth: tensor([[<span class="number">0.2566</span>],</span><br><span class="line">        [<span class="number">0.3694</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">&gt;&gt;median shape torch.Size([<span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><blockquote><p>the shape of depth: torch.Size([1, 88, 88])<br>the shape of left_eye_info : torch.Size([16, 3])<br>left_eye_info[j, 2] tensor(0.9088, device=’cuda:0’)<br>median depth: tensor([[0.6016],<br>        [0.6016],<br>        [0.5006],<br>        [0.5006],<br>        [0.6016],<br>        [0.6016],<br>        [0.5006],<br>        [0.5025],<br>        [0.5006],<br>        [0.4179],<br>        [0.4179],<br>        [0.5025],<br>        [0.3485],<br>        [0.5006],<br>        [0.4179],<br>        [0.5025]], device=’cuda:0’)<br>median shape torch.Size([16, 1])<br>Traceback (most recent call last):<br>  File “/home/workspace/feifeizhang/RGBDgaze/RGBD-Gaze-master/code/trainer_aaai.py”, line 676, in <module><br>    trainer.train_base(epochs= total_epochs, lr=learning_rate,use_refined_depth=True)<br>  File “/home/workspace/feifeizhang/RGBDgaze/RGBD-Gaze-master/code/trainer_aaai.py”, line 119, in train_base<br>    self._train_base_epoch()<br>  File “/home/workspace/feifeizhang/RGBDgaze/RGBD-Gaze-master/code/trainer_aaai.py”, line 419, in _train_base_epoch<br>    left_eye_info[j, 2] = th.mean(a).item() * face_factor<br>RuntimeError: expand(torch.cuda.FloatTensor{[16, 1]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (2)</module></p></blockquote><p>找到原因啦！其实是face_factor是这个batch的face_factor</p><h1 id="程序结构"><a href="#程序结构" class="headerlink" title="程序结构"></a>程序结构</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p><img src="/2021/10/18/RGBD-based-gaze%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95/codedir.png" alt="image-20211019145428957"></p><p><img src="/2021/10/18/RGBD-based-gaze%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95/codedir1.png" alt="image-20211019154913500"></p><p><img src="/2021/10/18/RGBD-based-gaze%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95/codedir2.png" alt="image-20211025165956657"></p><blockquote><p>gen landmark.py</p></blockquote><h2 id="trainer-aaai-py"><a href="#trainer-aaai-py" class="headerlink" title="trainer_aaai.py"></a>trainer_aaai.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GazeTrainer</span>(<span class="params">Trainer</span>):</span></span><br><span class="line"><span class="comment">#继承Trainer类</span></span><br><span class="line"></span><br><span class="line">self.weights_init(self.models.decoder)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_base</span>(<span class="params">self, epochs, lr=<span class="number">1e-4</span>, use_refined_depth=<span class="literal">False</span>, fine_tune_headpose=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_headpose</span>(<span class="params">self, epochs, lr=<span class="number">2e-4</span>, lambda_loss_mse=<span class="number">1</span></span>):</span></span><br><span class="line">        self.temps.headpose_logger = self.logger.getChild(<span class="string">&#x27;train_headpose&#x27;</span>)</span><br><span class="line">        self.temps.headpose_logger.info(<span class="string">&#x27;preparing for headpose training loop.&#x27;</span>)</span><br><span class="line">        self.temps.headpose_logger = self.logger.getChild(<span class="string">&#x27;train_headpose&#x27;</span>)</span><br><span class="line">        self.temps.headpose_logger.info(<span class="string">&#x27;preparing for headpose training loop.&#x27;</span>)</span><br><span class="line">        <span class="comment">#没看明白此处log来自于哪里？</span></span><br><span class="line">        <span class="comment"># prepare logger</span></span><br><span class="line">        <span class="comment"># prepare dataloader 调用def _get_trainloader(self):</span></span><br><span class="line">        <span class="comment"># start training loop</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">resume</span>(<span class="params">self, filename</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prepare_model</span>(<span class="params">self, model, train=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_trainloader</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment">#data_transforms，</span></span><br><span class="line">        <span class="comment">#调用        transformed_train_dataset = GazePointAllDataset(root_dir=self.data_root,transform=data_transforms[&#x27;train&#x27;],phase=&#x27;train&#x27;,face_image=True, face_depth=True, eye_image=True,eye_depth=True,info=True, eye_bbox=True, face_bbox=True, eye_coord=True)</span></span><br><span class="line">        <span class="comment">#参数有faceimage、depth、bbox，eye_image、eye_depth、eye_bbox、eye_coord）</span></span><br><span class="line">        <span class="comment">#调用gaze_dataset.py   </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_valloader</span>(<span class="params">self</span>):</span></span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_init_base_meters</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_headpose_meters</span>(<span class="params">self</span>):</span></span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_plot_base</span>(<span class="params">self</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_plot_headpose</span>(<span class="params">self</span>):</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_log_base</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_log_headpose</span>(<span class="params">self</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_train_base_epoch</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_train_headpose_epoch</span>(<span class="params">selfs</span>):</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_test_base</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_test_headpose</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    trainer = GazeTrainer(</span><br><span class="line">        exp_name=<span class="string">&quot;gaze_aaai_refine_headpose&quot;</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>在调用GazeTrainer类后会继承父类Trainer，父类位于文件下GAZE/RGBD-Gaze-master/code/utils/trainer.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, checkpoint_dir=<span class="string">&#x27;./&#x27;</span>, is_cuda=<span class="literal">True</span></span>):</span><span class="comment">#初始化</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure><h2 id="GAZE-RGBD-Gaze-master-code-utils-edict-py"><a href="#GAZE-RGBD-Gaze-master-code-utils-edict-py" class="headerlink" title="GAZE/RGBD-Gaze-master/code/utils/edict.py"></a>GAZE/RGBD-Gaze-master/code/utils/edict.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对于字典的编辑</span></span><br></pre></td></tr></table></figure><h2 id="gaze-aaai-py"><a href="#gaze-aaai-py" class="headerlink" title="gaze_aaai.py"></a>gaze_aaai.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet34</span>(<span class="params">pretrained=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment">#左右眼decoder</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DepthBCE</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RefineDepth</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    self.depth_block4 </span><br><span class="line"><span class="comment">#faceblock1、2、3、4，depthblock1、2、3、4</span></span><br><span class="line">    <span class="comment">#每一层输出的向量维度一样，卷积，bn，relu 。in3，每一个block out 64、128、256、512</span></span><br><span class="line">    self.down4 </span><br><span class="line">    <span class="comment">#4个down降低维度层数，in1024，out512，256，128，64，卷积、bn、rule，第一个down再relu之后加了resnet.</span></span><br><span class="line">    self.head_pose</span><br><span class="line">    <span class="comment">#in512，out1024，128</span></span><br><span class="line">    self.gen_block1 </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>down层输入为1024，感觉是两个512的feature级联起来的。</p></blockquote><h2 id="GAZE-RGBD-Gaze-master-code-data-gaze-dataset-py"><a href="#GAZE-RGBD-Gaze-master-code-data-gaze-dataset-py" class="headerlink" title="GAZE/RGBD-Gaze-master/code/data/gaze_dataset.py"></a>GAZE/RGBD-Gaze-master/code/data/gaze_dataset.py</h2><p><strong>27英寸，长60厘米，宽34厘米</strong>（精确值：59.77厘米，33.62厘米）</p><p>在trainer_aaai.py调用时候</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">transformed_train_dataset = GazePointAllDataset(root_dir=self.data_root,transform=data_transforms[<span class="string">&#x27;train&#x27;</span>],phase=<span class="string">&#x27;train&#x27;</span>,face_image=<span class="literal">True</span>, face_depth=<span class="literal">True</span>, eye_image=<span class="literal">True</span>,eye_depth=<span class="literal">True</span>,info=<span class="literal">True</span>, eye_bbox=<span class="literal">True</span>, face_bbox=<span class="literal">True</span>, eye_coord=<span class="literal">True</span>)</span><br><span class="line">       <span class="comment">#参数有face_image、depth、bbox，eye_image、eye_depth、eye_bbox、face_bbox、info、eye_coord）</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GazePointAllDataset</span>(<span class="params">data.Dataset</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, root_dir, w_screen=<span class="number">59.77</span>, h_screen=<span class="number">33.62</span>, transform=<span class="literal">None</span>, phase=<span class="string">&quot;train&quot;</span>, **kwargs</span>):</span></span><br></pre></td></tr></table></figure><p>程序错误：数据集train_csv没有left_eye_coord、lift_eye_coord,感觉是这个类调用有问题，或者是类的构造有问题。导致加载数据出错。</p><h1 id="数据集结构"><a href="#数据集结构" class="headerlink" title="数据集结构"></a>数据集结构</h1><p>数据集由165231个RGB/depth图像对组成。使用159个参与者对应的图像(119,318个RGB/depth图像对)作为训练数据，使用其余59个参与者对应的数据(45,913个RGB/depth图像对)作为测试数据。</p><p><img src="/2021/10/18/RGBD-based-gaze%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95/shanghaitech_rgb_dataset.png" alt="image-20211025183820769"></p><p>tran_meta.csv包含了119,318个RGB/depth图像对，表头有12项，分别为</p><p>A.图片index。</p><p>B.face_images图片存的位置,位于color文件夹下。</p><p>C.face_depth存的位置，位于projected_depth_calibration。</p><p>D.face_bbox，存于txt文件中，为4个坐标值,位于color文件夹下。</p><p>E.left_eye_image,位于color文件夹下。</p><p>F.right_eye_image,位于color文件夹下。</p><p>G.left_eye_depth，位于projected_depth_calibration下。</p><p>H.right_eye_depth，位于projected_depth_calibration下。</p><p>I.left_eye_bbox,位于color文件夹下。</p><p>J.right_eye_bbox,位于color文件夹下。</p><p>K.gaze_point,位于coordinate文件夹下，文件名以.npy结尾，文件中储存的是坐标。</p><p>L.has_landmark,值为TRUE或者FALSE，大部分值都为TRUE。</p><p>color、projected_depth_calibration、coordinate文件夹均有219个文件夹对应219个志愿者，每个志愿者的文件夹下有多组实验的数据。</p><p>color文件夹下存放了219个志愿者的rgb相关信息，一组实验包含7个信息，全脸、左右眼的图片以及bbox，还有人脸的68点landmark。</p><p><img src="/2021/10/18/RGBD-based-gaze%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95/colordir.png" alt="image-20211025195154291"></p><p>projected_depth_calibration文件夹下存放了219个志愿者的depth相关信息，一组实验包含3个信息，分别为左右眼和全脸depth。</p><p>coordinate文件夹下存放了219个志愿者眼睛坐标系。</p><blockquote><p>少了 le 坐标系，r坐标系。不知道如果这俩怎么在网络中使用？如果拿掉会怎么样。.mat格式里边是不是保存有？python转换一个,mat文件至excel看一下格式吧</p></blockquote><h1 id="读取数据集遇到的问题"><a href="#读取数据集遇到的问题" class="headerlink" title="读取数据集遇到的问题"></a>读取数据集遇到的问题</h1><h2 id="企图通过各个特征维度来获得-le-cord-和ri-cord是啥？-ㄒoㄒ"><a href="#企图通过各个特征维度来获得-le-cord-和ri-cord是啥？-ㄒoㄒ" class="headerlink" title="企图通过各个特征维度来获得 le cord 和ri cord是啥？/(ㄒoㄒ)/~~"></a>企图通过各个特征维度来获得 le cord 和ri cord是啥？/(ㄒoㄒ)/~~</h2><p><img src="/2021/10/18/RGBD-based-gaze%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95/lefeat_rfeat.png" alt="image-20211118165313972"></p><p>resnet 34返回特征 512维度。（经过pooling，每个维度只有一个点）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, face, depth</span>):</span></span><br><span class="line">    face_f1 = self.face_block1(face)<span class="comment">#3-&gt;64卷积、bn、relu</span></span><br><span class="line">    face_f2 = self.face_block2(face_f1)<span class="comment">#64-&gt;128</span></span><br><span class="line">    face_f3 = self.face_block3(face_f2)<span class="comment">#128-&gt;256</span></span><br><span class="line">    face_f4 = self.face_block4(face_f3)<span class="comment">#256-&gt;512  28X28</span></span><br><span class="line">    depth_f1 = self.depth_block1(depth)<span class="comment">#1-&gt;64</span></span><br><span class="line">    depth_f2 = self.depth_block2(depth_f1)<span class="comment">#64-&gt;128</span></span><br><span class="line">    depth_f3 = self.depth_block3(depth_f2)<span class="comment">#128-&gt;256</span></span><br><span class="line">    depth_f4 = self.depth_block4(depth_f3)<span class="comment">#256-&gt;512 28X28</span></span><br><span class="line">    mixed_f4 = self.down1(th.cat([face_f4, depth_f4], dim=<span class="number">1</span>))<span class="comment">#1024-&gt;512</span></span><br><span class="line">    mixed_f3 = self.down2(th.cat([face_f3, depth_f3], dim=<span class="number">1</span>))<span class="comment">#512-&gt;256</span></span><br><span class="line">    mixed_f2 = self.down3(th.cat([face_f2, depth_f2], dim=<span class="number">1</span>))<span class="comment">#256-&gt;128</span></span><br><span class="line">    mixed_f1 = self.down4(th.cat([face_f1, depth_f1], dim=<span class="number">1</span>))<span class="comment">#128-&gt;64 28X28</span></span><br><span class="line">    </span><br><span class="line">    gen_f3 = self.gen_block1(mixed_f4) + mixed_f3<span class="comment">#512-&gt;256反卷积 56X56</span></span><br><span class="line">    gen_f2 = self.gen_block2(gen_f3) + mixed_f2<span class="comment">#256-&gt;128 反卷积112X112</span></span><br><span class="line">    gen_f1 = self.gen_block3(gen_f2) + mixed_f1<span class="comment">#128-&gt;64  反卷积224X224</span></span><br><span class="line">    gen_depth = self.gen_block4(gen_f1)<span class="comment">#64-&gt;1 </span></span><br><span class="line">    <span class="comment">#和论文中的框图不一样，不过维度和 synthesize depth 对上了</span></span><br><span class="line">    </span><br><span class="line">    head_pose = self.head_pose(mixed_f4)<span class="comment">#512,28X28-&gt;512,14X14-&gt;1024,7X7-&gt;1024,1X1-&gt;128,1X1</span></span><br><span class="line">    <span class="keyword">return</span> head_pose.view(head_pose.size(<span class="number">0</span>), -<span class="number">1</span>), gen_depth<span class="comment">#6276. 1,224X224</span></span><br></pre></td></tr></table></figure><h2 id="似乎找到问题的答案啦！"><a href="#似乎找到问题的答案啦！" class="headerlink" title="似乎找到问题的答案啦！"></a>似乎找到问题的答案啦！</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#train_aaai.py</span></span><br><span class="line">                <span class="keyword">for</span> j, lb <span class="keyword">in</span> <span class="built_in">enumerate</span>(left_eye_bbox):</span><br><span class="line">                    cur_depth = refined_depth[j, :, <span class="built_in">int</span>(lb[<span class="number">1</span>]):<span class="built_in">int</span>(lb[<span class="number">3</span>]), <span class="built_in">int</span>(lb[<span class="number">0</span>]):<span class="built_in">int</span>(lb[<span class="number">2</span>])]</span><br><span class="line">                    left_eye_info[j, <span class="number">2</span>] = th.median(cur_depth).item() * face_factor</span><br><span class="line">                <span class="keyword">for</span> j, rb <span class="keyword">in</span> <span class="built_in">enumerate</span>(right_eye_bbox):</span><br><span class="line">                    cur_depth = refined_depth[j, :, <span class="built_in">int</span>(rb[<span class="number">1</span>]):<span class="built_in">int</span>(rb[<span class="number">3</span>]), <span class="built_in">int</span>(rb[<span class="number">0</span>]):<span class="built_in">int</span>(rb[<span class="number">2</span>])]</span><br><span class="line">                    right_eye_info[j, <span class="number">2</span>] = th.median(cur_depth).item() * face_factor</span><br><span class="line">                    </span><br><span class="line"> <span class="comment">#gaze_aaai.py</span></span><br><span class="line">l_coord = self.lcoord(th.cat([l_coord_feat, head_pose, linfo], <span class="number">1</span>))</span><br><span class="line">        r_coord = self.rcoord(th.cat([r_coord_feat, head_pose, rinfo], <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    rinfo最终concatenate的有三个信息。depth均值和眼睛中间点级联了xe，ye</span><br><span class="line">    </span><br></pre></td></tr></table></figure><p>使用眼边缘六个点做平均获得眼睛中心位置。</p><p><img src="/2021/10/18/RGBD-based-gaze%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95/faceimge.png" alt="image-20211122151234305"></p><p><img src="/2021/10/18/RGBD-based-gaze%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95/face_landmark.png" alt="image-20211122151515079"></p><blockquote><p>屏幕分辨率1080（540），1960（980）。对于一个图片矩阵，（0,0）在左上，所以这里显示的landmark图片是upside down的。</p></blockquote><h3 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h3><p>读出保存数据的csv，计算landmark眼周六点的平均值，获得双眼中心，保存至csv。对于landmark不存在的数据，从保存数据的csv中删除。</p><p>测试集没有删除数据，训练集合数据从119317删除至98629。</p><h2 id="程序中对于gt处理，为什么有一个screen-2平移"><a href="#程序中对于gt处理，为什么有一个screen-2平移" class="headerlink" title="程序中对于gt处理，为什么有一个screen/2平移"></a>程序中对于gt处理，为什么有一个screen/2平移</h2><p>w_screen=59.77, h_screen=33.62</p><p>​    gt[0] -= self.w_screen / 2</p><p>​    gt[1] -= self.h_screen / 2</p><blockquote><p>为什么程序有一个减法？相当于平移。</p></blockquote><p><img src="/2021/10/18/RGBD-based-gaze%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95/Blog\source\_posts\RGBD-based-gaze试验记录\location_in_screen.png" alt="image-20211122145542394"></p><p><img src="/2021/10/18/RGBD-based-gaze%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95/color00007_face.jpg" alt></p><p><img src="/2021/10/18/RGBD-based-gaze%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95/color00008_face.jpg" alt="color00008_face"></p><p><img src="/2021/10/18/RGBD-based-gaze%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95/color00009_face.jpg" alt="color00009_face"></p>]]></content>
      
      
      <categories>
          
          <category> 视线估计 </category>
          
          <category> 试验记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实验记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>lab多模态红外</title>
      <link href="/2021/10/04/%E2%80%9Clab%E5%A4%9A%E6%A8%A1%E6%80%81%E7%BA%A2%E5%A4%96%E2%80%9D/"/>
      <url>/2021/10/04/%E2%80%9Clab%E5%A4%9A%E6%A8%A1%E6%80%81%E7%BA%A2%E5%A4%96%E2%80%9D/</url>
      
        <content type="html"><![CDATA[<p>红外与可见光图像融合论文阅读（一） - 奥本海默的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/387858991">https://zhuanlan.zhihu.com/p/387858991</a></p><p>红外热成像 - simple林的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/28895578">https://zhuanlan.zhihu.com/p/28895578</a></p><p>红外图和温感图看起来有啥不同？</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>eye gaze调研</title>
      <link href="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/"/>
      <url>/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/</url>
      
        <content type="html"><![CDATA[<h1 id="Gaze-Estimation"><a href="#Gaze-Estimation" class="headerlink" title="Gaze  Estimation"></a>Gaze  Estimation</h1><blockquote><p>本文的内容转载自知乎用户<a href="https://www.zhihu.com/people/yu-rain-17/posts"> T骨牛排 - 知乎 (zhihu.com)</a>的<a href="https://zhuanlan.zhihu.com/p/112097446">视线估计(Gaze Estimation)简介 - T骨牛排的文章 - 知乎</a>的系列文章。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>广义的Gaze Estimation 泛指与<strong>眼球</strong>、<strong>眼动、视线</strong>等相关的研究。</p><blockquote><p>不少做saliency和egocentric的论文也以gaze为关键词。</p><p>近些年随着数据和技术的发展，对gaze的需求渐渐浮出水面，这方面的研究也开始进入主流的视野。</p></blockquote><p><img src="https://pic4.zhimg.com/80/v2-5d28d5b34067a60edb6b0b56b7ea45bf_1440w.jpg" alt="img"></p><h2 id="根据不同的场景与应用大致可分为三类"><a href="#根据不同的场景与应用大致可分为三类" class="headerlink" title="根据不同的场景与应用大致可分为三类"></a>根据不同的场景与应用大致可分为三类</h2><p>注视目标估计、注视点估计以及三维视线估计。</p><p><img src="https://pic2.zhimg.com/80/v2-502dd1b1678d3bb84d2d51c7955038b5_1440w.jpg" alt="img"></p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>Tobii CEO：AR/VR的未来形态将广泛结合眼球追踪<a href="https://www.bilibili.com/read/cv13178442?from=articleDetail&amp;spm_id_from=333.976.b_726561645265636f6d6d656e64496e666f.2">Tobii CEO：AR/VR的未来形态将广泛结合眼球追踪 - 哔哩哔哩 (bilibili.com)</a></p><h3 id="用Tobii眼动仪玩游戏的Demo"><a href="#用Tobii眼动仪玩游戏的Demo" class="headerlink" title="用Tobii眼动仪玩游戏的Demo"></a>用Tobii眼动仪玩游戏的Demo</h3><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/tobii.png" alt></p><h3 id="VR"><a href="#VR" class="headerlink" title="VR"></a>VR</h3><p>VR头盔。现阶段VR的问题是全场景精细渲染对硬件要求较高导致硬件成本居高不下。如果能够通过头盔内置摄像头准确估计人的视线方向，则可以对场景做局部精细渲染，即仅对人注视范围内的场景精细渲染，从而大大降低硬件成本。</p><h3 id="医疗"><a href="#医疗" class="headerlink" title="医疗"></a>医疗</h3><p>gaze在医疗方面的应用主要是两类。一类是用于检测和诊断精神类或心理类的疾病。一个典型例子是自闭症儿童往往表现出与正常儿童不同的gaze行为与模式。另一类是通过基于gaze的交互系统来为一些病人提供便利。如渐冻症患者可以使用眼动仪来完成一些日常活动。</p><h3 id="辅助驾驶（智能座舱）"><a href="#辅助驾驶（智能座舱）" class="headerlink" title="辅助驾驶（智能座舱）"></a>辅助驾驶（智能座舱）</h3><p>gaze在辅助驾驶上有两方面应用。一是检测驾驶员是否疲劳驾驶以及注意力是否集中。二是提供一些交互从而解放双手。</p><h3 id="线下零售"><a href="#线下零售" class="headerlink" title="线下零售"></a>线下零售</h3><p>人的注意力某种程度上反映了其兴趣，可以提供大量的信息。但是目前并没有看到相关的应用，包括Amazon Go。或许现阶段精度难以达到要求。可以通过gaze行为做市场调研。</p><h3 id="其他交互类应用"><a href="#其他交互类应用" class="headerlink" title="其他交互类应用"></a>其他交互类应用</h3><p>如手机解锁、短视频特效等。</p><h2 id="相关团队与公司"><a href="#相关团队与公司" class="headerlink" title="相关团队与公司"></a>相关团队与公司</h2><p>ETH的Otmar Hilliges教授和东京大学的Yusuke Sugano教授。</p><p>- EPFL与Idiap的感知组：<a href="https://link.zhihu.com/?target=https%3A//www.idiap.ch/~odobez/">https://www.idiap.ch/~odobez/</a></p><ul><li>Improving Few-Shot User-Specific Gaze Adaptation via Gaze Redirection Synthesis, CVPR 2019</li><li>Unsupervised Representation Learning for Gaze Estimation, CVPR 2020</li><li>A Differential Approach for Gaze Estimation, PAMI accepted 2019</li></ul><p>- ETH交互组：<a href="https://link.zhihu.com/?target=https%3A//ait.ethz.ch/people/hilliges/">https://ait.ethz.ch/people/hill</a></p><p>- 德国马普所交互组：<a href="https://link.zhihu.com/?target=https%3A//perceptualui.org/people/bulling/">https://perceptualui.org/people</a></p><p>- MIT Antonio Torralba组：<a href="https://link.zhihu.com/?target=http%3A//web.mit.edu/torralba/www/">http://web.mit.edu/torralba/www/</a></p><p>- 伦斯勒理工Qiang Ji组：<a href="https://link.zhihu.com/?target=https%3A//www.ecse.rpi.edu/~qji/">https://www.ecse.rpi.edu/~qji/</a></p><p>- 东京大学Sugano组：<a href="https://link.zhihu.com/?target=https%3A//www.yusuke-sugano.info/">https://www.yusuke-sugano.info/</a></p><p>- 北航Feng Lu组：<a href="https://link.zhihu.com/?target=http%3A//phi-ai.org/default.htm">http://phi-ai.org/default.htm</a></p><p>工业界方面，目前主力依旧在欧美。大公司，如Facebook Reality Lab（去年组织举办了第一届gaze相关的challenge）， 微软Hololens，谷歌广告，NVIDIA自动驾驶等团队都在致力于gaze方面的研究。而专注于gaze的中小型公司，龙头老大当属瑞典公司Tobii，其眼动仪已臻物美价廉之境。另外也可以关注下瑞士创业公司eyeware。</p><h1 id="注视目标估计"><a href="#注视目标估计" class="headerlink" title="注视目标估计"></a>注视目标估计</h1><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/objgaze.png" alt></p><p>注视目标估计英文关键词为gaze following，即检测给定人物所注视的目标。MIT Antonio Torralba组最先提出了这一问题并公开了相关数据集[1]。</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/mitgazefollow.png" alt></p><blockquote><p>我们引入了一个新的数据集，用于在自然图像中跟踪视线。在左边，我们展示了几个示例注释和图像。在右边的图中，我们总结了一些关于数据集测试分区的统计信息。前三张热图显示了头部位置、固定位置和固定位置相对于头部位置归一化的概率密度。下图显示了不同头部位置的平均凝视方向。</p></blockquote><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/whataretheylooking" alt="image-20211005193952263"></p><center><p>Where are they looking?</p></center><p>网络主要由两个支路组成，一个支路（Saliency Pathway）以原始图片为输入，用于显著性检测，输出为反映显著性的heat map。另一个支路（Gaze Pathway）以某一个人的头部图片和头部位置为输入，用于检测这个人可能的注视区域，输出同样是一个heat map。这两个heat map的乘积反映了目标显著性与可能的注视区域的交集，即可能的注视目标。整个网络以端对端的方式训练。以AUC为指标，文章最后得到了0.878的精度。</p><p>这种结构设计也适用于多人注视目标的检测。只需要将Gaze Pathway中的头部图片与位置更换为另一个人的即可。然而这种方案的一大局限是，人与其注视的目标必须同时出现在同一张图片中。这大大限制了其应用范围。</p><p>作者们在ICCV 2017提出了针对视频的跨帧注视目标检测[2]，即人与注视目标可出现在不同视频帧中，如下图所示。</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/tomhanksvedio1" alt="image-20211005203207646"></p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/tomhanksvedio2" alt="image-20211005203432979"></p><center><p>Following Gaze in Video</p></center><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/gazefollow_vedio.png" alt="image-20211005204012006"></p><p>整个框架由三条支路构成。与之前框架相比，现在的方案增加了一个Transformation Pathway，用于估计source frame（人所在帧）与target frame（目标所在帧）的几何变换。而现在的Gaze Pathway则用于估计一个视锥的参数。这两路网络的输出表示source frame中的人可能注视的target frame区域。下面的图更为直观。</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/vedioprject.png" alt="image-20211005204541675"><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211005204913343.png" alt="image-20211005204913343"><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/vedio_heatmap.png" alt="image-20211005205031681"></p><blockquote><p>注视点估计的相关工作（这个方向的论文同样不多），然后再介绍gaze领域的重点研究对象，三维视线估计。</p></blockquote><h1 id="注视点估计"><a href="#注视点估计" class="headerlink" title="注视点估计"></a>注视点估计</h1><p>注视点估计即估算人双目视线聚焦的落点。</p><p>其一般场景是估计人在一个二维平面上的注视点。</p><p>这个二维平面可以是手机屏幕，pad屏幕和电视屏幕等，而模型输入的图像则是这些设备的前置摄像头。</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/mitEyeTrackingforEveryone.png" alt="这里放图片显示不出的时候出现的文字" style="zoom:这里写缩放的百分比，比如:30%"></p><center><p>Eye Tracking for Everyone MIT 2016CVPR</p></center><p>这个工作同样来自MIT Antonio Torralba组。</p><p>他有四个输入，左眼图像、右眼图像、人脸图像（由iPhone拍照软件检测）以及人脸位置。</p><p>四种输入由四条支路（眼睛图像的支路参数共享）分别处理，融合后输出得到一个二维坐标位置。</p><p>实验表明，模型在iPhone上的误差是1.71cm，而在平板上的误差是2.53cm（误差为标注点与估计点之间的欧式距离）。该工作收集并公布了一个<strong>涵盖1400多人、240多万样本</strong>的数据集， <strong>GazeCapture</strong>。</p><p>人脸主要提供头部姿态信息（head pose），而人脸位置主要提供眼睛位置信息。这里存在一定的信息冗余。基于这一观察，Google对上述模型做了进一步压缩，即将人脸和人脸位置这两个输入替换为四个眼角的位置坐标，如下图所示。</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/ONdecvice2019CVPRW.png" alt="image-20211006143655188"></p><center><p>On-device few-shot personalization for real-time gaze estimation Google 2019ICCVW</p></center><p>眼角位置坐标不仅直接提供了眼睛位置信息，同时又暗含head pose信息（眼角间距越小，head pose越大，反之头部越正）。实验结果表明，这个精简后的模型在iPhone上的误差为1.78cm，与原始模型的精度相差无几。同时，该模型在Google Pixel 2 Phone的处理速度达到10ms/帧。</p><p>三星在2019年也公开了相关研究A Generalized and Robust Method<br>Towards Practical Gaze Estimation on Smart Phone.。他们采取的网络架构与[1]类似，不同点是在网络训练过程中加入了distillation与pruning等技巧，来防止过拟合并获得更鲁棒的结果。</p><blockquote><p>蒸馏了什么？</p></blockquote><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/tabletgaze.png" alt="image-20211006160228094"></p><center><p>TabletGaze2015</p></center><p>2015年莱斯大学已公开了一篇针对平板的注视点估计论文TabletGaze[4]。但当时的深度学习还不像今天这样盛行，作者使用了传统特征（LBP、HOG等）+ 统计模型的方式来解决这一问题.</p><h1 id="三维视线估计（通用方法）"><a href="#三维视线估计（通用方法）" class="headerlink" title="三维视线估计（通用方法）"></a>三维视线估计（通用方法）</h1><p>三维视线估计的目标是从眼睛图片或人脸图片中推导出人的视线方向。通常，这个视线方向是由两个角度，<strong>pitch（垂直方向）和 yaw（水平方向）</strong>来表示的，见下图a。需要注意的是，在相机坐标系下，视线的方向<strong>不仅取决于眼睛的状态（眼珠位置，眼睛开合程度等）</strong>，还取决于<strong>头部姿态</strong>（见图b：虽然眼睛相对头部是斜视，但在相机坐标系下，他看的是正前方)。</p><p><img src="https://pic3.zhimg.com/v2-1748477bc2558a9442ff18d8280bb79a_r.jpg" alt="preview"></p><h2 id="传统方法："><a href="#传统方法：" class="headerlink" title="传统方法："></a>传统方法：</h2><p>视线估计方法分为：</p><p>基于几何的方法（Geometry Based Methods）。</p><p>基于外观的方法（Appearance Based Methods）两大类。</p><p>基于几何的方法的基本思想是检测眼睛的一些特征（例如眼角、瞳孔位置等关键点），然后根据这些特征来计算gaze。而基于外观的方法则是<strong>直接学习一个将外观映射到gaze的模型</strong>。</p><p>几何方法相对更准确，且对不同的domain表现稳定，然而这类方法<strong>对图片的质量和分辨率</strong>有很高的要求。</p><blockquote><p>基于几何特征。对于不同的domain稳定，此处的domain指的是？</p></blockquote><p>基于外观的方法对低分辨和高噪声的图像表现更好，但<strong>模型的训练需要大量数据，并且容易对domain overfitting</strong>。</p><blockquote><p>appearance</p></blockquote><p>随着深度学习的崛起以及大量数据集的公开，基于外观的方法越来越受到关注。</p><p><strong>通用（person independent）的视线估计方法</strong>，即模型的训练数据与测试数据采集自不同的人（与之相对的是<strong>个性化视线估计</strong>，即训练数据与测试数据采集自相同的人）。按照方法所依赖的信息，将他们分类为<strong>单眼/双眼视线估计</strong>，<strong>基于语义信息的视线估计</strong>和<strong>全脸视线估计</strong> 三类。</p><h1 id="单眼-双眼视线估计："><a href="#单眼-双眼视线估计：" class="headerlink" title="单眼/双眼视线估计："></a>单眼/双眼视线估计：</h1><h2 id="Appearance-based-gaze-estimation-in-the-wild"><a href="#Appearance-based-gaze-estimation-in-the-wild" class="headerlink" title="Appearance-based gaze estimation in the wild"></a>Appearance-based gaze estimation in the wild</h2><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/2017TAPMI_MPIIGaze_pipeline.png" alt="image-20211007135523674"></p><blockquote><p>多个模态，有哪些模态？  </p></blockquote><p>他们当时使用的是一个类似于LeNet的浅层架构，还称不上“深度”学习。而其中一个有启发性的贡献是，他们将<strong>头部姿态（head pose）信息与提取出的眼睛特征拼接</strong>，用以学习相机坐标系下的gaze。该工作的另一个重要贡献是提出并公开了gaze领域目前最常用的数据集之一：MPIIGaze。在MPIIGaze数据集上，该工作的误差为6.3度。</p><h2 id="Real-World-Dataset-and-Deep-Appearance-Based-Gaze-Estimation"><a href="#Real-World-Dataset-and-Deep-Appearance-Based-Gaze-Estimation" class="headerlink" title="Real-World Dataset and Deep Appearance-Based Gaze Estimation"></a>Real-World Dataset and Deep Appearance-Based Gaze Estimation</h2><p>Xucong Zhang在他2017年的工作中[2]，用VGG16 代替了这个浅层网络，大幅提升了模型精度，将误差缩小到了5.4度。</p><h2 id><a href="#" class="headerlink" title=" "></a> </h2><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/MPIIGaze.png" alt="image-20211007135414078"></p><center><p>MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation  2017 TPAMI    单目相机进行凝视估计    </p></center><h3 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h3><ul><li><p>面部landmark检测，(眼睛和嘴角）（Baltruˇsaitis et al.</p></li><li><p>通过使用EPnP算法[21]估计初始解来拟合模型，并通过非线性优化进一步<strong>精炼姿态</strong>。<strong>三维头部旋转</strong>定义为从<strong>头部坐标系到摄像机坐标系的旋转camera calibration</strong>，眼睛位置定义为每只眼睛的眼角中点。</p></li><li><p>虽然之前的作品假设了准确的头部姿态，但我们使用一个通用。平均面部形状模型进行三维姿态估计<strong>generic face model</strong>，以评估实际环境中的整个凝视估计管道。</p></li><li><p>在收集数据之前，使用外部立体摄像机记录所有参与者的6个地标的3D位置，并建立通用形状作为所有参与者的平均形状。</p></li></ul><p>上面两个工作都以单眼图像为输入，没有充分利用<strong>双眼的互补信息</strong>。北航博士Yihua Cheng在ECCV 2018上提出了一个基于双眼的非对称回归方法[3]。其方法框图如下：</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/symmetric.png" alt="image-20211007154253501"></p><center><p>Appearance-based gaze estimation via evaluation- guided asymmetric regression ECCV2018</p></center><p>AR-Net（非对称回归网络），以双眼为输入，经四个支路处理后得<strong>到两只眼睛的不同视线方向</strong>；E-Net（评价网络），同样以双眼为输入，输出是两个权重，用于加权AR-Net训练过程中两只眼睛视线的loss。其基本思想是，双眼中某一只眼睛可能因为一些原因（如光照原因等），更容易得到精准的视线估计，因此在AR-Net训练中应该赋予这只眼睛对应的loss更大的权重。该工作在MPIIGaze数据集上取得了5.0度的误差。</p><h1 id="基于语义信息的视线估计"><a href="#基于语义信息的视线估计" class="headerlink" title="基于语义信息的视线估计"></a>基于语义信息的视线估计</h1><p>基于几何的方法是通过检测眼睛特征，如关键点位置，来估计视线的。</p><blockquote><p>几何检测眼睛的几何信息。</p><h2 id="Deep-Pictorial-Gaze-Estimation-2018ECCV"><a href="#Deep-Pictorial-Gaze-Estimation-2018ECCV" class="headerlink" title="Deep Pictorial Gaze Estimation 2018ECCV"></a>Deep Pictorial Gaze Estimation 2018ECCV</h2><p>这启发了一部分工作使用<strong>额外的语义信息</strong>来帮助提升视线估计的精度。ETH博士Park等在ECCV 2018上提出了一种基于眼睛图形表示的视线估计方法。</p></blockquote><p>通过深度网络将眼睛抽象为一个眼球图形表示来提升视线估计（这一表示相对gaze来说更具象也更易学习）。其中，眼球图形表示这一监督信号是由视线的ground truth经几何方法反推生成的。</p><p>不是直接回归两个角度的俯仰和偏航的眼球，而是回归到一个中间的图像表示，这反过来简化了三维注视方向估计的任务。</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Deep Pictorial Gaze Estimatio.png" alt="image-20211007160924206">  </p><center><p>Deep Pictorial Gaze Estimation 2018ECCV </p></center> <p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Deep Pictorial Gaze Estimation1.png" alt="image-20211007161335847"></p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Deep Pictorial Gaze Estimation2.png" alt="image-20211007162046332"></p><p>ETH在ETRA 2018上的工作[5]，利用眼睛关键点的heat map估计视线。方法框架如下图所示，其中眼睛关键点这一监督信息由合成数据集UnityEyes提供，这里不展开了。</p><p><img src="https://pic4.zhimg.com/v2-baec315548674738560b1acb384e77cf_r.jpg" alt="preview"></p><h2 id="Deep-multitask-gaze-estimation-with-a-constrained-landmark-gaze-model"><a href="#Deep-multitask-gaze-estimation-with-a-constrained-landmark-gaze-model" class="headerlink" title="Deep multitask gaze estimation with a constrained landmark-gaze model"></a>Deep multitask gaze estimation with a constrained landmark-gaze model</h2><p>下一篇论文<img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/constrained landmark-gaze model.png" alt="image-20211007164953107"></p><center><p> Deep multitask gaze estimation with a constrained landmark-gaze model 2018 ECCVW</p></center><p>(a)从UnityEyes地标集(顶部)选择地标(底部)。(b)地标水平或垂直位置与凝视偏航或俯仰角度之间的相关系数。(c)<strong>虹膜中心水平</strong>或垂直位置和偏航或俯仰凝视角度的联合分布图。</p><p>此文提出了一种基于约束模型的视线估计方法[6]，其基本出发点是<strong>多任务学习</strong>的思想，即在<strong>估计gaze的同时检测眼睛关键点位置</strong>，两个任务<strong>同时学习，信息互补</strong>，可以在一定程度上得到共同提升。</p><ul><li><p><strong>首先对眼睛关键点与视线的关系进行了统计建模</strong>。具体地，对于合成数据集UnityEyes中的一个样本，我们抽取其17个眼睛关键点与两个gaze角度，将他们展开并拼接为一个36维（17<em>2 + 2）的向量。然后将n个样本对应的n个向量堆叠生成一个n</em>36大小的矩阵，对该矩阵做PCA分解后可以得到一个mean shape和一系列deformation basis。</p><p>这两个部分共同构成了一个约束模型。其中mean shape代表眼睛的平均形状以及对应的gaze平均值，而deformation basis则包含了眼睛形状与gaze的协同变化信息。约束模型的建立过程如下图所示。</p></li><li><p><img src="https://pic3.zhimg.com/80/v2-3026afc055f18e5e3e0a0a65f94c4a16_1440w.jpg" alt="img"></p></li></ul><p>对于一个输入样本，如果能学习出这个约束模型中的deformation basis系数，并与mean shape组合，就可以重建出这个样本的眼睛形状和gaze。</p><p>因此我们使用的网络架构如下，网络的主要<strong>输出即约束模型的系数，用以重建眼睛的形状和gaze</strong>。另外，由于约束模型表示的眼睛形状是经过归一化操作的，网络同时学习一个缩放系数，和一个平移向量，通过几何变换（decoder）得到正确的眼睛关键点位置。网络通过优化关键点位置loss与视线loss实现end to end training。实验结果表明，取得了比直接回归更精准的结果。</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/constrained landmark-gaze_framework.png" alt="image-20211007170804421"></p><center><p>Deep multitask gaze estimation with a constrained landmark-gaze model 2018ECCVW</p></center><blockquote><p>线性重建的方法类似于人脸重建？</p></blockquote><h1 id="全脸视线估计"><a href="#全脸视线估计" class="headerlink" title="全脸视线估计"></a>全脸视线估计</h1><h2 id="It’s-Written-All-Over-Y-our-Face-Full-Face-Appearance-Based-Gaze-Estimation-CVPR2017"><a href="#It’s-Written-All-Over-Y-our-Face-Full-Face-Appearance-Based-Gaze-Estimation-CVPR2017" class="headerlink" title="It’s Written All Over Y our Face:Full-Face Appearance-Based Gaze Estimation CVPR2017"></a>It’s Written All Over Y our Face:Full-Face Appearance-Based Gaze Estimation CVPR2017</h2><p>以上视线估计方法都要求单眼/双眼图像为输入，有两个缺陷：</p><p>1）需要额外的模块检测眼睛；</p><p>2）需要额外的模块估计头部姿态。</p><p>基于此，Xucong Zhang等于2017年提出了基于注意力机制的全脸视线估计方法[7]。<img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Blog\source\_posts\eye-gaze调研\It’s Written All Over Y our Face.png" alt="image-20211007202852686"></p><p>这里注意力机制的主要思想是<strong>通过一个支路学习人脸区域各位置的权重</strong>，其目标是增大眼睛区域的权重，抑制其他与gaze无关的区域的权重。网络的输入为人脸图像并采用end to end的学习策略，直接学习出最终相机坐标系下的gaze。这一工作同时公开了全脸视线数据集MPIIFaceGaze。</p><p>然MPIIGaze与MPIIFaceGaze使用的是同一批数据，但并不是同一个数据集（许多论文把这两个数据集混淆）。首先MPIIGaze数据集并不包含全脸图片，其次MPIIFaceGaze的ground truth定义方式与MPIIGaze不同。该工作最终在MPIIFaceGaze数据集上取得了4.8度的精度。</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/sensetime.png" alt="image-20211014172151988"></p><p>与上面工作不同的是，除人脸输入外，该工作同时要求输入眼睛图片，如图所示。该工作主要认为工作[1]中gaze特征与head pose拼接的方式并不能准确地反映两者的的几何关系。因此，该工作提出了一个gaze的几何变换层，用于将head pose（人脸支路学习得到）与人脸坐标系下的gaze（眼睛支路学习得到）进行几何解析，得到最终相机坐标系下的gaze。该工作在自己收集的数据集上取得了4.3度的误差。</p><p>不知各位读者发现没有，在person independent（训练数据与测试数据采集自不同的人）这一设定下，上述方法的精度大都在4-5度之间徘徊，似乎很难得到进一步的提升。这个<strong>瓶颈主要是由人的眼球内部构造造成的</strong>。如果希望继续提升精度，一般要使用个性化策略。这一部分内容准备在下下一个篇章中讲解。在下一篇章中，我会简要介绍三维视线数据如何收集标注的问题，以及如何在数据集短缺的情况下，训练一个gaze模型。</p><h1 id="视线估计-Gaze-Estimation-简介-五-三维视线估计（数据集问题）"><a href="#视线估计-Gaze-Estimation-简介-五-三维视线估计（数据集问题）" class="headerlink" title="视线估计(Gaze Estimation)简介(五)-三维视线估计（数据集问题）"></a>视线估计(Gaze Estimation)简介(五)-三维视线估计（数据集问题）</h1><p>1.介绍三维视线数据如何收集和标注的问题。</p><p>2.以及如何在数据集短缺的情况下，训练一个gaze模型。</p><h2 id="数据收集："><a href="#数据收集：" class="headerlink" title="数据收集："></a>数据收集：</h2><p>与分类、检测等任务不同，<strong>三维视线难以人工标注</strong>。</p><p>一位参与者坐在深度摄像头Kinect前，而一名实验人员则手握一根吊着乒乓球的棍子，操纵乒乓球在参与者面前随机运动。参与者被要求始终盯着乒乓球，而<strong>深度摄像头</strong>则会记录下整个过程。数据收集完毕后，我们可以通过算法或人工的方式<strong>标注RGB视频中的眼睛中心点位置</strong>和<strong>乒乓球位置</strong>。</p><p>我们把这两个位置<strong>映射到深度摄像头记录的三维点云中</strong>，从而得到<strong>对应的三维位置坐标</strong>。这两个<strong>三维位置坐标相减后即得到视线方向</strong>。这种数据收集和标注方式不仅精准而且相对简单。额外的要求就是需要一台深度摄像头。</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/eyediap.png" alt="image-20211014190045184"></p><p>另一种数据收集方式以MPIIGaze[2]为代表，仅仅需要普通的RGB摄像头即可。其基本做法是利用<strong>相机的公开参数</strong>，将<strong>gaze目标以及眼睛位置坐标</strong>（通过一个三维的6关键点模型得到）通过算法变换到相机坐标下，然后再计算gaze作为ground truth。但是这种标注方法不仅操作复杂，而且并不准确。</p><p>由以上两个例子可以看到，gaze数据的收集和标注比较耗时耗力。因此在实际应用中，如何在数据短缺的情况下训练一个可靠的gaze模型，就成了一个亟待解决的问题。本篇剩余部分主要介绍三类针对数据短缺的解决方案：<strong>基于合成数据的方法</strong>、<strong>基于Domain Adaptation的方法</strong>以及<strong>基于无监督学习的方法</strong>。</p><h2 id="基于合成数据的方法："><a href="#基于合成数据的方法：" class="headerlink" title="基于合成数据的方法："></a>基于合成数据的方法：</h2><h3 id="Rendering-of-Eyes-for-Eye-Shape-Registration-and-Gaze-Estimation"><a href="#Rendering-of-Eyes-for-Eye-Shape-Registration-and-Gaze-Estimation" class="headerlink" title="Rendering of Eyes for Eye-Shape Registration and Gaze Estimation"></a>Rendering of Eyes for Eye-Shape Registration and Gaze Estimation</h3><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Rendering of Eyes.png" alt="image-20211014201017047" style="zoom:50%;"></p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Rendering of Eyes model preparation.png" alt="image-20211014195732273"></p><p>模型准备过程概述:密集3D头部扫描(140万个多边形)(a)首先重新拓扑成动画的最佳形式(9005个多边形)(b)。高分辨率皮肤表面细节通过位移图恢复(c)，人工标注虹膜和眼睑的3D地标(d)。如图(e)所示。</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Rendering of Eyes position change.png" alt="image-20211014201322789"></p><h2 id="基于Domain-Adaptation的方法"><a href="#基于Domain-Adaptation的方法" class="headerlink" title="基于Domain Adaptation的方法"></a>基于Domain Adaptation的方法</h2><p>Shrivastava, A., Pfister, T., Tuzel, O., Susskind, J., Wang, W., and Webb, R. (2017). Learning from simulated and unsupervised images through adversarial training. In <em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, volume 3, page 6.</p><h2 id="基于无监督学习的方法-标注数据少"><a href="#基于无监督学习的方法-标注数据少" class="headerlink" title="基于无监督学习的方法,标注数据少"></a>基于无监督学习的方法,标注数据少</h2><h3 id="CVPR2020-Unsupervised-Representation-Learning-for-Gaze-Estimation余"><a href="#CVPR2020-Unsupervised-Representation-Learning-for-Gaze-Estimation余" class="headerlink" title="CVPR2020 Unsupervised Representation Learning for Gaze Estimation余"></a>CVPR2020 Unsupervised Representation Learning for Gaze Estimation余</h3><p>从无标签的数据中学习gaze表征。实验表明，通过我们的方法学习到的gaze表征与真实值呈强线性关系。在实际使用时，仅需要极少量的标注样本（&lt;=100），就可以得到有效可靠的视线估计模型。据我所知，这应该是第一篇通过无监督的方式学习gaze的论文。</p><h3 id="CVPR2019-Improving-few-shot-user-specific-gaze-adaptation-via-gaze-redirection-synthesis"><a href="#CVPR2019-Improving-few-shot-user-specific-gaze-adaptation-via-gaze-redirection-synthesis" class="headerlink" title="CVPR2019 Improving few-shot user-specific gaze adaptation via gaze redirection synthesis"></a>CVPR2019 Improving few-shot user-specific gaze adaptation via gaze redirection synthesis</h3><p>注视作为人类注意的指示物，是一种微妙的行为线索，具有广泛的应用价值。然而，由于缺乏大量的数据(真实的凝视是昂贵的，现有的数据集使用不同的设置)，以及由于<strong>个体差异而固有的凝视偏差</strong>，即使对深度神经网络来说，推断3D凝视方向也是具有挑战性的。在这项工作中，我们只<strong>从少数参考训练样本</strong>中解决了特定于人的凝视模型的适应问题。</p><p>主要和新颖的想法是，通过从<strong>已有的参考样本合成凝视重定向的眼睛图像来生成额外的训练样本，以提高凝视适应能力</strong>。</p><p>在此过程中，我们的贡献有三个方面:</p><p>(i)我们从合成数据中设计了我们的注视重定向框架，使我们能够从对齐的训练样本对中受益，以预测精确的<strong>逆映射域</strong>;</p><p>(ii)提出<strong>领域适应的self-supervised自我监督</strong>方法;</p><blockquote><p>domain adaption</p></blockquote><p>(iii)我们<strong>利用凝视重定向来提高特定于人的凝视估计的性能</strong>。在两个公共数据集上的大量实验证明了我们的视线重定向和视线估计框架的有效性。</p><blockquote><p>用少样本domain adaption生成很多样本。元学习的部分如何让理解？对原本的gaze estimator重定向。</p></blockquote><p>我们当时采取了如下网络结构。训练该网络需要输入样本，目标样本，以及输入样本与目标样本间gaze角度（包括垂直方向角度pitch与水平方向角度yaw）的差值。网络通过解析输入样本与gaze差值输出两个光流场（垂直和水平两个方向），来对输入图像的像素重定向，从而得到接近于目标图像的输出。</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Improving few-shot user-specific gaze adaptation via gaze redirection synthesis.png" alt="image-20211015104645809"></p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Improving few-shot user-specific gaze adaptation via gaze redirection synthesis1.png" alt="image-20211015104707764"></p><p>我们期望通过缩小最终的输出图像与目标图像之间的loss，来迫使右上角的网络学习到gaze相关的表征（end to end training）。</p><p>这样一来我们就可以<strong>不依赖任何标注数据而学习出gaze表征</strong>。这对于视线估计，这个数据标注十分复杂和困难的领域十分有意义。需要注意的是，由于gaze由两个角度表示，我们的gaze表征也设定为二维。</p><p><strong>赋予物理意义：</strong>至此，网络或许能够学习出gaze相关的表征，但我们并不清楚这个表征所表示的物理意义。<strong>我们观察到，当人的视线上下变化时，眼皮和眼珠等部位主要在垂直方向运动，水平方向的运动几乎为0；当人的视线左右变化时，主要是眼珠在水平方向运动，几乎所有部位在垂直方向的运动为0。</strong></p><p>这也就是说，当垂直方向的gaze角度pitch变化（差值）为0时，视线重定向网络生成的垂直方向光流场应该接近于一个identity mapping；</p><p>而当水平方向gaze角度yaw变化（差值）为0时，水平方向光流场则接近于一个identity mapping。</p><p>由此我们提出一个针对光流场的正则：一方面，我们人为将gaze表征的第一维修改为0，然后输入重定向网络，并优化垂直方向光流场与identity mapping之间的差异；</p><p>另一方面，我们人为将gaze表征的第二维修改为0，输入重定向网络，优化水平方向光流场与identity mapping间的差异。</p><p>如下图所示。经此操作，gaze表征的第一维即对应pitch，第二维即对应yaw。</p><p><strong>扩展1-头部姿态估计：</strong>我们将提出的框架应用到了头部姿态估计（head pose estimation）这一任务中。我们使用BIWI数据集。</p><p><strong>扩展2-视线迁移：</strong>我们可以把person A的眼睛图片输入到表征学习网络中（抽取A的视线变化），而把person B的眼睛图片输入到视线重定向网络中，从而实现无监督视线迁移，即把A的gaze行为转移给B。</p><h1 id="视线估计-Gaze-Estimation-简介-六-三维视线估计（个性化问题）"><a href="#视线估计-Gaze-Estimation-简介-六-三维视线估计（个性化问题）" class="headerlink" title="视线估计(Gaze Estimation)简介(六)-三维视线估计（个性化问题）"></a>视线估计(Gaze Estimation)简介(六)-三维视线估计（个性化问题）</h1><p>在上上一篇中我们提到，在person independent（训练数据与测试数据采集自不同的人）设定下，大部分主流视线估计方法的<strong>精度大都在4-5度之间徘徊</strong>，很难得到进一步的提升。其主要原因是<strong>人与人之间存在一定的视线偏差</strong>。对于不同的两个人，即便眼球的旋转角度完全相同，其<strong>视线也会存在2到3度的不同</strong>。</p><h2 id="视线偏差产生原因："><a href="#视线偏差产生原因：" class="headerlink" title="视线偏差产生原因："></a>视线偏差产生原因：</h2><p>视线偏差产生的原因当然跟每个人的<strong>眼睛形状，眼珠大小</strong>等因素相关。但这些因素导致的视线偏差事实上很小，并且这些都是视觉元素，他们与gaze的相关性是可以从图像中学习得到的。真正<strong>产生视线偏差的原因来自于眼球内部构造</strong>。</p><p><img src="https://pic1.zhimg.com/v2-1f1ed9ae231cd771a1da6a584a947964_r.jpg" alt="preview"></p><p>Kenneth在他CVPR 2014年的论文中[1]较详细的介绍了原因。如图是一个眼球的构造图，</p><p>其中v表示视线，</p><p>o表示optical axis（瞳孔中心与眼球中心的连线，不知道中文该怎么翻），</p><p>pc是眼球中心，</p><p>而fovea是视网膜上对光敏感度最高的一个点，</p><p>N则代表一个与眼球中心距离为d的节点。</p><p>直觉上来说，视线v应该就是瞳孔中心与眼球中心的连线。然而事实上，视线p是连接fovea与N的直线，它与我们通常认为的“视线”，即optical axis不同。我们用k来表示视线p与optical axis的夹角，<strong>k的大小因人而异，由人眼球内部参数决定，无法从图像中学习获得。</strong>了解了这个原因之后，我们就理解了为什么在person independent设定下，模型精度难以进一步提高的原因：训练数据的后验概率分布与测试数据的后验概率分布不同。</p><p><strong>偏差消除方法，偏差估计方法与模型微调方法</strong>。</p><h2 id="A-Differential-Approach-for-Gaze-Estimation"><a href="#A-Differential-Approach-for-Gaze-Estimation" class="headerlink" title="A Differential Approach for Gaze Estimation"></a>A Differential Approach for Gaze Estimation</h2><p>大多数非侵入性凝视估计方法直接从一张脸或眼睛的图像中回归凝视方向。然而，由于个体之间眼睛形状和内部眼睛结构的重要变量，通用模型获得的精度有限，它们的输出通常表现出高方差和受试者依赖偏差。因此，提高准确度通常是通过校准来完成的，允许对一个对象的凝视预测被映射到她的实际凝视。在本文中，我们介绍了一种新的方法，通过直接训练微分卷积神经网络来预测同一被试的两个眼睛输入图像之间的注视差异。然后，给定一组受试者特定的校准图像，我们可以利用推断的差异来预测新眼睛样本的注视方向。假设通过比较同一用户的眼睛图像，通常困扰单一图像预测方法的烦恼因素(对齐、眼睑闭合、光照扰动)可以大大减少，从而更好地进行预测。此外，差分网络本身可以通过微调进行调整，使预测与可用的用户参考对一致。在3个公共数据集上的实验验证了我们的方法，即使只使用一个校准样本或那些依赖于受试者特定的注视适应的方法，我们的方法也不断优于最先进的方法。</p><h1 id="视线估计-Gaze-Estimation-简介-七-三维视线估计（头部姿态问题）"><a href="#视线估计-Gaze-Estimation-简介-七-三维视线估计（头部姿态问题）" class="headerlink" title="视线估计(Gaze Estimation)简介(七)-三维视线估计（头部姿态问题）"></a>视线估计(Gaze Estimation)简介(七)-三维视线估计（头部姿态问题）</h1><p>视线的方向不仅取决于眼球的旋转，还取决于头部的姿态（head pose)。如图，虽然眼睛相对头部是斜视，但<strong>在相机坐标系下，他看的是正前方</strong>。</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/head_gaze.png" alt="image-20211015142501754"></p><p>在大多数情况下，我们希望得到的是相对于相机坐标系的视线，那么如何在视线估计中有效使用头部姿态信息就成了一个非常值得研究的问题。本篇会介绍五类方法。</p><p>其中前三类方法需要一个独立的前处理步骤事先估计出head pose，</p><p>而后两类方法则是在同一个框架内估计head pose和gaze。</p><h2 id="基于逐姿态视线估计的方法"><a href="#基于逐姿态视线估计的方法" class="headerlink" title="基于逐姿态视线估计的方法"></a>基于逐姿态视线估计的方法</h2><p>先对head pose聚类，然后对每一类的样本分别训练一个gaze模型。其代表是东京大学的Sugano教授发表在CVPR 2014上的工作[1]，如图所示。需要注意的是，为提高鲁棒性，该方法在训练一个随机森林时，也会用到相邻的head pose类的样本。这类方法的一大缺点是需要训练多个gaze模型，对实际应用参考意义不大。</p><h2 id="基于视角变换的方法"><a href="#基于视角变换的方法" class="headerlink" title="基于视角变换的方法"></a>基于视角变换的方法</h2><p>这类方法的一个前提条件是需要一个深度摄像头获取点云数据。在计算得到head pose之后，该方法利用head pose信息对点云数据作几何逆变换，从而得到前视视角（frontal view）的人脸或眼睛图像，如下图所示。该方法使用前视视角的样本训练gaze模型，再把估计得到的gaze利用head pose信息变换到相机坐标系。由于眼睛图像被统一变换到前视视角，可以认为这些训练样本处于同一个pose空间，因此该方法训练的模型相对比较准确。但这类方法也有两大缺点，一是需要深度摄像头（普通摄像头无法操作），二是在head pose较大时，视角变换后的样本信息会有所缺失（如图右侧眼睛）。</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/view_change.png" alt="image-20211015143935895"></p><h2 id="基于特征拼接的方法"><a href="#基于特征拼接的方法" class="headerlink" title="基于特征拼接的方法"></a>基于特征拼接的方法</h2><p>concatenate</p><h2 id="基于几何变换的方法"><a href="#基于几何变换的方法" class="headerlink" title="基于几何变换的方法"></a>基于几何变换的方法</h2><p>这类方法的代表作是商汤在ICCV 2017上发表的一个全脸视线估计工作[4]。该工作主要认为特征拼接的方式并不能准确地反映gaze与head pose的的几何关系。因此，该工作提出了一个gaze的几何变换层，用于将head pose（人脸支路学习得到）与人脸坐标系下的gaze（眼睛支路学习得到）进行几何解析，得到最终相机坐标系下的gaze。</p><p>该方法将head pose的估计和gaze的估计放在一个框架内（所需要的前处理步骤主要是眼睛的定位），因此容错性更大。</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/ICCV 2017sensetime.png" alt="image-20211014172151988"></p><h2 id="基于头部姿态隐式估计的方法"><a href="#基于头部姿态隐式估计的方法" class="headerlink" title="基于头部姿态隐式估计的方法"></a>基于头部姿态隐式估计的方法</h2><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/hiding_head.png" alt="image-20211007202852686" style="zoom:150%;"></p><p>Xucong Zhang等于2017年提出的基于注意力机制的全脸视线估计方法[5]。网络的输入是人脸图像，并采用end to end的学习策略，直接学习出最终相机坐标系下的gaze。因此我们可以认为这类方法隐式估计了head pose。这类方法的优点是简单直接，但内部机制不明，可解释性较差。</p><p>本篇主要介绍了五类处理头部姿态问题的方法。从我比较熟悉的单眼/双眼视线估计来说，我更推荐第三种方式：基于特征拼接的方法。商汤的论文[4]认为特征拼接并不是一个好的方式。他们通过在MPIIGaze上做实验发现处理head pose的weight几乎等于0，进而得出特征拼接效果欠佳这一结论。我认同这一现象，但不认同其结论。事实上，在我看来，造成这一现象的真正原因是<strong>MPIIGaze提供的head pose非常非常不准确</strong>，能提供的有效信息十分有限，用不用<strong>head pose差别不大（结果差0.1度左右）</strong>。相反，如果换做其他数据集，如<strong>ColumbiaGaze和UTMultiview</strong>，特征拼接则会显著提高精度（至少提高2度）。</p><p>在我们CVPR 2020的论文中[6]，我们也比较了特征拼接和几何变换（网络的输入是单眼，输出是头部坐标系下的gaze，然后通过事先获得的head pose对gaze进行几何变换，得到相机坐标系下的gaze）两种方式，结果是特征拼接的效果明显好于几何变换（1-2度）。这个结果可能与我们的直觉相悖。我理解的原因是，一般来说，<strong>如果注视目标位于前方，则相机坐标系下的gaze范围更小，而头部坐标系下的gaze范围更大</strong>。有兴趣的话可以尝试一下，<strong>变换头部姿态，但眼睛始终盯着正前方一个物体。如果这时正前方有一个摄像机，那么相机坐标系下你的gaze范围很小，接近于0度</strong>。<strong>但头部坐标系下的gaze，根据你头部运动幅度的大小，范围可以很大。拿ColumbiaGaze举例，相机坐标系下的gaze yaw的范围是-15~15，而头部坐标系下的gaze yaw是-45~45</strong>。因此，网络如果通过特征拼接的方式直接预测相机坐标系下的gaze的话，反而更容易学习。</p><h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><h2 id="ETH-XGaze-A-Large-Scale-Dataset-for-Gaze-Estimation-under-Extreme-Head-Pose-and-Gaze-Variation2020"><a href="#ETH-XGaze-A-Large-Scale-Dataset-for-Gaze-Estimation-under-Extreme-Head-Pose-and-Gaze-Variation2020" class="headerlink" title="ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation2020"></a>ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation2020</h2><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/eth_gaze.png" alt="image-20211015152031823"></p><h2 id="商汤ICCV-2017Monocular-free-head-3d-gaze-tracking-with-deep-learning-and-geometry-constraints数据集"><a href="#商汤ICCV-2017Monocular-free-head-3d-gaze-tracking-with-deep-learning-and-geometry-constraints数据集" class="headerlink" title="商汤ICCV 2017Monocular free-head 3d gaze tracking with deep learning and geometry constraints数据集"></a>商汤ICCV 2017Monocular free-head 3d gaze tracking with deep learning and geometry constraints数据集</h2><p>看一下有没有开源</p><h2 id="MPIIFaceGaze"><a href="#MPIIFaceGaze" class="headerlink" title="MPIIFaceGaze"></a>MPIIFaceGaze</h2><h2 id="UnityEyes"><a href="#UnityEyes" class="headerlink" title="UnityEyes"></a>UnityEyes</h2><p><a href="https://www.cl.cam.ac.uk/research/rainbow/projects/unityeyes/">UnityEyes (cam.ac.uk)</a></p><h2 id="Columbia"><a href="#Columbia" class="headerlink" title="Columbia"></a>Columbia</h2><p>eth eccv2018 眼睛黑白图片</p><h2 id="GazeCapture涵盖1400多人、240多万样本的数据集。"><a href="#GazeCapture涵盖1400多人、240多万样本的数据集。" class="headerlink" title="GazeCapture涵盖1400多人、240多万样本的数据集。"></a>GazeCapture涵盖1400多人、240多万样本的数据集。</h2><p>Krafka, K., Khosla, A., Kellnhofer, P ., Kannan, H., Bhandarkar,<br>S., Matusik, W., Torralba, A.: Eye tracking for everyone. In: Pro-<br>ceedings of the IEEE Conference on Computer Vision and Pattern<br>Recognition, pp. 2176–2184 (2016)，</p><h3 id="RGBD注视跟踪数据集Eyediap，该数据集由16个参与者的视频组成。"><a href="#RGBD注视跟踪数据集Eyediap，该数据集由16个参与者的视频组成。" class="headerlink" title="RGBD注视跟踪数据集Eyediap，该数据集由16个参与者的视频组成。"></a><strong>RGBD注视跟踪数据集</strong>Eyediap，该数据集由16个参与者的视频组成。</h3><p> UnityEyes 如图1a的第一排所示。考虑到依赖大量的路标无助于提高鲁棒性和准确性，只会增加方法的复杂性，我们只选择可用路标的一个子集作为替代。它包含来自眼睑的16个标志，虹膜中心是由虹膜轮廓标志估计出来的。如图1a第二行所示。</p><h2 id="MPIIGaze数据集"><a href="#MPIIGaze数据集" class="headerlink" title="MPIIGaze数据集"></a>MPIIGaze数据集</h2><p>MPIIGaze数据集，2015年，这是一个野外RGB凝视数据集，收集了15名参与者在几个月的日常使用笔记本电脑期间的数据。包含了在超过三个月的日常使用笔记本电脑期间收集的15名参与者的213659张图像。数据集在外观和照明方面比现有的数据集有更大的变化。Appearance-Based Gaze Estimation in the Wild提的。</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/MPIIGaze数据.png" alt="image-20211007142949977"></p><p>虽然MPIIGaze与MPIIFaceGaze使用的是同一批数据，但并不是同一个数据集（许多论文把这两个数据集混淆）。首先MPIIGaze数据集并不包含全脸图片，其次MPIIFaceGaze的ground truth定义方式与MPIIGaze不同。该工作最终在MPIIFaceGaze数据集上取得了4.8度的精度。</p><h2 id="Top-8-Eye-Tracking-Applications-in-Research-imotions-com"><a href="#Top-8-Eye-Tracking-Applications-in-Research-imotions-com" class="headerlink" title="Top 8 Eye Tracking Applications in Research (imotions.com)"></a><a href="https://imotions.com/blog/top-8-applications-eye-tracking-research/">Top 8 Eye Tracking Applications in Research (imotions.com)</a></h2><h2 id="UT-Multiview"><a href="#UT-Multiview" class="headerlink" title="UT Multiview"></a>UT Multiview</h2><h2 id="shanghaitech"><a href="#shanghaitech" class="headerlink" title="shanghaitech"></a>shanghaitech</h2><p>建立了迄今为止最大的RGB-D凝视跟踪数据集，收集了218名参与者，包含超过165000张图像。该数据集将向所有研究人员公开，以促进对数据驱动的凝视跟踪方法的研究</p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/shanghaitech.png" alt="image-20211006164028365"><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/shanghaitech_compare.png" alt="image-20211006164223297"></p><p><a href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9153754/akiny.t4-3013540-large.gif">https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9153754/akiny.t4-3013540-large.gif</a></p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211008155155643.png" alt="image-20211008155155643"></p><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211008155246008.png" alt="image-20211008155246008"></p>]]></content>
      
      
      <categories>
          
          <category> 视线估计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>leetcode题目记录</title>
      <link href="/2021/10/04/leetcode%E9%A2%98%E7%9B%AE%E8%AE%B0%E5%BD%95/"/>
      <url>/2021/10/04/leetcode%E9%A2%98%E7%9B%AE%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="20211004用两个栈实现队列"><a href="#20211004用两个栈实现队列" class="headerlink" title="20211004用两个栈实现队列"></a>20211004用两个栈实现队列</h1><p><a href="https://leetcode-cn.com/problems/yong-liang-ge-zhan-shi-xian-dui-lie-lcof/">剑指 Offer 09. 用两个栈实现队列 - 力扣（LeetCode） (leetcode-cn.com)</a></p><h2 id="我的思路"><a href="#我的思路" class="headerlink" title="我的思路"></a>我的思路</h2><p>栈和队列是相反的，先后入两个不同的栈相当于入一个队列，</p><p>比如Q{1，2，3，4}，head =1，tail=4，</p><p>S1=[4，3，2，1]，top=4，bottom=1，让S1中的元素一个个弹出至S2中，可以得到S2=[1，2，3，4]，top=1，bottom=4。</p><p>对于刚建立的空CQueue，返回值为null，</p><p>需要实现appendTail和deleteHead，</p><p>appendTail返回值是null，需要把新tail元素压入S1中位于S1top，把S2全部更新才可以实现把tail放至S2的bottom，但是这样的话每次appendTail都要产生一个全新的S2，会开一块新的内存空间，并且出栈入栈，那复杂度。。。。不不不，不用啦哈哈，因为有两个栈一个出清空，另一个入填满，来回倒来倒去就可以啦!</p><p>所以过程是S1=[4，3，2，1]，S2=[]</p><p>S1=[3，2，1]，S2=[4]；S1=[2，1]，S2=[3，4]…S1=[]，S2=[1，2，3，4]</p><h2 id="appendTail"><a href="#appendTail" class="headerlink" title="appendTail"></a>appendTail</h2><p>初始S1=[]，S2=[1，2，3，4]；S2出栈至S1，元素顺序全部倒置，变为S1=[4，3，2，1]，S2=[]，此时把tail=5入栈到S1，变为S1=[5，4，3，2，1]，S2=[]；再次让S1中的元素一个个弹出至S2中，变为S1=[]，S2=[1，2，3，4，5]，实现了appendTail功能。</p><p>输出null。</p><blockquote><p>每次append后重新倒了回去，多花了一倍时间20211028.</p></blockquote><h2 id="deleteHead"><a href="#deleteHead" class="headerlink" title="deleteHead"></a>deleteHead</h2><p>直接pop掉S2的顶部元素即可。输出栈顶元素。</p><h2 id="忽略点"><a href="#忽略点" class="headerlink" title="忽略点"></a>忽略点</h2><blockquote><p>用size作为循环的终止条件，size是会随着每一次的pop、push操作更新的。</p><p>在构造函数中定义的变量是由于在构造函数中定义，作用域为局部变量。</p></blockquote><h1 id="20211010剑指-Offer-30-包含min函数的栈"><a href="#20211010剑指-Offer-30-包含min函数的栈" class="headerlink" title="20211010剑指 Offer 30. 包含min函数的栈"></a>20211010<a href="https://leetcode-cn.com/problems/bao-han-minhan-shu-de-zhan-lcof/">剑指 Offer 30. 包含min函数的栈</a></h1><p>push、pop、top方法是一定的，当顶部数据弹出后，还是能够很快找到最小值，min的复杂度为O(1)。</p><p>设置一个数组用于排序或者从头到尾的hash表格也可以，但有负数。</p><blockquote><p>20211024</p></blockquote><p>1.想到啦！本题目的难点在于怎么可以找到一种数据结构，它有记忆性，当插入、删除一个数的时候，可以对数据进行排序，并且可以很方便的取出最小值。用最小优先队列就可以了，其实就是维护一个最小堆，一次操作中复杂度为O(lgn)但堆怎么删除一个数据呢？需要对于每个节点记录parent。</p><p>2.其实每次使用数组来存放stack里的所有数据的非降序排序结果也可以，但是每次push后需要扫描排序，排序复杂度为O(n)，当pop后也需要排序，主要是将后面的值向前挪动，复杂度为O(n)。</p><p>3.或者偷懒，使用优先队列priority_queue来实现自动排序 。orz</p><h2 id="忽略点-1"><a href="#忽略点-1" class="headerlink" title="忽略点"></a>忽略点</h2><p>对于priority_queue，pop掉的 是栈顶元素，即排序后的栈顶元素，而不是stack的顶部，怎么可以删除掉任意一个位置的元素呢？</p><p><a href="https://blog.csdn.net/justidle/article/details/106793522">(10条消息) O(1) 复杂度支持任意位置删除的 priority_queue 实现_努力中的老周的专栏-CSDN博客</a></p><p>MAXHEAP 内部有两个优先队列，其中 Q 队列保存当前元素，D 队列保存需要删除的元素。一个是数据，一个是待删除数据，当两个 priority_queue 的 top() 元素相同的时候，我们再删除两个优先队列的 top。</p><p>while(!D.empty()&amp;&amp;Q.top()==D.top()) {Q.pop();D.pop();}当删除的值是当前队列的头部，就删除，否则将其暂存在待删除队列当中，等到之后比待删除队列中大的元素删除后，再删除待删除队列的元素。</p><p>pq插入5，Q.top()=5,Q={5};</p><p>pq插入8，Q.top()=8,Q={8,5};</p><p>pq插入6，Q.top()=8,Q={8,5,6};</p><p>pq插入4，Q.top()=8,Q={8,5,6,4};</p><p>pq.erase(8),D.push(8),Q.pop(),Q={6,5,4};D.pop(),D={};Q.top()=6;</p><p>pq.erase(5),D.push(5),D={5}；</p><p>pq.erase(6),D.push(6),D={6,5}</p><blockquote><p>开辟了两个优先队列，其实占用了空间，也可以用两个数组每次把排序的结果存下来，来回倒。</p></blockquote><h2 id="好思路"><a href="#好思路" class="headerlink" title="好思路"></a>好思路</h2><p>两个stack分别作用，一个当作主stack，另一个用于存放比当前元素小的值。</p><h1 id="20211026剑指-Offer-06-从尾到头打印链表-力扣（LeetCode）-leetcode-cn-com"><a href="#20211026剑指-Offer-06-从尾到头打印链表-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211026剑指 Offer 06. 从尾到头打印链表 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211026<a href="https://leetcode-cn.com/problems/cong-wei-dao-tou-da-yin-lian-biao-lcof/">剑指 Offer 06. 从尾到头打印链表 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>链表反转，怎么表示一个链表?</p><p>指针，或者是用类来写链表的每一个node，包含key、pre、next，但怎么访问下一个值呢？</p><p>或者用结构体，结构体指针指向向下一个node，每次把链表的相连处断开，用temp存储临时断开的指针，在重新连到合适的位置上。</p><p>但要怎么拿到最后一个node呢？从前向后访问到尾部，复杂度。</p><h2 id="20211028"><a href="#20211028" class="headerlink" title="20211028"></a>20211028</h2><p>从头开始以链表中的每一个元素为单元，改变它的next就可以了。</p><p>对于head指针，若head指向了空，则是一个空链表。</p><p>对于第一个有数据的节点，将除开他的剩余部分先存下来，将其next赋值为NULL。</p><p>对于剩下的部分，若为NULL说明已经到达了，链表的尾部，可以结束扫描。</p><p>否则将剩余部分中第一个node的指针指向上一个已经处理过的node。</p><h2 id="忽略点-2"><a href="#忽略点-2" class="headerlink" title="忽略点"></a>忽略点</h2><p>比我想得简单，每一个node都带有数据。</p><blockquote><p>纠结的是链表的格式，传入的参数是头节点的指针，此指针可以指向数据域和域。用阶梯状那样的链表来在脑中可视化</p></blockquote><h1 id="20211029剑指-Offer-35-复杂链表的复制-力扣（LeetCode）-leetcode-cn-com"><a href="#20211029剑指-Offer-35-复杂链表的复制-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211029剑指 Offer 35. 复杂链表的复制 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211029<a href="https://leetcode-cn.com/problems/fu-za-lian-biao-de-fu-zhi-lcof/">剑指 Offer 35. 复杂链表的复制 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>看到题目的感觉是直接记录下一个节点的指针和random的值，但是random的类型是node型指针。怎么把*和出现的顺序的index结合起来呢？</p><h3 id="官方题解"><a href="#官方题解" class="headerlink" title="官方题解"></a>官方题解</h3><p>对一个特殊的链表进行深拷贝。如果是普通链表，可以直接按照遍历的顺序创建链表节点。</p><p>本题中因为随机指针的存在，当我们拷贝节点时，「当前节点的随机指针指向的节点」可能还没创建，因此我们需要变换思路。</p><p>一个可行方案是，我们利用回溯的方式，让每个节点的拷贝操作相互独立。</p><p>对于<strong>当前节点</strong>，我们首先要进行<strong>拷贝</strong>，然后我们进行「当前节点的后继节点」和「当前节点的随机指针指向的节点」拷贝，拷贝完成后将创建的<strong>新节点的指针返回</strong>，即可完成当前节点的两指针的赋值。</p><blockquote><p>读完这个话也想不到应该怎么做呢？</p></blockquote><p>具体地，我们<strong>用哈希表记录</strong>每一个节点<strong>对应新节点</strong>的创建情况，</p><p>遍历该链表的过程中，我们检查「当前节点的后继节点」和「当前节点的随机指针指向的节点」的创建情况。</p><p>如果这两个节点中的任何一个节点的新节点没有被创建，我们都立刻<strong>递归地进行创建</strong>。</p><p>当我们拷贝完成，回溯到当前层时，我们即可完成当前节点的指针赋值。注意一个节点可能被多个其他节点指向，因此我们可能递归地多次尝试拷贝某个节点，为了防止重复拷贝，我们需要首先检查当前节点是否被拷贝过，如果已经拷贝过，我们可以直接从哈希表中取出拷贝后的节点的指针并返回即可。</p><h2 id="20211030"><a href="#20211030" class="headerlink" title="20211030"></a>20211030</h2><p>假如不是一个复杂链表，对于一个简单链表的复制,</p><blockquote><p>算法笔记上是建立一个链表，他传入的参数是val的数组，而不是复制一个链表。</p></blockquote><p>能想到的是扫描老链表，用它的val来新建立链表。如果直接指针复制的话，那各种指针就是乱的。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">// Definition for a Node.</span></span><br><span class="line"><span class="comment">class Node &#123;</span></span><br><span class="line"><span class="comment">public:</span></span><br><span class="line"><span class="comment">    int val;</span></span><br><span class="line"><span class="comment">    Node* next;</span></span><br><span class="line"><span class="comment">    </span></span><br><span class="line"><span class="comment">    Node(int _val) &#123;</span></span><br><span class="line"><span class="comment">        val = _val;</span></span><br><span class="line"><span class="comment">        next = NULL;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">&#125;;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">Node* <span class="title">buildList</span><span class="params">(Node* head)</span> </span>&#123;</span><br><span class="line">        Node* new_list_cur = <span class="keyword">new</span> Node;</span><br><span class="line">        Node* newHead = new_list_cur;</span><br><span class="line">        Node* input_cur = head;</span><br><span class="line">        <span class="keyword">while</span>(input_cur!=<span class="literal">NULL</span>)&#123;</span><br><span class="line">            new_list_cur.val = input_cur.val;</span><br><span class="line">            new_list_cur.next =  <span class="keyword">new</span> Node;</span><br><span class="line">            input_cur = input_cur.next;</span><br><span class="line">            new_list_cur = new_list_cur.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> newHead;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>官方给了两种思路</p><h2 id="官方思路一"><a href="#官方思路一" class="headerlink" title="官方思路一"></a>官方思路一</h2><p>1.复制链表节点，两者建立map映射，key为输入链表节点，value为输出链表节点，一轮循环。</p><p>2.第二轮遍历，链接节点链接起来，对于新复制的节点，对于random和next使用输入链表的节点赋值。</p><p>两轮遍历复杂度O(N),使用map，空间复杂度O(N).</p><h2 id="官方思路二"><a href="#官方思路二" class="headerlink" title="官方思路二"></a>官方思路二</h2><p><a href="https://leetcode-cn.com/problems/fu-za-lian-biao-de-fu-zhi-lcof/comments/249391">https://leetcode-cn.com/problems/fu-za-lian-biao-de-fu-zhi-lcof/comments/249391</a></p><p>1.第一轮遍历，在每一个节点后边复制一个一样值的新节点。</p><p>2.第二轮遍历，对于复制节点，赋值随即指针（假如没有random，就不赋值）。</p><p>3.第三轮遍历，链表拆分，stride的设计。对于原链表的尾节点需要单独处理。</p><p><a href="https://leetcode-cn.com/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof/">剑指 Offer 03. 数组中重复的数字 - 力扣（LeetCode） (leetcode-cn.com)</a></p><h1 id="20211102剑指-Offer-03-数组中重复的数字-力扣（LeetCode）-leetcode-cn-com"><a href="#20211102剑指-Offer-03-数组中重复的数字-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211102剑指 Offer 03. 数组中重复的数字 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211102<a href="https://leetcode-cn.com/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof/">剑指 Offer 03. 数组中重复的数字 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>​    error: variable-sized object may not be initialized?这个提示是：变量大小的对象不能被初始化, const int max =100000;</p><h2 id="剑指-Offer-03-数组中重复的数字（哈希表-原地交换，清晰图解）-数组中重复的数字-力扣（LeetCode）-leetcode-cn-com"><a href="#剑指-Offer-03-数组中重复的数字（哈希表-原地交换，清晰图解）-数组中重复的数字-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="剑指 Offer 03. 数组中重复的数字（哈希表 / 原地交换，清晰图解） - 数组中重复的数字 - 力扣（LeetCode） (leetcode-cn.com)"></a><a href="https://leetcode-cn.com/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof/solution/mian-shi-ti-03-shu-zu-zhong-zhong-fu-de-shu-zi-yua/">剑指 Offer 03. 数组中重复的数字（哈希表 / 原地交换，清晰图解） - 数组中重复的数字 - 力扣（LeetCode） (leetcode-cn.com)</a></h2><p>好思路，从前向后排字典，后边在遇到重复的就可以发现了。</p><p>遍历数组 numsnums ，设索引初始值为 i = 0i=0 :</p><p>若 nums[i] = inums[i]=i ： 说明此数字已在对应索引位置，无需交换，因此跳过；<br>若 nums[nums[i]] = nums[i]nums[nums[i]]=nums[i] ： 代表索引 nums[i]nums[i] 处和索引 ii 处的元素值都为 nums[i]nums[i] ，即找到一组重复值，返回此值 nums[i]nums[i] ；<br>否则： 交换索引为 ii 和 nums[i]nums[i] 的元素值，将此数字交换至对应索引位置。</p><p>作者：jyd<br>链接：<a href="https://leetcode-cn.com/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof/solution/mian-shi-ti-03-shu-zu-zhong-zhong-fu-de-shu-zi-yua/">https://leetcode-cn.com/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof/solution/mian-shi-ti-03-shu-zu-zhong-zhong-fu-de-shu-zi-yua/</a><br>来源：力扣（LeetCode）<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><h1 id="20211102剑指-Offer-53-I-在排序数组中查找数字-I-力扣（LeetCode）-leetcode-cn-com"><a href="#20211102剑指-Offer-53-I-在排序数组中查找数字-I-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211102剑指 Offer 53 - I. 在排序数组中查找数字 I - 力扣（LeetCode） (leetcode-cn.com)"></a>20211102<a href="https://leetcode-cn.com/problems/zai-pai-xu-shu-zu-zhong-cha-zhao-shu-zi-lcof/">剑指 Offer 53 - I. 在排序数组中查找数字 I - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>此题在于怎么快速找到一个数字在排序中的位置。二分法。</p><h1 id="20211103剑指-Offer-53-II-0～n-1中缺失的数字-题解-力扣（LeetCode）-leetcode-cn-com"><a href="#20211103剑指-Offer-53-II-0～n-1中缺失的数字-题解-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211103剑指 Offer 53 - II. 0～n-1中缺失的数字 题解 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211103<a href="https://leetcode-cn.com/problems/que-shi-de-shu-zi-lcof/solution/">剑指 Offer 53 - II. 0～n-1中缺失的数字 题解 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>从头到尾扫描，遇到不一样的，break。假如是最后一个错误，for循环里边有个i++，可以自动的处理这种情况。</p><h1 id="20211103剑指-Offer-04-二维数组中的查找-力扣（LeetCode）-leetcode-cn-com"><a href="#20211103剑指-Offer-04-二维数组中的查找-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211103剑指 Offer 04. 二维数组中的查找 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211103<a href="https://leetcode-cn.com/problems/er-wei-shu-zu-zhong-de-cha-zhao-lcof/">剑指 Offer 04. 二维数组中的查找 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>感觉应该用DFS，或者BFS，斜着扫描。</p><p><a href="https://leetcode-cn.com/problems/er-wei-shu-zu-zhong-de-cha-zhao-lcof/solution/mian-shi-ti-04-er-wei-shu-zu-zhong-de-cha-zhao-zuo/">面试题04. 二维数组中的查找（标志数，清晰图解） - 二维数组中的查找 - 力扣（LeetCode） (leetcode-cn.com)</a>旋转45°</p><h1 id="20211103剑指-Offer-11-旋转数组的最小数字-力扣（LeetCode）-leetcode-cn-com"><a href="#20211103剑指-Offer-11-旋转数组的最小数字-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211103剑指 Offer 11. 旋转数组的最小数字 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211103<a href="https://leetcode-cn.com/problems/xuan-zhuan-shu-zu-de-zui-xiao-shu-zi-lcof/">剑指 Offer 11. 旋转数组的最小数字 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>找到一个数字比左边小，比右边大。</p><p>20211105<a href="https://leetcode-cn.com/problems/xuan-zhuan-shu-zu-de-zui-xiao-shu-zi-lcof/">剑指 Offer 11. 旋转数组的最小数字 - 力扣（LeetCode） (leetcode-cn.com)</a></p><p>硬找，用二分。</p><h1 id="20211106-剑指-Offer-50-第一个只出现一次的字符-力扣（LeetCode）-leetcode-cn-com"><a href="#20211106-剑指-Offer-50-第一个只出现一次的字符-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211106 剑指 Offer 50. 第一个只出现一次的字符 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211106 <a href="https://leetcode-cn.com/problems/di-yi-ge-zhi-chu-xian-yi-ci-de-zi-fu-lcof/submissions/">剑指 Offer 50. 第一个只出现一次的字符 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>unordered_map,排序方式不是按照进入的顺序排的。</p><h2 id="哈希表，auto遍历"><a href="#哈希表，auto遍历" class="headerlink" title="哈希表，auto遍历"></a><a href="https://www.cnblogs.com/averyfork/p/14420238.html">哈希表，auto遍历</a></h2><p><strong>首先是c++中的哈希表和Python中的字典</strong></p><h1 id="20211108剑指-Offer-32-II-从上到下打印二叉树-II-力扣（LeetCode）-leetcode-cn-com"><a href="#20211108剑指-Offer-32-II-从上到下打印二叉树-II-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211108剑指 Offer 32 - II. 从上到下打印二叉树 II - 力扣（LeetCode） (leetcode-cn.com)"></a>20211108<a href="https://leetcode-cn.com/problems/cong-shang-dao-xia-da-yin-er-cha-shu-ii-lcof/">剑指 Offer 32 - II. 从上到下打印二叉树 II - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>怎么记录每一层的高度呢？</p><p>分层打印的思路，一层开始时候，队列里边的元素是当前层的。<a href="https://leetcode-cn.com/problems/cong-shang-dao-xia-da-yin-er-cha-shu-ii-lcof/solution/mian-shi-ti-32-ii-cong-shang-dao-xia-da-yin-er-c-5/">面试题32 - II. 从上到下打印二叉树 II（层序遍历 BFS，清晰图解） - 从上到下打印二叉树 II - 力扣（LeetCode） (leetcode-cn.com)</a></p><p>dfs维护层数递归<a href="https://leetcode-cn.com/problems/cong-shang-dao-xia-da-yin-er-cha-shu-ii-lcof/solution/c-ceng-xu-bian-li-dfsbfs-tao-mo-ban-jiu-wan-shi-li/">C++, 层序遍历, DFS+BFS, 套模板就完事了 - 从上到下打印二叉树 II - 力扣（LeetCode） (leetcode-cn.com)</a></p><h1 id="20211108剑指-Offer-32-III-从上到下打印二叉树-III-力扣（LeetCode）-leetcode-cn-com"><a href="#20211108剑指-Offer-32-III-从上到下打印二叉树-III-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211108剑指 Offer 32 - III. 从上到下打印二叉树 III - 力扣（LeetCode） (leetcode-cn.com)"></a>20211108<a href="https://leetcode-cn.com/problems/cong-shang-dao-xia-da-yin-er-cha-shu-iii-lcof/">剑指 Offer 32 - III. 从上到下打印二叉树 III - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>想到的是仅在tmp完全存储后，用stack来颠倒数据。</p><h1 id="20211109递归方式解决-树的子结构-力扣（LeetCode）-leetcode-cn-com"><a href="#20211109递归方式解决-树的子结构-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211109递归方式解决 - 树的子结构 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211109<a href="https://leetcode-cn.com/problems/shu-de-zi-jie-gou-lcof/solution/di-gui-fang-shi-jie-jue-by-sdwwld/">递归方式解决 - 树的子结构 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><h2 id="二叉树题解总结"><a href="#二叉树题解总结" class="headerlink" title="二叉树题解总结"></a>二叉树题解总结</h2><p><a href="https://leetcode-cn.com/problems/shu-de-zi-jie-gou-lcof/solution/yi-pian-wen-zhang-dai-ni-chi-tou-dui-che-uhgs/">一篇文章带你吃透对称性递归(思路分析+解题模板+案例解读) - 树的子结构 - 力扣（LeetCode） (leetcode-cn.com)</a></p><h1 id="20211109剑指-Offer-27-二叉树的镜像（递归-辅助栈，清晰图解）-二叉树的镜像-力扣（LeetCode）-leetcode-cn-com"><a href="#20211109剑指-Offer-27-二叉树的镜像（递归-辅助栈，清晰图解）-二叉树的镜像-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211109剑指 Offer 27. 二叉树的镜像（递归 / 辅助栈，清晰图解） - 二叉树的镜像 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211109<a href="https://leetcode-cn.com/problems/er-cha-shu-de-jing-xiang-lcof/solution/mian-shi-ti-27-er-cha-shu-de-jing-xiang-di-gui-fu-/">剑指 Offer 27. 二叉树的镜像（递归 / 辅助栈，清晰图解） - 二叉树的镜像 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>递归。用栈。</p><p><a href="https://leetcode-cn.com/problems/er-cha-shu-de-jing-xiang-lcof/solution/4chong-jie-jue-fang-shi-bfsdfszhong-xu-bian-li-di-/">4种解决方式（BFS，DFS，中序遍历，递归方式），最好的击败了100%的用户 - 二叉树的镜像 - 力扣（LeetCode） (leetcode-cn.com)</a></p><p>还有一些其他的方法。</p><h1 id="20211109剑指-Offer-28-对称的二叉树-力扣（LeetCode）-leetcode-cn-com"><a href="#20211109剑指-Offer-28-对称的二叉树-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211109剑指 Offer 28. 对称的二叉树 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211109<a href="https://leetcode-cn.com/problems/dui-cheng-de-er-cha-shu-lcof/">剑指 Offer 28. 对称的二叉树 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>对称二叉树，镜像二叉树后，看是不是一样的结构。</p><h2 id="递归思路与上题类似，递归不能想太多啊-对称的二叉树-力扣（LeetCode）-leetcode-cn-com"><a href="#递归思路与上题类似，递归不能想太多啊-对称的二叉树-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="递归思路与上题类似，递归不能想太多啊 - 对称的二叉树 - 力扣（LeetCode） (leetcode-cn.com)"></a>递归思路<a href="https://leetcode-cn.com/problems/dui-cheng-de-er-cha-shu-lcof/solution/yu-shang-ti-lei-si-di-gui-bu-neng-xiang-5ts8h/">与上题类似，递归不能想太多啊 - 对称的二叉树 - 力扣（LeetCode） (leetcode-cn.com)</a></h2><h1 id="20211109动态规划套路详解-斐波那契数-力扣（LeetCode）-leetcode-cn-com"><a href="#20211109动态规划套路详解-斐波那契数-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211109动态规划套路详解 - 斐波那契数 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211109<a href="https://leetcode-cn.com/problems/fibonacci-number/solution/dong-tai-gui-hua-tao-lu-xiang-jie-by-labuladong/">动态规划套路详解 - 斐波那契数 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><h2 id="动态规划套路详解"><a href="#动态规划套路详解" class="headerlink" title="动态规划套路详解"></a>动态规划套路详解</h2><p>首先，动态规划问题的一般形式就是求最值。动态规划其实是运筹学的一种最优化方法，只不过在计算机问题上应用比较多，比如说让你求最长递增子序列呀，最小编辑距离呀等等。</p><p>重叠子问题。</p><p>最优子结构。</p><p>正确的状态转移方程。</p><p><strong>明确 base case -&gt; 明确「状态」-&gt; 明确「选择」 -&gt; 定义 dp 数组/函数的含义</strong>。</p><p>但凡遇到需要递归的问题，最好都画出递归树，这对你分析算法的复杂度，寻找算法低效的原因都有巨大帮助。</p><p>要符合「最优子结构」，子问题间必须互相独立。</p><p>确定 base case<strong>。确定「状态」，也就是原问题和子问题中会变化的变量。</strong>确定「选择」，也就是导致「状态」产生变化的行为<strong>。</strong>明确 <code>dp</code> 函数/数组的定义。一般来说函数的参数就是状态转移中会变化的量，也就是上面说到的「状态」；函数的返回值就是题目要求我们计算的量，与选择有关系的。</p><h2 id="vector初始化"><a href="#vector初始化" class="headerlink" title="vector初始化"></a>vector初始化</h2><p><a href="https://blog.csdn.net/qq_40147449/article/details/87892312">(12条消息) C++ vector的初始化_锤某的博客-CSDN博客_c++ vector 初始化</a></p><h1 id="20211112"><a href="#20211112" class="headerlink" title="20211112"></a>20211112</h1><p><a href="https://cloud.tencent.com/developer/article/1537457">C++ std::vector::resize() 方法解析（菜鸟看了秒懂） - 云+社区 - 腾讯云 (tencent.com)</a></p><p><a href="https://leetcode-cn.com/problems/coin-change/solution/322-by-ikaruga/">【零钱兑换】贪心 + dfs = 8ms （更新） - 零钱兑换 - 力扣（LeetCode） (leetcode-cn.com)</a></p><h1 id="20211113青蛙跳台阶剑指-Offer-10-II-青蛙跳台阶问题-力扣（LeetCode）-leetcode-cn-com"><a href="#20211113青蛙跳台阶剑指-Offer-10-II-青蛙跳台阶问题-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211113青蛙跳台阶剑指 Offer 10- II. 青蛙跳台阶问题 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211113青蛙跳台阶<a href="https://leetcode-cn.com/problems/qing-wa-tiao-tai-jie-wen-ti-lcof/">剑指 Offer 10- II. 青蛙跳台阶问题 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>1、<strong>确定base case</strong>，这个很简单，显然目标金额 amount 为 0 时算法返回 0，因为不需要任何硬币就已经凑出目标金额了。</p><p>2、确定「状态」，也就是<strong>原问题和子问题</strong>中会变化的变量。由于硬币数量无限，硬币的面额也是题目给定的，只有目标金额会不断地向 <strong>base case 靠近</strong>，所以唯一的「状态」就是<strong>目标金额 amount</strong>。</p><p>3、确定「选择」，也就是导致「状态」产生变化的行为。目标金额为什么变化呢，因为你在选择硬币，你每选择一枚硬币，就相当于减少了目标金额。所以说所有硬币的面值，就是你的「选择」。</p><p>4、明确 dp 函数/数组的定义。我们这里讲的是自顶向下的解法，所以会有一个递归的 dp 函数，一般来说函<strong>数的参数就是状态转移中会变化的量，也就是上面说到的「状态」</strong>；函数的返回值就是题目要求我们计算的量。就本题来说，状态只有一个，即「目标金额」，题目要求我们计算凑出目标金额所需的最少硬币数量。所以我们可以这样定义 dp 函数：</p><p>dp(n) 的定义：输入一个目标金额 n，返回凑出目标金额 n 的最少硬币数量。</p><p>是一个fib题目。</p><h1 id="20211113剑指-Offer-63-股票的最大利润-力扣（LeetCode）-leetcode-cn-com"><a href="#20211113剑指-Offer-63-股票的最大利润-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211113剑指 Offer 63. 股票的最大利润 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211113<a href="https://leetcode-cn.com/problems/gu-piao-de-zui-da-li-run-lcof/">剑指 Offer 63. 股票的最大利润 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>硬算，记录两个数字的差值，从前到后扫描是n(n-1)/2。</p><p>base</p><p>状态 i天数</p><p>选择</p><h2 id="找到最小值，在最小值之后看是否会涨价。"><a href="#找到最小值，在最小值之后看是否会涨价。" class="headerlink" title="找到最小值，在最小值之后看是否会涨价。"></a>找到最小值，在最小值之后看是否会涨价。</h2><h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><ul><li><p>状态定义： 设动态规划列表 dp ，dp[i] 代表以 prices[i]为结尾的子数组的最大利润（以下简称为 前 i 日的最大利润 ）。</p></li><li><p>转移方程： 由于题目限定 “买卖该股票一次” ，因此前 i 日最大利润 dp[i]等于前 i - 1 日最大利润 dp[i-1]和第 i 日卖出的最大利润中的最大值。</p></li><li><p><strong>初始状态：</strong> dp[0] = 0，即首日利润为 0 ；</p></li><li><strong>返回值：</strong> dp[n - 1] ，其中 <em>n</em> 为 <em>dp</em> 列表长度。</li></ul><h1 id="20211114-贪心-分治-动态规划法-面试题42-连续子数组的最大和-连续子数组的最大和-力扣（LeetCode）-leetcode-cn-com"><a href="#20211114-贪心-分治-动态规划法-面试题42-连续子数组的最大和-连续子数组的最大和-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211114 贪心+分治+动态规划法_面试题42. 连续子数组的最大和 - 连续子数组的最大和 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211114 <a href="https://leetcode-cn.com/problems/lian-xu-zi-shu-zu-de-zui-da-he-lcof/solution/tan-xin-fen-zhi-dong-tai-gui-hua-fa-by-luo-jing-yu/">贪心+分治+动态规划法_面试题42. 连续子数组的最大和 - 连续子数组的最大和 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>分治没怎么明白</p><h1 id="20211114十大排序算法-背诵版-动图-力扣（LeetCode）-leetcode-cn-com"><a href="#20211114十大排序算法-背诵版-动图-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211114十大排序算法(背诵版+动图) - 力扣（LeetCode） (leetcode-cn.com)"></a>20211114<a href="https://leetcode-cn.com/circle/article/0akb5U/">十大排序算法(背诵版+动图) - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><a href="https://www.iamshuaidi.com/">帅地玩编程-校招|面试|学习路线，你都可以在这里找到 (iamshuaidi.com)</a>面试经验</p><h1 id="20211115对链表进行插入排序-对链表进行插入排序-力扣（LeetCode）-leetcode-cn-com"><a href="#20211115对链表进行插入排序-对链表进行插入排序-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211115对链表进行插入排序 - 对链表进行插入排序 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211115<a href="https://leetcode-cn.com/problems/insertion-sort-list/solution/dui-lian-biao-jin-xing-cha-ru-pai-xu-by-leetcode-s/">对链表进行插入排序 - 对链表进行插入排序 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>0.若无空，直接返回。</p><p>1.哑节点，可以在head前插入节点。</p><p>2.lastSorted，已经排序部分的最后一个节点，开始为头节点。</p><p>3.维护待插入元素curr，开始时候为头节点下一个节点。</p><p>4.比较lastSorted和curr的节点值，开始时候为head.next。</p><p>a.lastSorted-&gt;val比curr-&gt;还小&lt;=，直接把lastSorted后移一位。</p><p>b.否则从链表头向后遍历,找到插入curr的位置，prev为插入curr位置的前一个结点。</p><p>5.令 <code>curr = lastSorted.next</code>，此时 <code>curr</code> 为下一个待插入的元素。</p><p>6.重复4、5，直到curr变成空。</p><p>7.返回哑节点的下一位置。</p><h1 id="20211116"><a href="#20211116" class="headerlink" title="20211116"></a>20211116</h1><p><a href="https://leetcode-cn.com/circle/discuss/fKBJcm/">内推｜字节跳动｜多项岗位｜北京+上海 - 力扣（LeetCode） (leetcode-cn.com)</a></p><p>不是每个题目都有参考的，要泛化呀。</p><h1 id="20211117剑指-Offer-46-把数字翻译成字符串-力扣（LeetCode）-leetcode-cn-com"><a href="#20211117剑指-Offer-46-把数字翻译成字符串-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211117剑指 Offer 46. 把数字翻译成字符串 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211117<a href="https://leetcode-cn.com/problems/ba-shu-zi-fan-yi-cheng-zi-fu-chuan-lcof/">剑指 Offer 46. 把数字翻译成字符串 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>类似于青蛙跳台阶，一次可以跳一格还是跳两格，是每次用一个字符还是两个字符。</p><p>两个字符可能会无效，必须小于25。</p><h2 id="to-string-函数"><a href="#to-string-函数" class="headerlink" title="to_string 函数"></a>to_string 函数</h2><p><a href="https://blog.csdn.net/liitdar/article/details/81145791">(17条消息) C++编程语言中整型转换为字符串类型的方法_liitdar的博客-CSDN博客_c++整型转字符串</a></p><h1 id="20211119面试题25-合并两个排序的链表（伪头节点，清晰图解）-合并两个排序的链表-力扣（LeetCode）-leetcode-cn-com"><a href="#20211119面试题25-合并两个排序的链表（伪头节点，清晰图解）-合并两个排序的链表-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211119面试题25. 合并两个排序的链表（伪头节点，清晰图解） - 合并两个排序的链表 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211119<a href="https://leetcode-cn.com/problems/he-bing-liang-ge-pai-xu-de-lian-biao-lcof/submissions/">面试题25. 合并两个排序的链表（伪头节点，清晰图解） - 合并两个排序的链表 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>双指针</p><h1 id="20211120【一句话，两张图】优雅理解原理-为何这样设计-两个链表的第一个公共节点-力扣（LeetCode）-leetcode-cn-com"><a href="#20211120【一句话，两张图】优雅理解原理-为何这样设计-两个链表的第一个公共节点-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211120【一句话，两张图】优雅理解原理 为何这样设计 - 两个链表的第一个公共节点 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211120<a href="https://leetcode-cn.com/problems/liang-ge-lian-biao-de-di-yi-ge-gong-gong-jie-dian-lcof/solution/yi-zhang-tu-jiu-ming-bai-ai-qing-jie-shi-up3a/">【一句话，两张图】优雅理解原理 为何这样设计 - 两个链表的第一个公共节点 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>比较地址。</p><h1 id="20211120剑指-Offer-21-调整数组顺序使奇数位于偶数前面（双指针，清晰图解）-调整数组顺序使奇数位于偶数前面-力扣（LeetCode）-leetcode-cn-com"><a href="#20211120剑指-Offer-21-调整数组顺序使奇数位于偶数前面（双指针，清晰图解）-调整数组顺序使奇数位于偶数前面-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211120剑指 Offer 21. 调整数组顺序使奇数位于偶数前面（双指针，清晰图解） - 调整数组顺序使奇数位于偶数前面 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211120<a href="https://leetcode-cn.com/problems/diao-zheng-shu-zu-shun-xu-shi-qi-shu-wei-yu-ou-shu-qian-mian-lcof/solution/mian-shi-ti-21-diao-zheng-shu-zu-shun-xu-shi-qi-4/">剑指 Offer 21. 调整数组顺序使奇数位于偶数前面（双指针，清晰图解） - 调整数组顺序使奇数位于偶数前面 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>快慢指针 用于调整数组位置。</p><p><a href="https://leetcode-cn.com/problems/diao-zheng-shu-zu-shun-xu-shi-qi-shu-wei-yu-ou-shu-qian-mian-lcof/solution/ti-jie-shou-wei-shuang-zhi-zhen-kuai-man-shuang-zh/">【题解】：首尾双指针，快慢双指针 - 调整数组顺序使奇数位于偶数前面 - 力扣（LeetCode） (leetcode-cn.com)</a></p><h1 id="20211120-面试题57-和为-s-的两个数字（双指针-证明，清晰图解）-和为s的两个数字-力扣（LeetCode）-leetcode-cn-com"><a href="#20211120-面试题57-和为-s-的两个数字（双指针-证明，清晰图解）-和为s的两个数字-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211120 面试题57. 和为 s 的两个数字（双指针 + 证明，清晰图解） - 和为s的两个数字 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211120 <a href="https://leetcode-cn.com/problems/he-wei-sde-liang-ge-shu-zi-lcof/solution/mian-shi-ti-57-he-wei-s-de-liang-ge-shu-zi-shuang-/">面试题57. 和为 s 的两个数字（双指针 + 证明，清晰图解） - 和为s的两个数字 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>相当于一种剪枝。</p><h1 id="20211125礼物的最大价值-礼物的最大价值-力扣（LeetCode）-leetcode-cn-com"><a href="#20211125礼物的最大价值-礼物的最大价值-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211125礼物的最大价值 - 礼物的最大价值 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211125<a href="https://leetcode-cn.com/problems/li-wu-de-zui-da-jie-zhi-lcof/solution/li-wu-de-zui-da-jie-zhi-by-yxiaojian/">礼物的最大价值 - 礼物的最大价值 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>除了使用dp之外，还可以使用dfs。</p><p><a href="https://leetcode-cn.com/problems/li-wu-de-zui-da-jie-zhi-lcof/solution/li-wu-de-zui-da-jie-zhi-by-yxiaojian/">礼物的最大价值 - 礼物的最大价值 - 力扣（LeetCode） (leetcode-cn.com)</a></p><p>dp是从上到下，dfs是从下到上回溯。</p><h1 id="20211127三种方法：-DFS-BFS-回溯（递归-迭代）-二叉树中和为某一值的路径-力扣（LeetCode）-leetcode-cn-com-二叉树路径寻找"><a href="#20211127三种方法：-DFS-BFS-回溯（递归-迭代）-二叉树中和为某一值的路径-力扣（LeetCode）-leetcode-cn-com-二叉树路径寻找" class="headerlink" title="20211127三种方法： DFS, BFS, 回溯（递归 + 迭代） - 二叉树中和为某一值的路径 - 力扣（LeetCode） (leetcode-cn.com)二叉树路径寻找"></a>20211127<a href="https://leetcode-cn.com/problems/er-cha-shu-zhong-he-wei-mou-yi-zhi-de-lu-jing-lcof/solution/san-chong-fang-fa-dfs-bfs-hui-su-di-gui-die-dai-by/">三种方法： DFS, BFS, 回溯（递归 + 迭代） - 二叉树中和为某一值的路径 - 力扣（LeetCode） (leetcode-cn.com)</a>二叉树路径寻找</h1><h1 id="20211129-二叉树的最近公共祖先-二叉树的最近公共祖先-力扣（LeetCode）-leetcode-cn-com"><a href="#20211129-二叉树的最近公共祖先-二叉树的最近公共祖先-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211129 二叉树的最近公共祖先 - 二叉树的最近公共祖先 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211129 <a href="https://leetcode-cn.com/problems/er-cha-shu-de-zui-jin-gong-gong-zu-xian-lcof/solution/er-cha-shu-de-zui-jin-gong-gong-zu-xian-6fdt7/">二叉树的最近公共祖先 - 二叉树的最近公共祖先 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><a href="https://leetcode-cn.com/problems/er-cha-shu-de-zui-jin-gong-gong-zu-xian-lcof/solution/mian-shi-ti-68-ii-er-cha-shu-de-zui-jin-gong-gon-7/512814">https://leetcode-cn.com/problems/er-cha-shu-de-zui-jin-gong-gong-zu-xian-lcof/solution/mian-shi-ti-68-ii-er-cha-shu-de-zui-jin-gong-gon-7/512814</a></p><p><a href="https://leetcode-cn.com/problems/er-cha-shu-de-zui-jin-gong-gong-zu-xian-lcof/solution/mian-shi-ti-68-ii-er-cha-shu-de-zui-jin-gong-gon-7/1076018">https://leetcode-cn.com/problems/er-cha-shu-de-zui-jin-gong-gong-zu-xian-lcof/solution/mian-shi-ti-68-ii-er-cha-shu-de-zui-jin-gong-gon-7/1076018</a></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1.</span> 若树里面存在p，也存在q，则返回他们的公共祖先。</span><br><span class="line"><span class="bullet">2.</span> 若树里面只存在p，或只存在q，则返回存在的那一个。</span><br><span class="line"><span class="bullet">3.</span> 若树里面既不存在p，也不存在q，则返回null。</span><br></pre></td></tr></table></figure><h1 id="20211129-11-盛最多水的容器（双指针，清晰图解）-盛最多水的容器-力扣（LeetCode）-leetcode-cn-com"><a href="#20211129-11-盛最多水的容器（双指针，清晰图解）-盛最多水的容器-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211129  11. 盛最多水的容器（双指针，清晰图解） - 盛最多水的容器 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211129  <a href="https://leetcode-cn.com/problems/container-with-most-water/solution/container-with-most-water-shuang-zhi-zhen-fa-yi-do/">11. 盛最多水的容器（双指针，清晰图解） - 盛最多水的容器 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><h1 id="2021130剑指-Offer-07-重建二叉树（分治算法，清晰图解）-重建二叉树-力扣（LeetCode）-leetcode-cn-com"><a href="#2021130剑指-Offer-07-重建二叉树（分治算法，清晰图解）-重建二叉树-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="2021130剑指 Offer 07. 重建二叉树（分治算法，清晰图解） - 重建二叉树 - 力扣（LeetCode） (leetcode-cn.com)"></a>2021130<a href="https://leetcode-cn.com/problems/zhong-jian-er-cha-shu-lcof/solution/mian-shi-ti-07-zhong-jian-er-cha-shu-di-gui-fa-qin/">剑指 Offer 07. 重建二叉树（分治算法，清晰图解） - 重建二叉树 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><h1 id="20211202面试题55-II-平衡二叉树（从底至顶、从顶至底，清晰图解）-平衡二叉树-力扣（LeetCode）-leetcode-cn-com"><a href="#20211202面试题55-II-平衡二叉树（从底至顶、从顶至底，清晰图解）-平衡二叉树-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211202面试题55 - II. 平衡二叉树（从底至顶、从顶至底，清晰图解） - 平衡二叉树 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211202<a href="https://leetcode-cn.com/problems/ping-heng-er-cha-shu-lcof/solution/mian-shi-ti-55-ii-ping-heng-er-cha-shu-cong-di-zhi/">面试题55 - II. 平衡二叉树（从底至顶、从顶至底，清晰图解） - 平衡二叉树 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><h1 id="202112024种解法秒杀TopK（快排-堆-二叉搜索树-计数排序）❤️-最小的k个数-力扣（LeetCode）-leetcode-cn-com"><a href="#202112024种解法秒杀TopK（快排-堆-二叉搜索树-计数排序）❤️-最小的k个数-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="202112024种解法秒杀TopK（快排/堆/二叉搜索树/计数排序）❤️ - 最小的k个数 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211202<a href="https://leetcode-cn.com/problems/zui-xiao-de-kge-shu-lcof/solution/3chong-jie-fa-miao-sha-topkkuai-pai-dui-er-cha-sou/">4种解法秒杀TopK（快排/堆/二叉搜索树/计数排序）❤️ - 最小的k个数 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><a href="https://vimsky.com/examples/usage/vector-assign-in-c-stl.html#:~:text=vector%3A%3Aassign (">C++ vector::assign()用法及代码示例 - 纯净天空 (vimsky.com)</a>是C%2B%2B中的STL，它通过替换旧元素为向量元素分配新值。 如果需要，它也可以修改向量的大小。 size-必须从头开始分配的元素数。 first,- 输入迭代器到初始位置范围。 last - 输入迭代器到最终位置范围。)</p><h1 id="20211209【图解】我们之前可能没有搞懂这个题-把数组排成最小的数-力扣（LeetCode）-leetcode-cn-com"><a href="#20211209【图解】我们之前可能没有搞懂这个题-把数组排成最小的数-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211209【图解】我们之前可能没有搞懂这个题 - 把数组排成最小的数 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211209<a href="https://leetcode-cn.com/problems/ba-shu-zu-pai-cheng-zui-xiao-de-shu-lcof/solution/tu-jie-wo-men-zhi-qian-ke-neng-mei-you-g-gcr3/">【图解】我们之前可能没有搞懂这个题 - 把数组排成最小的数 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><a href="https://leetcode-cn.com/problems/ba-shu-zu-pai-cheng-zui-xiao-de-shu-lcof/solution/tu-jie-wo-men-zhi-qian-ke-neng-mei-you-g-gcr3/">【图解】我们之前可能没有搞懂这个题 - 把数组排成最小的数 - 力扣（LeetCode） (leetcode-cn.com)</a></p><p>正确性、然后反正。</p><h1 id="20211210强连通分量"><a href="#20211210强连通分量" class="headerlink" title="20211210强连通分量"></a>20211210强连通分量</h1><p><a href="https://leetcode-cn.com/circle/article/4z7ODk/">【MIX】强连通分量(1) Tarjan SCC 缩点 - 力扣（LeetCode） (leetcode-cn.com)</a></p><p><a href="https://leetcode-cn.com/circle/article/NQumVG/#前文链接-强连通分量1">【MIX】强连通分量(2) Kosaraju &amp; 扩展 - 力扣（LeetCode） (leetcode-cn.com)</a></p><p><a href="https://leetcode-cn.com/circle/article/caJVAv/">图的相关算法 - 力扣（LeetCode） (leetcode-cn.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/58585315">leetcode刷题（四）：搜索(深度优先搜索,广度优先搜索）拓扑排序，强连通分量 - 知乎 (zhihu.com)</a></p><p><a href="https://leetcode-cn.com/problems/coloring-a-border/solution/ke-neng-shi-ke-du-xing-zui-qiang-de-ti-j-q6lu/">可能是可读性最强的题解。（TypeScript） - 边界着色 - 力扣（LeetCode） (leetcode-cn.com)</a></p><h1 id="20211211剑指-Offer-38-字符串的排列-力扣（LeetCode）-leetcode-cn-com"><a href="#20211211剑指-Offer-38-字符串的排列-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211211剑指 Offer 38. 字符串的排列 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211211<a href="https://leetcode-cn.com/problems/zi-fu-chuan-de-pai-lie-lcof/">剑指 Offer 38. 字符串的排列 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><a href="https://mp.weixin.qq.com/s/nMUHqvwzG2LmWA9jMIHwQQ">回溯算法详解（修订版） (qq.com)</a>讲的不错！</p><p><strong>1、</strong> <strong>路径</strong>：也就是已经做出的选择。</p><p><strong>2、选择列表</strong>：也就是你当前可以做的选择。</p><p><strong>3、结束条件</strong>：也就是到达决策树底层，无法再做选择的条件。\</p><p>请问大佬判断重复字符条件这里 !vis[j - 1] &amp;&amp; s[j - 1] == s[j] 是啥意思？</p><p><a href="https://leetcode-cn.com/problems/zi-fu-chuan-de-pai-lie-lcof/solution/zi-fu-chuan-de-pai-lie-by-leetcode-solut-hhvs/994831">https://leetcode-cn.com/problems/zi-fu-chuan-de-pai-lie-lcof/solution/zi-fu-chuan-de-pai-lie-by-leetcode-solut-hhvs/994831</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">result = []</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backtrack</span>(<span class="params">路径, 选择列表</span>):</span></span><br><span class="line">    <span class="keyword">if</span> 满足结束条件:</span><br><span class="line">        result.add(路径)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> 选择 <span class="keyword">in</span> 选择列表:</span><br><span class="line">        做选择</span><br><span class="line">        backtrack(路径, 选择列表)</span><br><span class="line">        撤销选择</span><br></pre></td></tr></table></figure><h1 id="20211211下一个排列算法详解：思路-推导-步骤，看不懂算我输！-下一个排列-力扣（LeetCode）-leetcode-cn-com"><a href="#20211211下一个排列算法详解：思路-推导-步骤，看不懂算我输！-下一个排列-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211211下一个排列算法详解：思路+推导+步骤，看不懂算我输！ - 下一个排列 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211211<a href="https://leetcode-cn.com/problems/next-permutation/solution/xia-yi-ge-pai-lie-suan-fa-xiang-jie-si-lu-tui-dao-/">下一个排列算法详解：思路+推导+步骤，看不懂算我输！ - 下一个排列 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><h1 id="20211213一个模板刷遍所有字符串句子题目！（归纳总结-分类模板-题目分析）-翻转单词顺序-力扣（LeetCode）-leetcode-cn-com"><a href="#20211213一个模板刷遍所有字符串句子题目！（归纳总结-分类模板-题目分析）-翻转单词顺序-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211213一个模板刷遍所有字符串句子题目！（归纳总结+分类模板+题目分析） - 翻转单词顺序 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211213<a href="https://leetcode-cn.com/problems/fan-zhuan-dan-ci-shun-xu-lcof/solution/yi-ge-mo-ban-shua-bian-suo-you-zi-fu-chu-x6vh/">一个模板刷遍所有字符串句子题目！（归纳总结+分类模板+题目分析） - 翻转单词顺序 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><h1 id="20211213剑指-Offer-31-栈的压入、弹出序列-力扣（LeetCode）-leetcode-cn-com"><a href="#20211213剑指-Offer-31-栈的压入、弹出序列-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211213剑指 Offer 31. 栈的压入、弹出序列 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211213<a href="https://leetcode-cn.com/problems/zhan-de-ya-ru-dan-chu-xu-lie-lcof/">剑指 Offer 31. 栈的压入、弹出序列 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><h1 id="https-leetcode-cn-com-problems-zhan-de-ya-ru-dan-chu-xu-lie-lcof-solution-mian-shi-ti-31-zhan-de-ya-ru-dan-chu-xu-lie-mo-n-2-507148"><a href="#https-leetcode-cn-com-problems-zhan-de-ya-ru-dan-chu-xu-lie-lcof-solution-mian-shi-ti-31-zhan-de-ya-ru-dan-chu-xu-lie-mo-n-2-507148" class="headerlink" title="https://leetcode-cn.com/problems/zhan-de-ya-ru-dan-chu-xu-lie-lcof/solution/mian-shi-ti-31-zhan-de-ya-ru-dan-chu-xu-lie-mo-n-2/507148"></a><a href="https://leetcode-cn.com/problems/zhan-de-ya-ru-dan-chu-xu-lie-lcof/solution/mian-shi-ti-31-zhan-de-ya-ru-dan-chu-xu-lie-mo-n-2/507148">https://leetcode-cn.com/problems/zhan-de-ya-ru-dan-chu-xu-lie-lcof/solution/mian-shi-ti-31-zhan-de-ya-ru-dan-chu-xu-lie-mo-n-2/507148</a></h1><h1 id="20211215剑指-Offer-14-I-剪绳子-力扣（LeetCode）-leetcode-cn-com"><a href="#20211215剑指-Offer-14-I-剪绳子-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211215剑指 Offer 14- I. 剪绳子 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211215<a href="https://leetcode-cn.com/problems/jian-sheng-zi-lcof/">剑指 Offer 14- I. 剪绳子 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><a href="https://leetcode-cn.com/problems/jian-sheng-zi-lcof/solution/jian-zhi-offer-14-i-jian-sheng-zi-huan-s-xopj/">剑指 Offer 14- I. 剪绳子，还是动态规划好理解，但是贪心真的快 - 剪绳子 - 力扣（LeetCode） (leetcode-cn.com)</a></p><p><a href="https://leetcode-cn.com/problems/integer-break/">343. 整数拆分 - 力扣（LeetCode） (leetcode-cn.com)</a></p><h1 id="20211215剑指-Offer-19-正则表达式匹配-力扣（LeetCode）-leetcode-cn-com"><a href="#20211215剑指-Offer-19-正则表达式匹配-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211215剑指 Offer 19. 正则表达式匹配 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211215<a href="https://leetcode-cn.com/problems/zheng-ze-biao-da-shi-pi-pei-lcof/">剑指 Offer 19. 正则表达式匹配 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><h1 id="20211213面试题29-顺时针打印矩阵（模拟、设定边界，清晰图解）-顺时针打印矩阵-力扣（LeetCode）-leetcode-cn-com"><a href="#20211213面试题29-顺时针打印矩阵（模拟、设定边界，清晰图解）-顺时针打印矩阵-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211213面试题29. 顺时针打印矩阵（模拟、设定边界，清晰图解） - 顺时针打印矩阵 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211213<a href="https://leetcode-cn.com/problems/shun-shi-zhen-da-yin-ju-zhen-lcof/solution/mian-shi-ti-29-shun-shi-zhen-da-yin-ju-zhen-she-di/">面试题29. 顺时针打印矩阵（模拟、设定边界，清晰图解） - 顺时针打印矩阵 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><h1 id="202111216面试题16-数值的整数次方（快速幂，清晰图解）-数值的整数次方-力扣（LeetCode）-leetcode-cn-com"><a href="#202111216面试题16-数值的整数次方（快速幂，清晰图解）-数值的整数次方-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="202111216面试题16. 数值的整数次方（快速幂，清晰图解） - 数值的整数次方 - 力扣（LeetCode） (leetcode-cn.com)"></a>202111216<a href="https://leetcode-cn.com/problems/shu-zhi-de-zheng-shu-ci-fang-lcof/solution/mian-shi-ti-16-shu-zhi-de-zheng-shu-ci-fang-kuai-s/">面试题16. 数值的整数次方（快速幂，清晰图解） - 数值的整数次方 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><img src="https://pic.leetcode-cn.com/379a042b9d8df3a96d1ac0f27346718033bf3bfce69731bab52bf6f372b4c8f4-Picture2.png" alt="Picture2.png"></p><p>n=5，res=3，x=3*3=9，n=2。</p><p>n =2，x=9*9=81，n=1。</p><p>n=1，res=81*3，x=81X81，n=0。</p><h1 id="20211218剑指-Offer-65-不用加减乘除做加法-力扣（LeetCode）-leetcode-cn-com"><a href="#20211218剑指-Offer-65-不用加减乘除做加法-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211218剑指 Offer 65. 不用加减乘除做加法 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211218<a href="https://leetcode-cn.com/problems/bu-yong-jia-jian-cheng-chu-zuo-jia-fa-lcof/">剑指 Offer 65. 不用加减乘除做加法 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><a href="https://leetcode-cn.com/problems/bu-yong-jia-jian-cheng-chu-zuo-jia-fa-lcof/comments/242474">https://leetcode-cn.com/problems/bu-yong-jia-jian-cheng-chu-zuo-jia-fa-lcof/comments/242474</a></p><p><a href="https://leetcode-cn.com/problems/bu-yong-jia-jian-cheng-chu-zuo-jia-fa-lcof/solution/mian-shi-ti-65-bu-yong-jia-jian-cheng-chu-zuo-ji-7/809000">https://leetcode-cn.com/problems/bu-yong-jia-jian-cheng-chu-zuo-jia-fa-lcof/solution/mian-shi-ti-65-bu-yong-jia-jian-cheng-chu-zuo-ji-7/809000</a>  sum、carry</p><p><a href="https://leetcode-cn.com/problems/bu-yong-jia-jian-cheng-chu-zuo-jia-fa-lcof/solution/mian-shi-ti-65-bu-yong-jia-jian-cheng-chu-zuo-ji-7/539350//">https://leetcode-cn.com/problems/bu-yong-jia-jian-cheng-chu-zuo-jia-fa-lcof/solution/mian-shi-ti-65-bu-yong-jia-jian-cheng-chu-zuo-ji-7/539350//</a>   C++中负数不支持左移位，因为结果是不定的</p><h1 id="20211218剑指-Offer-62-圆圈中最后剩下的数字-力扣（LeetCode）-leetcode-cn-com"><a href="#20211218剑指-Offer-62-圆圈中最后剩下的数字-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211218剑指 Offer 62. 圆圈中最后剩下的数字 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211218<a href="https://leetcode-cn.com/problems/yuan-quan-zhong-zui-hou-sheng-xia-de-shu-zi-lcof/">剑指 Offer 62. 圆圈中最后剩下的数字 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>0，1，2，3，4。</p><p>删除2，从3开始。</p><p><a href="https://leetcode-cn.com/problems/yuan-quan-zhong-zui-hou-sheng-xia-de-shu-zi-lcof/solution/jian-zhi-offer-62-yuan-quan-zhong-zui-ho-dcow/1154322">https://leetcode-cn.com/problems/yuan-quan-zhong-zui-hou-sheng-xia-de-shu-zi-lcof/solution/jian-zhi-offer-62-yuan-quan-zhong-zui-ho-dcow/1154322</a></p><p><a href="https://leetcode-cn.com/problems/yuan-quan-zhong-zui-hou-sheng-xia-de-shu-zi-lcof/solution/jian-zhi-offer-62-yuan-quan-zhong-zui-ho-dcow/1152926">https://leetcode-cn.com/problems/yuan-quan-zhong-zui-hou-sheng-xia-de-shu-zi-lcof/solution/jian-zhi-offer-62-yuan-quan-zhong-zui-ho-dcow/1152926</a></p><h2 id="讲的最好的"><a href="#讲的最好的" class="headerlink" title="讲的最好的"></a>讲的最好的</h2><p><a href="https://leetcode-cn.com/problems/yuan-quan-zhong-zui-hou-sheng-xia-de-shu-zi-lcof/solution/huan-ge-jiao-du-ju-li-jie-jue-yue-se-fu-huan-by-as/">换个角度举例解决约瑟夫环 - 圆圈中最后剩下的数字 - 力扣（LeetCode） (leetcode-cn.com)</a></p><p><a href="https://leetcode-cn.com/problems/yuan-quan-zhong-zui-hou-sheng-xia-de-shu-zi-lcof/solution/huan-ge-jiao-du-ju-li-jie-jue-yue-se-fu-huan-by-as/771860">https://leetcode-cn.com/problems/yuan-quan-zhong-zui-hou-sheng-xia-de-shu-zi-lcof/solution/huan-ge-jiao-du-ju-li-jie-jue-yue-se-fu-huan-by-as/771860</a></p><h1 id="20211220剑指-Offer-56-I-数组中数字出现的次数-力扣（LeetCode）-leetcode-cn-com"><a href="#20211220剑指-Offer-56-I-数组中数字出现的次数-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211220剑指 Offer 56 - I. 数组中数字出现的次数 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211220<a href="https://leetcode-cn.com/problems/shu-zu-zhong-shu-zi-chu-xian-de-ci-shu-lcof/">剑指 Offer 56 - I. 数组中数字出现的次数 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><a href="https://leetcode-cn.com/problems/shu-zu-zhong-shu-zi-chu-xian-de-ci-shu-lcof/solution/jian-zhi-offer-56-i-shu-zu-zhong-shu-zi-tykom/889103">https://leetcode-cn.com/problems/shu-zu-zhong-shu-zi-chu-xian-de-ci-shu-lcof/solution/jian-zhi-offer-56-i-shu-zu-zhong-shu-zi-tykom/889103</a></p><h1 id="20211221剑指-Offer-56-II-数组中数字出现的次数-II-力扣（LeetCode）-leetcode-cn-com"><a href="#20211221剑指-Offer-56-II-数组中数字出现的次数-II-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211221剑指 Offer 56 - II. 数组中数字出现的次数 II - 力扣（LeetCode） (leetcode-cn.com)"></a>20211221<a href="https://leetcode-cn.com/problems/shu-zu-zhong-shu-zi-chu-xian-de-ci-shu-ii-lcof/">剑指 Offer 56 - II. 数组中数字出现的次数 II - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><a href="https://leetcode-cn.com/problems/shu-zu-zhong-shu-zi-chu-xian-de-ci-shu-ii-lcof/comments/501039">https://leetcode-cn.com/problems/shu-zu-zhong-shu-zi-chu-xian-de-ci-shu-ii-lcof/comments/501039</a></p><h1 id="20211121剑指-Offer-66-构建乘积数组-力扣（LeetCode）-leetcode-cn-com"><a href="#20211121剑指-Offer-66-构建乘积数组-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211121剑指 Offer 66. 构建乘积数组 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211121<a href="https://leetcode-cn.com/problems/gou-jian-cheng-ji-shu-zu-lcof/">剑指 Offer 66. 构建乘积数组 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><a href="https://leetcode-cn.com/problems/gou-jian-cheng-ji-shu-zu-lcof/solution/mian-shi-ti-66-gou-jian-cheng-ji-shu-zu-biao-ge-fe/">剑指 Offer 66. 构建乘积数组（表格分区，清晰图解） - 构建乘积数组 - 力扣（LeetCode） (leetcode-cn.com)</a></p><p>这两个思路写的比较好</p><p><a href="https://leetcode-cn.com/problems/gou-jian-cheng-ji-shu-zu-lcof/solution/mian-shi-ti-66-gou-jian-cheng-ji-shu-zu-biao-ge-fe/417592">https://leetcode-cn.com/problems/gou-jian-cheng-ji-shu-zu-lcof/solution/mian-shi-ti-66-gou-jian-cheng-ji-shu-zu-biao-ge-fe/417592</a></p><p><a href="https://leetcode-cn.com/problems/gou-jian-cheng-ji-shu-zu-lcof/solution/mian-shi-ti-66-gou-jian-cheng-ji-shu-zu-biao-ge-fe/447760">https://leetcode-cn.com/problems/gou-jian-cheng-ji-shu-zu-lcof/solution/mian-shi-ti-66-gou-jian-cheng-ji-shu-zu-biao-ge-fe/447760</a></p><h1 id="20211221-剑指-Offer-49-丑数-力扣（LeetCode）-leetcode-cn-com"><a href="#20211221-剑指-Offer-49-丑数-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211221 剑指 Offer 49. 丑数 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211221 <a href="https://leetcode-cn.com/problems/chou-shu-lcof/">剑指 Offer 49. 丑数 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>数组合并</p><p><a href="https://leetcode-cn.com/problems/chou-shu-lcof/comments/250364">https://leetcode-cn.com/problems/chou-shu-lcof/comments/250364</a></p><p>i,j,k=0;</p><p>idx=1，tmp=min(1x2,min(1x3,1x5))=2;i++;u=1,2</p><p>idx=2，tmp=min(2x2,min(1x3,1x5))=3;j++;u=1,2,3</p><p>idx=3，tmp=min(2x2,min(2x3,1x5))=4;i++;u=1,2,3,4</p><p>idx=4，tmp=min(3x2,min(2x3,1x5))=5;k++;u=1,2,3,4,5</p><p>idx=5，tmp=min(3x2,min(2x3,2x5))=5;i++,k++;u=1,2,3,4,5,6</p><h1 id="20211221-剑指-Offer-60-n个骰子的点数-力扣（LeetCode）-leetcode-cn-com"><a href="#20211221-剑指-Offer-60-n个骰子的点数-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211221 剑指 Offer 60. n个骰子的点数 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211221 <a href="https://leetcode-cn.com/problems/nge-tou-zi-de-dian-shu-lcof/">剑指 Offer 60. n个骰子的点数 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><a href="https://leetcode-cn.com/problems/nge-tou-zi-de-dian-shu-lcof/solution/jian-zhi-offer-60-n-ge-tou-zi-de-dian-sh-z36d/882642">https://leetcode-cn.com/problems/nge-tou-zi-de-dian-shu-lcof/solution/jian-zhi-offer-60-n-ge-tou-zi-de-dian-sh-z36d/882642</a></p><h1 id="20211218-面试题17-打印从-1-到最大的-n-位数（分治算法-全排列，清晰图解）-打印从1到最大的n位数-力扣（LeetCode）-leetcode-cn-com"><a href="#20211218-面试题17-打印从-1-到最大的-n-位数（分治算法-全排列，清晰图解）-打印从1到最大的n位数-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211218 面试题17. 打印从 1 到最大的 n 位数（分治算法 / 全排列，清晰图解） - 打印从1到最大的n位数 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211218 <a href="https://leetcode-cn.com/problems/da-yin-cong-1dao-zui-da-de-nwei-shu-lcof/solution/mian-shi-ti-17-da-yin-cong-1-dao-zui-da-de-n-wei-2/">面试题17. 打印从 1 到最大的 n 位数（分治算法 / 全排列，清晰图解） - 打印从1到最大的n位数 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><a href="https://leetcode-cn.com/circle/article/Sy1x7o/">大数加减乘除运算总结 - 力扣（LeetCode） (leetcode-cn.com)</a></p><h1 id="20211223Dijkstra-相关题目-概率最大的路径-力扣（LeetCode）-leetcode-cn-com"><a href="#20211223Dijkstra-相关题目-概率最大的路径-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211223Dijkstra 相关题目 - 概率最大的路径 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211223<a href="https://leetcode-cn.com/problems/path-with-maximum-probability/solution/dijkstra-xiang-guan-ti-mu-by-snowmelthe-cqz6/">Dijkstra 相关题目 - 概率最大的路径 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><a href="https://leetcode-cn.com/problems/path-with-maximum-probability/solution/dijkstra-suan-fa-xiang-jie-by-labuladong-8zhv/">Dijkstra 算法详解 - 概率最大的路径 - 力扣（LeetCode） (leetcode-cn.com)</a></p><h1 id="20211223207-课程表-力扣（LeetCode）-leetcode-cn-com"><a href="#20211223207-课程表-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211223207. 课程表 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211223<a href="https://leetcode-cn.com/problems/course-schedule/">207. 课程表 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><a href="https://leetcode-cn.com/problems/course-schedule/solution/course-schedule-tuo-bu-pai-xu-bfsdfsliang-chong-fa/286184">https://leetcode-cn.com/problems/course-schedule/solution/course-schedule-tuo-bu-pai-xu-bfsdfsliang-chong-fa/286184</a></p><p>[[1,4],[2,4],[3,1],[3,2]],n=5。</p><p>adj：</p><p>0：</p><p>1：3</p><p>2：3</p><p>3：</p><p>4：1，2</p><p>indegree：</p><p>0：0</p><p>1：1</p><p>2：1</p><p>3：2</p><p>4：0</p><p>num=3,Q={0,4}.</p><p>f=4,</p><p>0：0</p><p>1：0</p><p>2：0</p><p>3：2</p><p>4：0</p><p>num=1,Q={1,2}.</p><h1 id="20211227吃🐳！🤷‍♀️竟然一眼秒懂合并区间！-合并区间-力扣（LeetCode）-leetcode-cn-com"><a href="#20211227吃🐳！🤷‍♀️竟然一眼秒懂合并区间！-合并区间-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20211227吃🐳！🤷‍♀️竟然一眼秒懂合并区间！ - 合并区间 - 力扣（LeetCode） (leetcode-cn.com)"></a>20211227<a href="https://leetcode-cn.com/problems/merge-intervals/solution/chi-jing-ran-yi-yan-miao-dong-by-sweetiee/">吃🐳！🤷‍♀️竟然一眼秒懂合并区间！ - 合并区间 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><h1 id="2022010333-搜索旋转排序数组-力扣（LeetCode）-leetcode-cn-com"><a href="#2022010333-搜索旋转排序数组-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="2022010333. 搜索旋转排序数组 - 力扣（LeetCode） (leetcode-cn.com)"></a>20220103<a href="https://leetcode-cn.com/problems/search-in-rotated-sorted-array/">33. 搜索旋转排序数组 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p><a href="https://leetcode-cn.com/problems/search-in-rotated-sorted-array/solution/yi-wen-jie-jue-4-dao-sou-suo-xuan-zhuan-pai-xu-s-2/">一文解决 4 道「搜索旋转排序数组」题！ - 搜索旋转排序数组 - 力扣（LeetCode） (leetcode-cn.com)</a></p><h1 id="20220106128-最长连续序列-力扣（LeetCode）-leetcode-cn-com"><a href="#20220106128-最长连续序列-力扣（LeetCode）-leetcode-cn-com" class="headerlink" title="20220106128. 最长连续序列 - 力扣（LeetCode） (leetcode-cn.com)"></a>20220106<a href="https://leetcode-cn.com/problems/longest-consecutive-sequence/">128. 最长连续序列 - 力扣（LeetCode） (leetcode-cn.com)</a></h1><p>只关注第一个打头的字符就可以了。</p><p><a href="https://leetcode-cn.com/problems/longest-consecutive-sequence/solution/xiao-bai-lang-ha-xi-ji-he-ha-xi-biao-don-j5a2/">【小白郎】哈希集合/哈希表/动态规划/并查集四种方法，绝对够兄弟们喝一壶的！ - 最长连续序列 - 力扣（LeetCode） (leetcode-cn.com)</a></p><h1 id="20220113"><a href="#20220113" class="headerlink" title="20220113"></a>20220113</h1><p><a href="https://leetcode-cn.com/problems/search-a-2d-matrix-ii/">240. 搜索二维矩阵 II - 力扣（LeetCode） (leetcode-cn.com)</a></p><p>二分，旋转数组那道题目。</p><h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p><a href="https://mp.weixin.qq.com/mp/homepage?__biz=MzI4Njc4MzMwMw==&amp;hid=1&amp;sn=58bf8e995138b26984c05fd51f198196经典重点题目归纳">https://mp.weixin.qq.com/mp/homepage?__biz=MzI4Njc4MzMwMw==&amp;hid=1&amp;sn=58bf8e995138b26984c05fd51f198196经典重点题目归纳</a></p><p><a href="https://leetcode-cn.com/problem-list/2cktkvj/">力扣 (leetcode-cn.com)</a></p><p>LeetCode 热题 HOT 100a</p><p><a href="https://leetcode-cn.com/problem-list/xb9nqhhg/">力扣 (leetcode-cn.com)</a></p><p>剑指 Offer（第 2 版）</p><p><a href="https://leetcode-cn.com/study-plan/lcof/?progress=pmhin4r">「剑指 Offer」 - 学习计划 - 力扣（LeetCode）全球极客挚爱的技术成长平台 (leetcode-cn.com)</a></p><p>剑指 Offer（第 2 版）分类</p><p><a href="https://leetcode-cn.com/problem-list/2ckc81c/">力扣 (leetcode-cn.com)</a></p><p>LeetCode 精选 TOP 面试题</p><p><img src="/2021/10/04/leetcode%E9%A2%98%E7%9B%AE%E8%AE%B0%E5%BD%95/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211028152640610.png" alt="image-20211028152640610"></p><p><a href="https://leetcode-cn.com/study-plan/lcof/?progress=m1kbic6">「剑指 Offer」 - 学习计划 - 力扣（LeetCode）全球极客挚爱的技术成长平台 (leetcode-cn.com)</a></p><h1 id="学习计划广场"><a href="#学习计划广场" class="headerlink" title="学习计划广场"></a>学习计划广场</h1><p><a href="https://leetcode-cn.com/study-plan/">学习计划 - 力扣（LeetCode）全球极客挚爱的技术成长平台 (leetcode-cn.com)</a></p><p>可以做单题</p><h1 id="图论"><a href="#图论" class="headerlink" title="图论"></a>图论</h1><p><a href="https://leetcode-cn.com/circle/article/Z6QGK0/">递推-综合应用，图 - 力扣（LeetCode） (leetcode-cn.com)</a></p><p><a href="https://leetcode-cn.com/problems/path-with-maximum-probability/solution/dijkstra-suan-fa-xiang-jie-by-labuladong-8zhv/">Dijkstra 算法详解 - 概率最大的路径 - 力扣（LeetCode） (leetcode-cn.com)</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>知识蒸馏文章阅读</title>
      <link href="/2021/09/29/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/"/>
      <url>/2021/09/29/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="Learning-with-Privileged-Information-for-Efficient-Image-Super-Resolution"><a href="#Learning-with-Privileged-Information-for-Efficient-Image-Super-Resolution" class="headerlink" title="Learning with Privileged Information for Efficient Image Super-Resolution"></a>Learning with Privileged Information for Efficient Image Super-Resolution</h1><p>评价：利用特权信息 Privileged Information学习高效图像超分辨率。</p><p>针对问题：单图超分FSRCNN网络的轻量化。</p><p><img src="/2021/09/29/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/super_resolution_psnr.png" alt></p><p>本文的目的：对模型的参数量和运行时间进行优化。</p><p>实现的方法：</p><p><img src="/2021/09/29/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/super_resolution_network.png" alt="image-20210930111044063"><img src="/2021/09/29/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20210930124323667.png" alt="image-20210930124323667"></p><p>使用自编码器提取关键信息</p><p><img src="/2021/09/29/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/L^T_im.png" alt="image-20210930124801961"></p><p><img src="/2021/09/29/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/L^T_recon.png" alt="image-20210930124857285"></p><p><img src="/2021/09/29/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/L^T_total.png" alt="image-20210930124916318"></p><blockquote><p>对于自编码获得的压缩信息做了约束。</p></blockquote><p><img src="/2021/09/29/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/L^S_recon" alt="image-20210930124948265"></p><p><img src="/2021/09/29/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/L^S_distill.png" alt="image-20210930130733399"></p><p><img src="/2021/09/29/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/feature_inter_info.png" alt="image-20210930130756672"></p><p><img src="/2021/09/29/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/L^S_total" alt="image-20210930130825225"></p><p>个人的思考:知识蒸馏一般用在分类问题上比较多，这篇文章使用自编码器，可以把知识蒸馏用在回归问题上。</p><h1 id="Anomaly-Detection-in-Video-via-Self-Supervised-and-Multi-Task-Learning"><a href="#Anomaly-Detection-in-Video-via-Self-Supervised-and-Multi-Task-Learning" class="headerlink" title="Anomaly Detection in Video via Self-Supervised and Multi-Task Learning"></a>Anomaly Detection in Video via Self-Supervised and Multi-Task Learning</h1><p><img src="/2021/09/29/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/yolo_distillation.png" alt="image-20210930150701656"></p><p>知识蒸馏也是模型的压缩，希望小网络能够最大程度上接近大网络的输出，YOLOV3和ResNet50都是teacher network，特征提取网络(3D 卷积) 是student network：</p><p>1：ResNet50是非常好的特征提取器，但是其网络结构复杂，运行时间长，在进行异常检测时不能满足实时性要求，所以通过知识蒸馏使得特征提取网络性能接近ResNet50，所以在训练时数据仍需经过ResNet50，在异常检测时抛弃ResNet50。</p><p>2：YOLOV3对物体进行检测，通过知识蒸馏，3D卷积网络学习目标检测。</p><p>通过知识蒸馏，3D卷积学习到了YOLOV3的目标检测，同时满足了特征提取的要求。</p><p>异常类别：知识蒸馏后，3D卷积遇到正常物体(人行道行人)与YOLOV3检测结果差异小，遇到异常物体(人行道车辆)与YOLOV3检测结果差异较大，能够完成异常物体类别的异常检测，同时特征提取的能力帮助其他三个代理任务完成异常检测。</p>]]></content>
      
      
      <categories>
          
          <category> 知识蒸馏 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>低分辨率及三维人脸关键点定位调研</title>
      <link href="/2021/09/28/3D-RGBD-landmark-%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94/"/>
      <url>/2021/09/28/3D-RGBD-landmark-%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94/</url>
      
        <content type="html"><![CDATA[<h1 id="LR-face-landmark"><a href="#LR-face-landmark" class="headerlink" title="LR face landmark"></a>LR face landmark</h1><h2 id="采用鲁棒显著性人脸地标检测实现高效灵活的人脸图像去模糊Towards-more-efficient-and-flexible-face-image-deblurring-using-robust-salient-face-landmark-detection"><a href="#采用鲁棒显著性人脸地标检测实现高效灵活的人脸图像去模糊Towards-more-efficient-and-flexible-face-image-deblurring-using-robust-salient-face-landmark-detection" class="headerlink" title="采用鲁棒显著性人脸地标检测实现高效灵活的人脸图像去模糊Towards more efficient and flexible face image deblurring using robust salient face landmark detection"></a>采用鲁棒显著性人脸地标检测实现高效灵活的人脸图像去模糊Towards more efficient and flexible face image deblurring using robust salient face landmark detection</h2><blockquote><p>Huang, Y., Yao, H., Zhao, S. <em>et al.</em> Towards more efficient and flexible face image deblurring using robust salient face landmark detection. <em>Multimed Tools Appl</em> <strong>76,</strong> 123–142 (2017). <a href="https://doi.org/10.1007/s11042-015-3009-3">https://doi.org/10.1007/s11042-015-3009-3</a></p><p>Yinghao Huang1·Hongxun Yao1·Sicheng Zhao1·Yanhao Zhang1</p><p>School of Computer Science and Technology, Harbin Institute of Technology</p></blockquote><p>评价：将经典的<strong>$L_0$去模糊方法与人脸地标检测</strong>相结合实现高效灵活的人脸图像去模糊。</p><p>针对问题：人脸图像的去模糊。</p><p>本文的目的：更好地处理各种复杂的人脸姿态、形状和表情，同时大大减少了计算时间。(基于样本数据集的人脸图像去模糊方法的问题会出现即使是最好的候选图像也不能很好地匹配输入图像)</p><p>实现的方法：地标探测器被用来检测主要的面部轮廓，然后将检测到的<strong>轮廓作为显著边缘引导盲图像去卷积</strong>。</p><p>方法的框架如图2所示，主要由三个部分组成:</p><p>(1)鲁棒人脸地标检测器训练;</p><p>(2)显著轮廓检测和。</p><p>(3)盲图像去模糊。</p><p><img src="/2021/09/28/3D-RGBD-landmark-%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94/landmark_blur.png" alt="image-20210910100026769"></p><p>方法的框架。这里，数字(1-3)和字母(a-e)分别表示操作和实体。如图所示，整个算法可以分为三个步骤:(1)鲁棒检测器训练，(2)显著轮廓检测，(3)盲图像去模糊。在检测器训练阶段(1)，根据大的训练数据集(a)构造回归森林集合，得到回归森林集合，这是几个广泛使用的<strong>人脸地标检测数据集的模糊版本</strong>。在测试阶段,给定一个模糊的图像(b),首先应用训练凸轮廓探测器得到凸边,如(2)所示。然后估计凸轮廓的帮助下,实现大幅面图像通过盲图像去模糊(3)。</p><p>利用检测到的显著面部轮廓尽可能准确地估计模糊核，之后恢复潜在图像，采用共轭梯度法求解。</p><h1 id="The-3D-Menpo-Facial-Landmark-Tracking-Challenge"><a href="#The-3D-Menpo-Facial-Landmark-Tracking-Challenge" class="headerlink" title="The 3D Menpo Facial Landmark Tracking Challenge"></a>The 3D Menpo Facial Landmark Tracking Challenge</h1><blockquote><p><a href="https://ieeexplore.ieee.org/xpl/conhome/8234943/proceeding">2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</a></p><p>Stefanos Zafeiriou∗,1,2Grigorios G. Chrysos∗,1Anastasios Roussos∗,1,3 Evangelos V erveras1Jiankang Deng1 George Trigeorgis1<br>1Department of Computing, Imperial College London, UK<br>2Center for Machine Vision and Signal Analysis, University of Oulu, Finland<br>3Department of Computer Science, University of Exeter, UK</p><p>引用量235</p></blockquote><p>评价：</p><p>提供了一个包含3DA -2D和3D面部地标的大规模面部图像数据库。</p><p>提出了一个复杂的程序来估计任意“野外”视频中的3DA - 2D和3D地标。该程序是高度准确的，并用于提供超过28万注释帧。</p><p>我们给出了3DA - 2D和3D地标跟踪的第一个挑战的结果。</p><p>针对问题：所提供的2D标注很少捕捉到面部的3D结构(这在面部边界上尤其明显)。也就是说，注释既不提供深度估计，也不对应于三维面部结构的2D投影。</p><p>2D数据集许多<strong>地标几乎不对应于人脸的3D结构</strong>。也就是说，它们<strong>不能准确地对应于三维面部结构的任何地标在图像平面上的投影</strong>。此外，上述基准的<strong>2D注释不包含关于3D面深度的任何信息</strong>。在本文中，我们将图像<strong>平面上的三维地标的二维投影称为3DA -2D地标</strong>，以区别于三维场景中面部地标的三维坐标。</p><p><img src="/2021/09/28/3D-RGBD-landmark-%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94/MenpoFacialLandmark.png" alt="image-20210910145246469"></p><p>侧脸情况下隐藏的半边脸的标注是另一边脸的轮廓。</p><p><a href="https://ibug.doc.ic.ac.uk/resources/1st-3d-face-tracking-wild-competition/">https://ibug.doc.ic.ac.uk/resources/1st-3d-face-tracking-wild-competition/</a></p><blockquote><p>用此数据集训练脸？问题在于它的标注是估计出来的，无法保证精确性。</p></blockquote><hr><h1 id="RGB-D-based-gaze-point-estimation-via-multi-column-CNNs-and-facial-landmarks-global-optimization"><a href="#RGB-D-based-gaze-point-estimation-via-multi-column-CNNs-and-facial-landmarks-global-optimization" class="headerlink" title="RGB-D-based gaze point estimation via multi-column CNNs and facial landmarks global optimization"></a>RGB-D-based gaze point estimation via multi-column CNNs and facial landmarks global optimization</h1><blockquote><p>Zhang, Z., Lian, D. &amp; Gao, S. RGB-D-based gaze point estimation via multi-column CNNs and facial landmarks global optimization. <em>Vis Comput</em> <strong>37,</strong> 1731–1741 (2021). <a href="https://doi.org/10.1007/s00371-020-01934-1">https://doi.org/10.1007/s00371-020-01934-1</a></p></blockquote><p>评价：基于rgb - d的多列cnn和人脸地标全局优化的注视点估计。利用一个多列multi-column cnn框架，从一个人的RGB-D图像来估计坐在显示器前的人的注视点。</p><p><img src="/2021/09/28/3D-RGBD-landmark-%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94/gazeShanghaitech.png" alt="image-20210910171732013"></p><p>针对问题：捕获的<strong>深度图像通常包含噪声和黑洞</strong>，阻碍我们获得可靠的头部姿态和3D眼睛位置估计。</p><p>实现的方法：视点由头部姿态、眼球姿态和眼睛三维位置决定。推断这三个分量，然后将它们整合来估计注视点。</p><p>从RGB人脸图像中估计出<strong>68个人脸关键点的相对深度</strong>，然后利用这些相对深度和捕获的原始深度（[40]），通过全局优化求解所有<strong>人脸关键点的绝对深度</strong>。精确的深度提供可靠的头部姿态和3D眼睛位置的估计。</p><p>眼球姿势，通过多列CNN得到；</p><p>头部姿势和眼睛位置通过两步程序从RGB和深度图像中估计68个3D面部地标(见图4)来解决的。</p><p><img src="/2021/09/28/3D-RGBD-landmark-%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94/2019aaaiStructure.png" alt="image-20210916170140209"></p><p>我们的注视点估计框架。使用三个独立的特征提取器(即Eyeball- c1、Eyeball- c2和Eyeball- c3，在我们的实现中这三个独立的ResNet-18)从<strong>两张单眼图像和人脸图像中提取眼球姿态特征</strong>。人脸标志物的相对深度是由相对深度估计器(另一种ResNet-18)从人脸图像中估计出来的。然后，利用<strong>估计的相对深度和原始深度</strong>，通过<strong>全局优化获得可靠的三维面部地标</strong>。三个CNN列提取的眼球构成特性和最优的三维面部地标被送入三套完全连接层(即FC-C1-1、FC-C2-1 FC-C3-1图,在我们的实现中，每个层都由两个级联的完全连接的层组成))为注视点回归。最后，由最后一个完全连接层收集来自CNN三个栏目的三个独立的凝视预测，进行最终预测。</p><p>采用了与[40]相似的策略，其中使用RGB图像来预测面部地标对的相对深度，并通过使用估计的相对深度和原始深度进行全局优化获得绝对深度。</p><blockquote><p>看一下关键点是怎么得出的？使用了dlib toolkit。</p></blockquote><h2 id="数据收集方法"><a href="#数据收集方法" class="headerlink" title="数据收集方法"></a>数据收集方法</h2><p>每个参与者都被要求坐下在<strong>27英寸的iMac前，电脑安装了英特尔RealSense SR300摄像头，距离从50厘米到100厘米不等</strong>，当参与者准备好了，他或她可以在我们的数据收集软件中输入他或她的名字，并开始一个收集会话。在每个实验中，随机出现50个半径为30像素的白点(在1920×1080像素分辨率下约为0.93 cm)，参与者被要求依次点击白点的中心。同时，数据采集软件记录每个白点的位置，每次点击的位置，以及参与者的RGB-D图像。在一个会话结束后，参与者可以在开始下一个会话之前短暂休息，直到全部16个会话结束。</p><p>在数据收集过程中，我们假设参与者应该盯着中间的白点。然而，这并不总是正确的，特别是当参与者在点击点时分心。为了避免这种情况，我们还用蓝点向参与者展示了每次点击的位置。此外，我们通过测量白点中心和点击位置之间的距离来确定样品。如果距离比给定的阈值(10个像素0.31cm 1920×1080像素))更远，我们就简单地从数据集中排除该样本。训练集包含159个参与者的119,318个样本，验证集包含其余59个参与者的45,913个样本。</p><p><img src="/2021/09/28/3D-RGBD-landmark-%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94/gaze_sample.png" alt></p><p><strong>没有固定参与者和显示器之间的距离，也没有固定左右的深度，每个参与者的眼睛变化幅度很大</strong>，<strong>超过一半的参与者在实验中佩戴了眼镜，这使得他们的深度图像中眼睛区域包含了大的黑洞</strong>。</p><p><img src="/2021/09/28/3D-RGBD-landmark-%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94/2019aaai_dataset_compare.png" alt></p><p><img src="/2021/09/28/3D-RGBD-landmark-%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94/2Dgaze.png" alt="image-20210916202616025"></p><h1 id="Disentangling-3D-Pose-in-A-Dendritic-CNN-for-Unconstrained-2D-Face-Alignment"><a href="#Disentangling-3D-Pose-in-A-Dendritic-CNN-for-Unconstrained-2D-Face-Alignment" class="headerlink" title="Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face Alignment"></a>Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face Alignment</h1><blockquote><p>CVPR2018 </p><p>Amit Kumar Rama Chellappa<br>Department of Electrical and Computer Engineering, CFAR and UMIACS<br>University of Maryland-College Park,USA</p></blockquote><p>评价：在树突状CNN中解纠缠3D姿态用于无约束2D人脸对齐，提出了姿态条件树突卷积神经网络Pose Conditioned Dendritic Convolution Neural Network (PCD-CNN);它使用单个CNN来模拟面部地标的树突结构。</p><p>针对问题：利用贝叶斯框架对头部姿态进行解缠，对不同的姿态的地标估计来分解人脸图像的3D姿态，使其与人脸姿态无关，从而减少了定位误差。</p><p><img src="/2021/09/28/3D-RGBD-landmark-%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20210917153505618.png" alt="image-20210917153505618"></p><p><img src="/2021/09/28/3D-RGBD-landmark-%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20210917162310562.png" alt="image-20210917162310562"></p><p>提出的方法的细节。卷积层顶部的虚线表示残差连接。树突关键点网是基于PoseNet的。灰色盒子内的网络代表了建议的PCD-CNN，而蓝色盒子内的第二个网络是模块化的，可以用于辅助任务。与这些辅助网络一起使用的是用于更精细定位的卷积-反卷积网络。</p><p>提出了人脸标志点的树突结构，实现了标志点之间的有效信息共享。树状结构的节点是反卷积的输出，而节点与节点之间的边缘采用卷积函数(fij)建模。反卷积网络结构如图3所示。</p><h1 id="How-far-are-we-from-solving-the-2D-amp-3D-Face-Alignment-problem-and-a-dataset-of-230-000-3D-facial-landmarks"><a href="#How-far-are-we-from-solving-the-2D-amp-3D-Face-Alignment-problem-and-a-dataset-of-230-000-3D-facial-landmarks" class="headerlink" title="How far are we from solving the 2D &amp; 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)"></a>How far are we from solving the 2D &amp; 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)</h1><blockquote><p>2017ICCV</p><p>Adrian Bulat and Georgios Tzimiropoulos<br>Computer Vision Laboratory, The University of Nottingham<br>Nottingham, United Kingdom</p></blockquote><hr><p>LS3D-W</p>]]></content>
      
      
      <categories>
          
          <category> 人脸关键点检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>testpicsconfig</title>
      <link href="/2021/09/28/testpicsconfig/"/>
      <url>/2021/09/28/testpicsconfig/</url>
      
        <content type="html"><![CDATA[<p><img src="/2021/09/28/testpicsconfig/1002.jpg" alt></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>试验记录Aggregation via Separation: Boosting Facial Landmark Detector with</title>
      <link href="/2021/09/08/%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95Aggregation-via-Separation-Boosting-Facial-Landmark-Detector-with/"/>
      <url>/2021/09/08/%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95Aggregation-via-Separation-Boosting-Facial-Landmark-Detector-with/</url>
      
        <content type="html"><![CDATA[<h1 id="配准环境遇到的问题"><a href="#配准环境遇到的问题" class="headerlink" title="配准环境遇到的问题"></a>配准环境遇到的问题</h1><h2 id="关于tensorboardX的安装PackagesNotFoundError"><a href="#关于tensorboardX的安装PackagesNotFoundError" class="headerlink" title="关于tensorboardX的安装PackagesNotFoundError"></a>关于tensorboardX的安装PackagesNotFoundError</h2><p>用conda安装tensorboardX，由于配置的源找不到包，于是用pip安装，使用pip安装遇到了问题，如链接中所示</p><p><a href="https://github.com/lanpa/tensorboardX/issues/19">ImportError: cannot import name ‘SummaryWriter’ · Issue #19 · lanpa/tensorboardX (github.com)</a></p><blockquote><p>File “”, line 1, in<br>File “/home/rahul/anaconda3/envs/semseg/lib/python2.7/site-packages/tensorboardX/<strong>init</strong>.py”, line 5, in<br>from .torchvis import TorchVis<br>File “/home/rahul/anaconda3/envs/semseg/lib/python2.7/site-packages/tensorboardX/torchvis.py”, line 11, in<br>from .writer import SummaryWriter<br>File “/home/rahul/anaconda3/envs/semseg/lib/python2.7/site-packages/tensorboardX/writer.py”, line 223<br>logdir: Optional[str] = None,<br>^<br>SyntaxError: invalid syntax</p></blockquote><p>解决的方案是用conda来装，不用pip装，于是先要处理这个问题</p><blockquote><p>PackagesNotFoundError: The following packages are not available from current channels</p></blockquote><p>解决方案可以参照<a href="https://blog.csdn.net/marleylee/article/details/103722080">(8条消息) PackagesNotFoundError: The following packages are not available from current channels 解决办法_marleylee的博客-CSDN博客</a>。具体参考方法二<strong>去base环境下执行搜索</strong>，在去<strong>实验配置的环境安装</strong>，否则会出现</p><blockquote><p>anaconda: command not found</p></blockquote><p><a href="https://blog.csdn.net/weixin_39585934/article/details/90384355#:~:text=确定已成功安装 anaconda 启动 anaconda -navigator  如果提示没有此命令 or,root  再输入  an... 如何 使用anaconda 安装或更新自己想要的库.">(8条消息) anaconda使用说明：anaconda-navigator: command not found_豌豆生的笔记本-CSDN博客</a></p><h2 id="ModuleNotFoundError-No-module-named-‘torch-utils-serialization’-的解决方案"><a href="#ModuleNotFoundError-No-module-named-‘torch-utils-serialization’-的解决方案" class="headerlink" title="ModuleNotFoundError: No module named ‘torch.utils.serialization’ 的解决方案"></a>ModuleNotFoundError: No module named ‘torch.utils.serialization’ 的解决方案</h2><p>在torch新版本中删除了load_lua，解决方法是<a href="https://blog.csdn.net/qq_36556893/article/details/102683398">(8条消息) ModuleNotFoundError: No module named ‘torch.utils.serialization’ 的解决方案_悲恋花丶无心之人的博客-CSDN博客</a></p><h2 id="使用Tmux避免断网之后服务器端进程断掉"><a href="#使用Tmux避免断网之后服务器端进程断掉" class="headerlink" title="使用Tmux避免断网之后服务器端进程断掉"></a>使用Tmux避免断网之后服务器端进程断掉</h2><p><a href="http://www.ruanyifeng.com/blog/2019/10/tmux.html">Tmux 使用教程 - 阮一峰的网络日志 (ruanyifeng.com)</a></p><blockquote><p>tmux detach 将当前会话与窗口分离。命令执行后，就会退出当前 Tmux 窗口，但是会话和里面的进程仍然在后台运行。</p></blockquote><p>vscode可以在termnal窗口右上角的“+”点击“∨”下拉选择tmux新建会话，使用“tmux ls”命令可以看见tmux打开的窗口会显示(attached)， <strong>会话默认是分离的</strong>，在termnal窗口右端删除tmux窗口后，此会话还在。</p><h1 id="数据集介绍WFLW"><a href="#数据集介绍WFLW" class="headerlink" title="数据集介绍WFLW"></a>数据集介绍WFLW</h1><p>原数据集可以从<a href="https://wywu.github.io/projects/LAB/WFLW.html">Look at Boundary: A Boundary-Aware Face Alignment Algorithm (wywu.github.io)</a>下载，不过在一些实验中会</p><p>7500训练 ，2500测试。人脸多种属性、关键点标注数据集，包含了10000张脸，其中7500用于训练，2500张用于测试，共98个关键点。除了关键点之外，还有姿态，表情，光照，妆容，遮挡，模糊。</p><p><img src="/2021/09/08/%E8%AF%95%E9%AA%8C%E8%AE%B0%E5%BD%95Aggregation-via-Separation-Boosting-Facial-Landmark-Detector-with/WFLW_landmarks.png" alt></p><h2 id="标注格式介绍"><a href="#标注格式介绍" class="headerlink" title="标注格式介绍"></a>标注格式介绍</h2><blockquote><p>下载原始数据集后，位于WFLW_annotations文件夹下，包含list_98pt_rect_attr_train_test和list_98pt_test两个子文件夹</p></blockquote><h3 id="list-98pt-rect-attr-train-test"><a href="#list-98pt-rect-attr-train-test" class="headerlink" title="list_98pt_rect_attr_train_test"></a>list_98pt_rect_attr_train_test</h3><p>文档中每一行有207个数据，98X2关键点+4左上右下+6属性+图片名称 = 196 +4+6+1 = 207。</p><h3 id="list-98pt-test"><a href="#list-98pt-test" class="headerlink" title="list_98pt_test"></a>list_98pt_test</h3><p>文档中每一行有197个数据，196 + 图片名称 =197。</p><h2 id="关于人脸关键点的数据集WFLW数据预处理"><a href="#关于人脸关键点的数据集WFLW数据预处理" class="headerlink" title="关于人脸关键点的数据集WFLW数据预处理"></a>关于人脸关键点的数据集WFLW数据预处理</h2><p><a href="https://blog.csdn.net/baidu_40840693/article/details/105542910">https://blog.csdn.net/baidu_40840693/article/details/105542910</a></p><h2 id="瞳间距Inter-Pupil-Normalization-Inter-Ocular-Normalization"><a href="#瞳间距Inter-Pupil-Normalization-Inter-Ocular-Normalization" class="headerlink" title="瞳间距Inter-Pupil Normalization Inter-Ocular Normalization"></a>瞳间距Inter-Pupil Normalization Inter-Ocular Normalization</h2><p><a href="https://blog.csdn.net/code_mart/article/details/100121890">https://blog.csdn.net/code_mart/article/details/100121890</a></p><h2 id="衡量指标"><a href="#衡量指标" class="headerlink" title="衡量指标"></a>衡量指标</h2><p>用瞳间距或者两个外眼角之间的距离归一化计算标注点平均错误率。如果归一化平均错误率大于0.1，就算失败。</p><h1 id="本实验中的数据处理"><a href="#本实验中的数据处理" class="headerlink" title="本实验中的数据处理"></a>本实验中的数据处理</h1><p>本实验开源代码中提供了<a href="https://github.com/TheSouthFrog/stylealign/tree/master/pytorch_code">https://github.com/TheSouthFrog/stylealign/tree/master/pytorch_code</a>  cropped图片，图片中包含了人脸的部分，标注格式是文档中每一行有197个数据，196 + 图片名称 =197。</p><p>在代码中stylealign-master/pytorch_code/new_dataloader.py中提供的编码信息中可以看出使用了88个对称的关键点，位于中线上的51、52、53、54、57、79、90、94、85、16没有使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">corr_list =[<span class="number">0</span>,<span class="number">32</span>,<span class="number">1</span>,<span class="number">31</span>,<span class="number">2</span>,<span class="number">30</span>,<span class="number">3</span>,<span class="number">29</span>,<span class="number">4</span>,<span class="number">28</span>,<span class="number">5</span>,<span class="number">27</span>,<span class="number">6</span>,<span class="number">26</span>,<span class="number">7</span>,<span class="number">25</span>,<span class="number">8</span>,<span class="number">24</span>,<span class="number">9</span>,<span class="number">23</span>,<span class="number">10</span>,<span class="number">22</span>,<span class="number">11</span>,<span class="number">21</span>,<span class="number">12</span>,<span class="number">20</span>,<span class="number">13</span>,<span class="number">19</span>,<span class="number">14</span>,<span class="number">18</span>,<span class="number">15</span>,<span class="number">17</span>,<span class="number">33</span>,<span class="number">46</span>,<span class="number">34</span>,<span class="number">45</span>,<span class="number">35</span>,<span class="number">44</span>,<span class="number">36</span>,<span class="number">43</span>,<span class="number">37</span>,<span class="number">42</span>,<span class="number">38</span>,<span class="number">50</span>,<span class="number">39</span>,<span class="number">49</span>,<span class="number">40</span>,<span class="number">48</span>,<span class="number">41</span>,<span class="number">47</span>,<span class="number">55</span>,<span class="number">59</span>,<span class="number">56</span>,<span class="number">58</span>,<span class="number">60</span>,<span class="number">72</span>,<span class="number">61</span>,<span class="number">71</span>,<span class="number">62</span>,<span class="number">70</span>,<span class="number">63</span>,<span class="number">69</span>,<span class="number">64</span>,<span class="number">68</span>,<span class="number">65</span>,<span class="number">75</span>,<span class="number">66</span>,<span class="number">74</span>,<span class="number">67</span>,<span class="number">73</span>,<span class="number">76</span>,<span class="number">82</span>,<span class="number">77</span>,<span class="number">81</span>,<span class="number">78</span>,<span class="number">80</span>,<span class="number">88</span>,<span class="number">92</span>,<span class="number">89</span>,<span class="number">91</span>,<span class="number">95</span>,<span class="number">93</span>,<span class="number">87</span>,<span class="number">83</span>,<span class="number">86</span>,<span class="number">84</span>,<span class="number">96</span>,<span class="number">97</span>]</span><br></pre></td></tr></table></figure><h1 id="小张的问题"><a href="#小张的问题" class="headerlink" title="小张的问题"></a>小张的问题</h1><p>用了对称数据？这是为什么？怎么用？</p><p>使得网络学到对称结构特征。</p>]]></content>
      
      
      <categories>
          
          <category> 人脸关键点检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实验记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2018 ECCV A Deeply-initialized Coarse-to-fine Ensemble of Regression Trees for Face Alignment</title>
      <link href="/2021/09/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBEfficient-and-Accurate-Face-Alignment-by-Global-Regression-and-Cascaded/"/>
      <url>/2021/09/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBEfficient-and-Accurate-Face-Alignment-by-Global-Regression-and-Cascaded/</url>
      
        <content type="html"><![CDATA[<p><a href="https://paperswithcode.com/paper/a-deeply-initialized-coarse-to-fine-ensemble">https://paperswithcode.com/paper/a-deeply-initialized-coarse-to-fine-ensemble</a></p><p><img src="/2021/09/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBEfficient-and-Accurate-Face-Alignment-by-Global-Regression-and-Cascaded/DCFErank20210903.png" alt="DCFErank20210903"></p><p><a href="https://paperswithcode.com/sota/face-alignment-on-300w">https://paperswithcode.com/sota/face-alignment-on-300w</a></p><p>DCFE在300W私有和公共数据集排第4，3.24。</p><p><a href="https://github.com/bobetocalo/bobetocalo_eccv18">https://github.com/bobetocalo/bobetocalo_eccv18</a></p><h2 id="评价："><a href="#评价：" class="headerlink" title="评价："></a>评价：</h2><p>提出了DCFE Deeply-initialized Coarse-to-fine Ensemble of Regression Trees，一种基于<strong>粗到细的</strong> <strong>回归树集合(ERT)Ensemble of Regression Trees </strong>的<strong>实时面部地标回归方法</strong>。</p><h2 id="针对的问题："><a href="#针对的问题：" class="headerlink" title="针对的问题："></a>针对的问题：</h2><ol><li>解决零件变形的组合爆炸问题 the combinatorial explosion of parts deformation。</li><li>解决鲁棒性地回归器初始化、自遮挡问题以及同时进行正面和侧面人脸分析。</li></ol><h2 id="本文的目的："><a href="#本文的目的：" class="headerlink" title="本文的目的："></a>本文的目的：</h2><h2 id="实现的方法："><a href="#实现的方法：" class="headerlink" title="实现的方法："></a>实现的方法：</h2><p><strong>基于cnn的刚性脸位姿计算</strong>和<strong>基于ert的非刚性脸变形估计</strong></p><p><img src="/2021/09/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBEfficient-and-Accurate-Face-Alignment-by-Global-Regression-and-Cascaded/DCFE_framework.png" alt="image-20210903121741171"></p><p><img src="/2021/09/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBEfficient-and-Accurate-Face-Alignment-by-Global-Regression-and-Cascaded/DCFE_regression.png" alt></p><ul><li>$P(I)$ a set of probability maps经过CNN得到，之后经过高斯平滑滤波器GS，再经过Max函数，得到粗糙的标定，由于对于遮挡的敏感性不一定是一个有效的脸型。</li><li>$x^0=g_0(P(I))$是过POSIT得到的,$x^0$是一个$L\times2$的向量，$x^0$是一个有效的脸型，它作为ERT的初始化。</li><li>$S=\{s_i\}^N_{i-1}$ the set of train face shapes，$s_i=\{I_i,x^g_i,v^g_i,w^g_i,x^g_i\}$，对于每一个训练形状$s_i$有$I_i$训练图片，真值形状$x^g_i$,真值可见标定$v^g_i$,带标注的地表标签$w^g_i$(1代表标注了的，0代表丢失的)。</li><li>初始化形状为$x_i^0$,gt即目标形状为$x_i^g$,是一个$L\times2$向量。$L\times1$向量$v_i^g$是每个标定点可见性的二进制标签，如果对于第$k$个组件，$v_g(k)=1$，表示对于第$k$个地标是可见的。</li><li>shape-indexed features  $\phi(P(I_i),x^t_i,w_i^g)$依赖于在图片$I_i$关键点的当前的形状和它们是否被标注了，即$x_i^t$和$w_i^t$。</li><li>把回归过程分为$T$个阶段，对于第$t$阶段学习$K$个回归树集合，$C_t(f_i)=x^{t-1}+\sum^K_{k=1}g_k(f_i)$，$f_i=\phi(P(I),x^{t-1}_i,w_i^g)$,$x^j$是第$j$个阶段估计出的标定点的坐标，在第一个阶段中为$x^0$，为了训练整个ERT，使用$S$中的N个训练样本生成一个增广训练集，$S_A$的基数是$N_A$=$|S_A|$。</li><li>对于每个训练形状$s_i$，通过改变初始形状生成了附加的训练样本，从平滑的概率图中随机抽取新的候选地标位置，以生成新的初始形状。</li><li>每个初始化形状可以逐步被估计的形状和可见性增量$C^v_t(\phi(P(I_i),x^{t-1}_i,w^g_i)$，$x_i^{t-1}$代表了第$i$个样本的当前形状，$C^v_t$训练为了最小化关键的错误，但在每个叶节点上，要输出平均形状和训练形状的可见性，定义$U_{t-1}=\{(x_i^{t-1},v_i^{t-1})\}^{N_A}_{i=1}$是所有训练数据的所有当前形状和相应可见性向量的集合。</li></ul><ol><li><p>使用简单的卷积神经网络(CNN)来生成地标位置的概率图，深度神经网络的大感受野对人脸旋转、缩放和变形具有高度的鲁棒性。</p></li><li><p>拟合一个三维人脸模型，通过拟合一个<strong>刚性3D头部模型</strong>到估计的2D地标位置来计算初始形状，增强全局人脸形状先验，并估计全局人脸方向以解决自遮挡。</p></li><li><p>通过ERT回归器进一步细化特征，从粗到细的方法可以处理局部形变，让ERT很容易地解决<strong>非刚性零件</strong>所有可能变形的组合爆炸。</p><p><img src="/2021/09/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBEfficient-and-Accurate-Face-Alignment-by-Global-Regression-and-Cascaded/DCFElocal_deformation_coarse2fine.png" alt></p></li></ol><h3 id="ERT方法的优点"><a href="#ERT方法的优点" class="headerlink" title="ERT方法的优点"></a>ERT方法的优点</h3><p>容易并行化,它们的估计中隐含地强加了形状一致性。在粗到细的框架内使用ERT来实现精度和效率。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="/2021/09/02/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBEfficient-and-Accurate-Face-Alignment-by-Global-Regression-and-Cascaded/DCFE_Rseult.png" alt></p><h2 id="领域内目前存在的问题"><a href="#领域内目前存在的问题" class="headerlink" title="领域内目前存在的问题"></a>领域内目前存在的问题</h2><p>1.级联形状回归器(CSR)框架对回归过程的起点非常敏感。</p><p>2.深度模型的级联，逐步细化估计，从而增加计算需求。</p><h1 id="发现"><a href="#发现" class="headerlink" title="发现"></a>发现</h1><p>1.ECCV论文不是双列排版</p><p>2.第4页结尾v和w说反了？</p><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>1.Deeply-initialized的含义是什么？</p><p>2.Coarse-to-fine Ensemble是什么？</p><p>3.怎么用回归树？</p><p>4.实现方法中CNN提取特征生成标定位置概率图之后是ERT细化，ERT回归器的初始化方法具体怎么实现？</p><p>5.零件变形的组合爆炸问题 the combinatorial explosion of parts deformation是什么问题？</p><p>6.ERT回归树怎么做到加强形状一致性呢？人脸配准中的难点形状的一致性指的是什么?</p><p>7.RCN？怎么修改RCN?如何引入loss处理丢失的点？</p><p>8.softPOSIT算法？</p><p>9.初始化在什么时候初始化呢？在输入回归树之前吗？</p><p>10.$x^0$形式$L\times2$ with $L$ 2D landmarks coordinates是什么样子的？</p><p>11.$x_i^g$,是一个$L\times2$向量with the $L$ landmarks coordinates，语法上的with，这样用是表示含义吗？</p><p>12.如果对于第$k$个组件，$v_g(k)=1$，表示对于第$k$个地标是可见的。地标和部件代表的是一样的吗？</p><p>13.$\{C^v_t\}^T_{t=1}$算法中update的部分让人困惑，它到底是残差还是最最终值呀？</p>]]></content>
      
      
      <categories>
          
          <category> 人脸关键点检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>华为面试经历</title>
      <link href="/2019/09/18/%E5%8D%8E%E4%B8%BA%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86/"/>
      <url>/2019/09/18/%E5%8D%8E%E4%B8%BA%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86/</url>
      
        <content type="html"><![CDATA[<p>这篇博文主要来说说自己的华为面试经历吧。</p><p>这是一条时间轴(。・∀・)ノ——我是7月17号投的简历，那一阵子应该是在做数字图像的课程设计，之前给自己立的flag是7月20号之前投简历，也算是完成了。8月13号收到了上机通知，当时因为完全没准备，心里没底，给拒掉了。理论上应该是一周后的8月20号再次收到通知，但是并没有。就这样时间推移到了8月26号，华为来我们学校开岗位推介会了，我最终投递了<strong>上海无线网络产品线</strong>的<strong>嵌入式软件开发</strong>岗位。终于在9月3号我又收了上机通知，9月4号完成了上机，9月5号做了性格测试。正式面试时间安排在9月17、18号，我被安排在了18号。</p><p>9月30号我收到录用通知啦！</p><span id="more"></span><h3 id="上机考试"><a href="#上机考试" class="headerlink" title="上机考试"></a>上机考试</h3><p>软件类上机考试一般有三道题，我前期准备的时候用的是牛客网的华为上机练习。本身我以为我会在一周后考试，</p><p>当时我打算108道题，差不多七天能做完吧，结果发现自己真的是太naive且盲目乐观吧Orz。</p><p>上机考试中有很多题用python写就很方便，所以我复习了一波python。</p><p>好了，说说这次的上机题目。</p><h5 id="第一题：小明买钉子"><a href="#第一题：小明买钉子" class="headerlink" title="第一题：小明买钉子"></a>第一题：小明买钉子</h5><blockquote><p>小明去钉子店买钉子，钉子只有两种包装，4个一盒装、9个一盒装，钉子只能整盒买。</p><p>输入：小明需要的钉子数n（n&lt;200）。</p><p>输出：如果可以购买，请输出购买的最小盒数</p><p>​           如果无法购买，请输出-1。</p></blockquote><p>这一题一上来我有点慌，因为按照牛客的题目我期待中的第一题应该是字符串那种比较简单的题目，这个计数，那个反转啦……我又想了下递归那种，感觉很难，主要是自己很菜，愣了二十分钟。换了一种思路，直接暴力求解了，找整数解就行了，一个变量作为循环的index，看另一个能不能有整数解，如果为负数就直接结束循环。代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#第一题 4x+ 9y=n</span><br><span class="line">def jiefc(z):</span><br><span class="line">    jie = []</span><br><span class="line">    maxN = z//9 #最大循环次数</span><br><span class="line">    for N in range(maxN + 1):</span><br><span class="line">        F = (z - N*9)/4 #求解</span><br><span class="line">        if F&lt;0:</span><br><span class="line">            break</span><br><span class="line">        elif (F - int(F))==0:#是否为整数</span><br><span class="line">            jie.append([F,N])</span><br><span class="line">    return jie               </span><br><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        n = int(input())</span><br><span class="line">        answer = jiefc(n)</span><br><span class="line">        if answer==[]:</span><br><span class="line">            print(-1)</span><br><span class="line">        else:</span><br><span class="line">            total =[j[0]+j[1] for j in answer]</span><br><span class="line">            print(int(min(total)))</span><br><span class="line">    except:</span><br><span class="line">        break</span><br></pre></td></tr></table></figure><h5 id="第二题：实现一个简单代码解释器"><a href="#第二题：实现一个简单代码解释器" class="headerlink" title="第二题：实现一个简单代码解释器"></a>第二题：实现一个简单代码解释器</h5><blockquote><p>输入：</p><p>第一行输入一个整数n，接下来会输入n行代码；</p><p>输入的代码只有赋值和计算，计算只有加法；</p><p>如</p><p>4<br>  xx=       4<br>yy       =  1<br>              zz=  xx<br>cd  =               xx  +  yy+  zz</p><p>所有的变量名中不包含空格，所有代码输入符合格式。</p><p>输出：对代码进行解释，如果代码出错，输出NA</p><p>比如下边代码中第三行等号右边的cc在之前的代码里没有出现，这段代码显然是有问题的</p><p>4<br>xx  =  4<br>yy   =  1<br>zz=    cc<br>cd=xx  +  yy+  zz</p><p>假如代码无误，则输出最后一行等号左边量最终的结果</p></blockquote><p>这道题一上来也懵。但仔细想想，其实就是只有加分和赋值，赋值分为用整数直接赋值和用有效变量赋值。</p><p>唯一出错需要返回NA的原因就是使用了前边未出现过但现在出现在了等号右侧的无效变量。这题的思路就是把变量和其数值存起来，遇到无效变量直接返回NA，接收下一组代码；先判断前n-1行代码是否无误，无误则处理最后一行代码，单独判断最后一行代码有没有问题，没问题就输最后一行左边出最终结果。代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">def jiafa(s,chart):#求和，chart为有效变量表</span><br><span class="line">    jiasu = s.split(&#x27;+&#x27;)</span><br><span class="line">    he = 0</span><br><span class="line">    legal = 1    </span><br><span class="line">    for i in jiasu:</span><br><span class="line">        if i.isdigit():</span><br><span class="line">            he += int(i)</span><br><span class="line">        elif i in chart.keys():</span><br><span class="line">            he += chart[i]</span><br><span class="line">        else:</span><br><span class="line">            legal = 0</span><br><span class="line">            break</span><br><span class="line">    if legal ==1:</span><br><span class="line">        return he</span><br><span class="line">    else:</span><br><span class="line">        return &#x27;e&#x27;</span><br><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        kv = &#123;&#125;</span><br><span class="line">        n = int(input())</span><br><span class="line">        stopflag = 0</span><br><span class="line">        for i in range(n-1):</span><br><span class="line">            exp = input().split(&#x27;=&#x27;)</span><br><span class="line">            zuo, you = exp[0].replace(&#x27; &#x27;,&#x27;&#x27;), exp[1].replace(&#x27; &#x27;,&#x27;&#x27;)</span><br><span class="line">            if &#x27;+&#x27; in you:</span><br><span class="line">                kv[zuo] = jiafa(you,kv)</span><br><span class="line">                if kv[zuo] == &#x27;e&#x27;:</span><br><span class="line">                    print(&#x27;NA&#x27;)</span><br><span class="line">                    stopflag = 1  </span><br><span class="line">                    break  </span><br><span class="line">            else:</span><br><span class="line">                if you.isdigit(): #只赋值,无运算</span><br><span class="line">                    kv[zuo] = int(you)</span><br><span class="line">                elif you in kv.keys():</span><br><span class="line">                    kv[zuo] = kv[you]</span><br><span class="line">                else:</span><br><span class="line">                    stopflag = 1  </span><br><span class="line">                    print(&#x27;NA&#x27;)</span><br><span class="line">                    break </span><br><span class="line">        if stopflag:</span><br><span class="line">            stopflag = 0</span><br><span class="line">        else:</span><br><span class="line">            exp = input().split(&#x27;=&#x27;)</span><br><span class="line">            zuo, you = exp[0].replace(&#x27; &#x27;,&#x27;&#x27;), exp[1].replace(&#x27; &#x27;,&#x27;&#x27;)</span><br><span class="line">            if &#x27;+&#x27; in you:</span><br><span class="line">                kv[zuo] = jiafa(you,kv)</span><br><span class="line">                if kv[zuo] == &#x27;e&#x27;:</span><br><span class="line">                    print(&#x27;NA&#x27;)</span><br><span class="line">                else:</span><br><span class="line">                    print(kv[zuo])</span><br><span class="line">                    </span><br><span class="line">            else:</span><br><span class="line">                if you.isdigit(): #只赋值,无运算</span><br><span class="line">                    kv[zuo] = int(you)</span><br><span class="line">                    print(kv[zuo])</span><br><span class="line">                elif you in kv.keys():</span><br><span class="line">                    kv[zuo] = kv[you]</span><br><span class="line">                    print(kv[zuo])</span><br><span class="line">                else:</span><br><span class="line">                    print(&#x27;NA&#x27;)</span><br><span class="line">    except:</span><br><span class="line">        break</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="第三题-判断json格式是否错误"><a href="#第三题-判断json格式是否错误" class="headerlink" title="第三题 判断json格式是否错误"></a>第三题 <strong>判断json格式是否错误</strong></h5><blockquote><p><strong>输入 json格式数据</strong></p><ol><li><p>键值对{“*<em>”: ”\</em>*<em>\</em>”}；</p></li><li><p>一键对应多个值{“***”: [”****”, “***”]}；</p></li><li><p>键值对可以嵌套使用</p></li></ol><p>{“<em>**</em>”:</p><p>​      {</p><p>​           “<em>**\</em>*”,</p><p>​           “*****”</p><p>​      }</p><p>}</p><p>输入为多组json字符串，每组就只是一行字符串；</p><p><strong>错误类型：</strong></p><p>如果缺少大括号，记为0；</p><p>缺少中括号，记为1；</p><p>缺少引号，记为2；</p><p>缺少逗号，记为3；</p><p><strong>输出</strong><br>返回错误类型代号，多次同类型错误返回一次。比如犯了1，2，3类型错误，就输出123；<br>如果没错就输出‘y’。</p></blockquote><p>这题一上来，我先想到的正则，但正则只能知道对在哪里，不知道错误类型，而且这道题正则真的也不好写。而且json键值对可以嵌套使用，感觉这题得用嵌套、递归最小子问题那一套，我就很虚。</p><p>后来有和同学交流，他用简单粗暴的方法就是直接统计数量。前括号，后括号看数量是否一致；引号数量看是否为偶数。逗号就没办法了。这种方法显然是错的。(ノへ￣、)</p><p>我自己又想了想实在是想不出来，计算机的朋友说这个类似于编译原理词法分析器，另一位计算机的朋友说她的思路是栈，遇到符号先push，遇到匹配项pop，全部遍历完后，栈里剩的就是没有匹配项的错误字符。但是我不会栈，哎。我有顺着词法分析器去查怎么实现json解析器。看到了好久之前大概17年时候，我有留意的一个教程，是<a href="https://github.com/miloyip">Milo Yip</a>大佬出的<a href="https://github.com/miloyip/json-tutorial">从零开始的 JSON 库教程</a>，有一种自己在平行时空里兜圈子的感觉。又去试着学习一下，不过c确实忘得太多了，<strong>那我要立一个flag，过一阵子我就回来填坑</strong>。</p><h3 id="面试"><a href="#面试" class="headerlink" title="面试"></a>面试</h3><p>面试前几天，我在复习自己参加的比赛，做过的项目。面试头一天，我在网上查了华为嵌入式面试的题目，有很多c，指针，操作系统，寄存器的问题，特别虚。。。晚上我问了下同学关于第三题的想法，被告知有很厉害的大佬面试凉了，一上来直接Leetcode上中档或者难的题目，而且还提前被通知面试了。晚上赶紧看了一下大佬们遇到的题目：接雨水、锯齿数组。哎/_ \，确实不经过练习是有难度的。、</p><h5 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h5><p>我接到通知短信安排面试时间是9月18号上午10:00，过去先刷身份证排队，我很快就一面了。我一面的面试官是8月26号岗位推介会上见过的，对方当时对我印象应该不错。一上来我就说，“又见面了”，面试官回“那你应该没那么紧张了”(￣︶￣*))。</p><p>先是自我介绍。再问我关于上机题目的思路。看了下我github的库，我还真的有点紧张，面试官电脑上不了外网，我用我的手机给他看github，紧张的找不到浏览器放在了哪里Orz。我没有在上边放c的代码，可能对方比较关注c，问了我关于strlen()和sizeof()的区别，我记得sizeof会返回类型所占的内存byte数，strlen忘了。所以这题答得不好。</p><p>还问了之前做数学建模的我的角色。</p><p>接下来就是手撕代码，给我了一道括号匹配的问题，知识点就是栈，我赶紧补充了下上机第三题用栈的思路，并表示自己并不会栈。面试官给我换了一道题</p><blockquote><p>给一个数组，比如[3,3,4,2]，返回数组中数字能组成的周长最大的三角形周长，为7。</p></blockquote><p>我一上来没看到题目提示“排序”，铁憨憨地认为是求出所有3个数和的结果存在list中，在求最大值。面试官说我想的太复杂了，而且三角形需要满足两边之和大于第三边，提示我答题卡上的关键词为“排序”，我就有了思路，想把所有数从大到小排出来，从前朝后找最大的三个和且满足三角形条件的。不过后边写的是时候，出了点问题，我没想全，在排序之后，认位1,2位置固定，遍历找第三个，其实假如第三个也不符合，就直接不符合了，因为在3位置之后的一定不会比3位置上的数大。正确思路是在排序之后，从前向后找最大边长出现的位置，比如[6, 3, 3, 2, 2]，满足条件的是3，2，2。</p><p>这个时候我以为我凉了，我就问说假如第一轮题目就没做出来，是不是就凉了。他说是，我说那我就没通过了。面试官说我通过了，因为这题主要考排序，而我用的python一个sort()就做了，他之前有问一下sort的接口参数。还挺喜出望外的，因为听说大佬们都很难，自己确实没什么信心。只能说自己很幸运了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#用python就很简洁啊！</span><br><span class="line">a = [1, 2, 2, 3, 3, 6]</span><br><span class="line">a.sort(reverse = True)</span><br><span class="line">for i in range(len(a)-2):</span><br><span class="line">    if a[i]&lt;a[i+1]+a[i+2]:</span><br><span class="line">        print(a[i]+ a[i+1]+ a[i+2])</span><br><span class="line">        break</span><br></pre></td></tr></table></figure><p>结束时，一面面试官评价我，最让他感动的是我学习的精神，对上机题目的探究，各种问别人，（<em>゜ー゜</em>）就是基础不太行。</p><h5 id="二面"><a href="#二面" class="headerlink" title="二面"></a>二面</h5><p>一面结束后，很快就收到一面通过和去二面的通知，大概五分钟以内吧。</p><p>第二面也是先自我介绍，说我成绩不错。然后问了我最近用的语言，我说最近python用的多，比较熟，因为上机python比较方便。我补充说嵌入式用c比较多，c的话我们有课但是有一阵子不用了。面试官说，没事现在主流语言用的都挺多。</p><p>然后项目，让我说一个自己觉得比较好的项目。我就说了python爬虫那个，以及里边遇到的问题。json格式可以直接解析，我用了正则；数据库匹配失败，用了excel，面试官说，数据库不是你项目里的主要部分所以就放弃使用了，我回是的，并且excel可以来代替，只是一个excel文件里存多张表单文件会变大加载速度会变慢，不很方便。面试官说，看来excel还是不能完全代替数据库啊。我有说了因为网慢，返回错误5XX。去网吧跑程序的经历。还说了关于unix时间戳的事情。感觉面试官都get到了我的点！又说了有的页面还是有问题，我的方法是改变循环的start index，比如第5个好友空间页面的错了，下次就从第6个好友开始，面试官说了一个特别专业的词汇错误断点，异常断点之类的，我给忘了。</p><p>接下来，又是考察知识点的问题，问怎么排序，我说python就很容易，面试官说sort呀，我说是，就很好用。他问我知道哪几种排序，我说上午看了一点有冒泡，快速排序，插入排序，别的忘了。</p><p>他给我出的题目是写冒泡排序并优化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">6</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(a)-<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(a)):</span><br><span class="line">        <span class="keyword">if</span> a[i]&gt;a[j]: <span class="comment">#升序</span></span><br><span class="line">            a[i], a[j] = a[j], a[i]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure><p>怎么肥事，写的时候脑子很空。</p><p>接着对方问我怎么优化，比如[1, 2, 3, 4, 5, 6, 7, 8, 10, 9]，其实是不用排前边的，只需要交换最后两个就行，不需要遍历多余的次数，怎么优化。我的思路一上来就是很复杂的，用(index, value)，判断对应位置上元素变了吗，没变就是不用排，不过似乎更复杂了。面试官问，假如是不用这么复杂的，就比如第一轮就把10，9交换了，后边就不用排了。在我的程序里，是最后一轮才会交换10，9。Orz，后边的问题似乎就将错就错了。</p><p>我想到用一个flag，初始0，假如没交换位置的话，flag记为1，如果是1，就break。他说着就很容易break掉，我说要是能用Verilog的并行就好了，他说你其实是太紧张了，你想得出来的，没关心，这个是额外的。我又想了下换种思路，还是flag法，假如初始为0，交换是1，一直不换就break。发现面试的时候好像写错了Orz。不过没事对方应该明白了我的想法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(a)-<span class="number">1</span>):</span><br><span class="line">    flag = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(a)):</span><br><span class="line">        <span class="keyword">if</span> a[i]&gt;a[j]: <span class="comment">#升序</span></span><br><span class="line">            a[i], a[j] = a[j], a[i]</span><br><span class="line">            flag = <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> flag:</span><br><span class="line">        <span class="keyword">break</span>                </span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure><p>大概就这样，不用遍历完，发现面试的时候真的很乱啊Orz，希望不要有课后阅卷。</p><p>然后面试官说，感觉你挺喜欢编程的，刚才讲自己讲爬虫的时候很开心。我说嗯，是一个很有趣的体验吧，也体现出了爱去网吧的特点，感觉自己膨胀了，赶紧说，这是玩笑的，其实我觉得变成很重要，是做科研，做技术必备的工具。</p><p>然后二面就结束了，告诉我在外面稍等一会儿。</p><h5 id="三面"><a href="#三面" class="headerlink" title="三面"></a>三面</h5><p>三面是主管面，感觉是个领导，应该不做tech了？？？</p><p>问的问题有：</p><p>1.研究方向是什么？有点懵，我回答的是课程分类。他问我你的毕业设计是什么？还没选呢。</p><p>2.做项目你是怎么选择学习？</p><p>找网上的资源跟开源资料。</p><p>这么多资料怎么选择？</p><p>以任务为导向，请教他人。</p><p>一般在那些开源平台学习？</p><p>中国大学mooc，courses，书籍。</p><p>3.最近看了什么技术类的书？</p><p>一本网络书籍，python—cookbook。</p><p>你有什么收获，总结下书的重点？</p><p>这本书类似于速查手册，我能很快的掌握python的某种方法，而不用了解底层的东西。</p><p>没明白，什么叫底层？</p><p>就是最近在做上机，看这个类似于复习，我需要一个提纲一样的东西我不需要像第一遍看一样了解很多细节。</p><p>除了这本，还看过什么？</p><p>去年看了linux嵌入式编程，里边讲了一些嵌入式编程和软件变成的区别，比如const，volatile之类的怎么用。</p><p>4.团队合作你愿意做什么角色？</p><p>组织者，而不是领导者，提出自己的想法。</p><p>就是一个提意见的角色？</p><p>对，但是是以完成任务为导向的。</p><p>5.遇到什么问题，怎么解决，找方案，还是放弃？</p><p>爬虫遇到的问题，怎么解。</p><p>6.华为压力大？</p><p>做比赛压力就很大，我抗压能了不错的。、</p><p>7.想来华为吗？</p><p>特别想，优秀的本科生，研究生都去了，我也去。</p><p>8.你有什么问题？</p><p>岗前培训。</p><blockquote><p>大概差不多就这样吧，今天面试感觉暴漏了很多自己的缺点，也展示了自己的优点。无功无过吧。</p><p>想想大概最难的是需要面对内心的struggle，这篇文章大概率会在10天内更新吧。</p><p>无论结果如何，我都觉得自己很幸运了。</p><p>今天天气凉爽，睡个好觉。哈哈哈哈又想起了海涅的那句话……つ﹏⊂</p></blockquote><h3 id="10月1号更新"><a href="#10月1号更新" class="headerlink" title="10月1号更新"></a>10月1号更新</h3><p>距离面试过去12天了，这些天总体来说还算是忙碌充实，但心里还是挺焦虑的。事情压在一起，人就会很乱。虽然说觉得自己大概率是可以有offer的，也做好了被刷掉的准备，但是总是希望有个好结果吧。happiness = reality - expectation，期望越大容易失望越大吧。</p><p>我以为9月29号会来通知的，结果并没有，30号又听到朋友说有的部门发通知了，隐隐的还是有点着急。晚上回去洗澡的时候，洗到一半还停水了，幸好还没用沐浴露，在心里自嘲——可真是给充满困惑的九月画上了一个圆满的问号？(⊙ˍ⊙)</p><p>昨天大晚上22:59收到了意向书邮件，23:01收到了通知短信。本身特别困，跟好朋友们和家里人分享了一下后自己躺在床上脑子里有很多很多杂念，想着自己做的很多选择，想着接下来的机遇跟挑战。00:30还没有入睡。本来是想第二天早上再去看意向书的，1:00干脆开电脑签了算了。大概是一点多快两点入睡的吧。好像很少有这种开心的睡不着的时刻吧，主要是这个消息是晚上来的，没有缓冲时间。</p><p>总而言之，能被录用真的觉得自己特别幸运。还有就是觉得自己之前做的好多事情，看到的内容可能当时看起来无关紧要，但其实都产生了蝴蝶效应，又能归因于随机过程啦，狗头。这次又算是我向宇宙发出了信号，宇宙给了我正向的回应吧。</p><blockquote><p>还有一些话就等到过一段时间再来说吧。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 面试经历 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EDA课程设计、dsp课程设计</title>
      <link href="/2019/07/05/EDA%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E3%80%81dsp%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1/"/>
      <url>/2019/07/05/EDA%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E3%80%81dsp%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<p>这篇博文来说说六月的EDA课程设计、dsp课程设计还有一些零零碎碎的想法。</p><p>本来是应该六月尾巴写这篇博文的，一不小心就拖到今天了Orz。</p><h3 id="EDA课程设计要求"><a href="#EDA课程设计要求" class="headerlink" title="EDA课程设计要求"></a>EDA课程设计要求</h3><p>我们的EDA课程计使用的是学院实验室的Altera试验箱，核心板是Cyclone II  EP2C5Q208C8芯片。</p><p>课程设计的要求如下：</p><blockquote><p>应用Verilog HDL硬件描述语言，设计一个基于FPGA的空气净化器程序，模拟空气净化器的运行，其功能为：</p><p>1 实现5档出风的风力控制，使用控制电机模拟<br>2 实现当前空气质量的检测，通过设置拨码开关模拟<br>3 实现当前状态的指示与显示<br>4 实现睡眠设置功能，在指定的设置时间后风量输出调节至1档<br>5 实现连续运行时间的记录，并进行更换空气过滤网提示报警<br>6 具有自动模式和手动模式选择，自动模式根据检测到的空气质量自动调节风速，手动模式通过按键设置风速档位<br>7 实现时钟功能，并且根据设定的时间自动关机</p></blockquote><p>代码我放在了Github上<a href="https://github.com/iszff">iszff</a>/<a href="https://github.com/iszff/Air-cleaner">Air-cleaner</a>。</p><h3 id="dsp课程设计要求"><a href="#dsp课程设计要求" class="headerlink" title="dsp课程设计要求"></a>dsp课程设计要求</h3><p>dsp课设使用的是TI2812的板子。</p><p>课程设计要求如下：</p><blockquote><p>1.设计60hz限波器；<br>2.采集一段语音信号，加入60hz干扰；<br>3.对干扰信号进行滤波；<br>4.展示滤波效果。</p></blockquote><span id="more"></span><h2 id="EDA课设"><a href="#EDA课设" class="headerlink" title="EDA课设"></a>EDA课设</h2><p>这个课设主要有两个模块构成：控制模块，计时模块。另外就是需要考虑到优先级的问题。</p><p>最终我们做出的结果如下：</p><blockquote><p>1.四个拨码SW1，2，3，4表示空气质量 ，会自动匹配灭灯的盏数（因为低电平是亮灯，三态情况下所有灯都亮，所以用灭灯表示了）。1111，1110，1100，1000，0000表示空气质量从好到差；自动匹配D0，1，2，3，4，5即电机档数由最弱一档到最强五档的情况为，10000，11000，11100，11110，11111。</p><p>2.四位按键（1，1）分十位（1，2）分个位（2，1）秒十位（2，2）秒个位，输入还有多长时间进入睡眠的时长量。（1，4）表示确认输入量，开始倒计时。还有多长时间进入睡眠由右边四位数码管显示。在倒计时即还没有进入睡眠之前仍然可以通过K7重新置数。进入睡眠之后，无论空气质量即拨码开关输入为何种情况，空气净化机自动为一档即只灭一盏灯。想要退出睡眠模式，需要使用K8全局复位。在全局复位后使用K7退出设置睡眠模式。</p><p>3.使用按键（1，4），（2，4），（3，4），（4，4）（4，3）分别表示手动控制电机从一档到五档。手动控制优先级高于自动控制，而且具有记录用户习惯的功能，即复位后不会清除手动控制，手动控制需要由用户通过按键（4，2）退出自定义模式。</p><p>4.正向记录运行时间，在程序中设定了开始运行10s后蜂鸣提醒更换滤网，该时间可以在程序中设定，在左边四位数码管显示运行时间记录。</p></blockquote><p>代码的部分就不多说了，说说自己的小小感悟吧。Verilog HDL是一种硬件描述语言，硬件描述语言当然是来描述硬件的。这里就不得不提FPGA可编程逻辑门阵列了，他内部的硬件逻辑电路会因为我们的程序而改变，而不是出厂后就规定死的。这一点直接决定了Verilog和C语言的不同，Verilog是并行的，变量在一处改变，在实际电路中就是逻辑门的改变，这会导致整个程序运行结果的改变。所以编程中对同一个变量的操作不能放在不同的进程中，简单理解不能放到不同的always()里。一开始，我没有意识到这一点程序就会报错。后边在处理手动挡控制和进入睡眠模式的逻辑时也是费了一些功夫的。</p><p>另外我遇到两个小错误就在顶层例化部分的问题。我在顶层例化在模块中漏例化了拨码开关输入量，而此时编译器不报错，但拨码开关输入量无效，导致程序下载运行后没有出现预期效果。经历了一段非常艰难的调试过程。</p><p>还有一个就是顶层声明数码管的时候没有写八段数码管的位宽，导致数码管显示有问题，后来检查让RTL原理图时才找到原因。</p><p>不过最后还是实现了所有的功能，还是很开心的。</p><h2 id="dsp课设"><a href="#dsp课设" class="headerlink" title="dsp课设"></a>dsp课设</h2><p>这个dsp课设可真是让人头秃啊，没用过dsp板子，一上来不知道从何处下手。</p><p>板子上自带了AIC23，可以采集语音并播放。需要对语音采样，通常情况下是44.1K的采样率，但这种情况下设计陷波滤波器难度就很大。好几万个点只滤去几个点….那滤波器的阶数自让就会很高。于是我选择用AIC23模块里最低的采样频率8021Hz，但即使这样，用matlab中FDAtool生成的滤波器阶数依然高达1000多阶。最后通过放宽对过渡带的要求才勉强生成了800多阶的滤波器。不过FDAtool生成的滤波器滤波效果并不好，于是我们使用了fir1()函数来生产滤波器，然后把滤波器系数导出为一个CCS中可以使用的头文件。</p><p>我们组的方案是想先采集语音，然后加入噪声，再滤波，用CCS中自带的graph工具通过看FFT后的频谱图来观察滤波结果。但是由于CCS中graph功能的FFT输入必须为整数。所以最终显示的结果很差。</p><p>这个dsp课设做的就有点失败Orz</p><h2 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h2><p>又到了碎碎念的环节QAQ，科技博客迟早变成情感博客…</p><p>dsp课设安排在六月的前两个周，Verilog课设安排在后两个周，6月21号有一门《无线电技术与应用》考试，6月26号又《随机过程》的考试。</p><p>一上来的dsp课设就让人喘不过气来，哎，主要是自己太菜了。而且由于做课设，留给复习的时间就很少，整个人就很烦。EDA课设做的还算是顺利吧。</p><p>我们这两个课设都是小组合作的，小组合作的时候就会有人滑水，而且是全程滑水，我就有点生气，当然最底层的症结就是我自己的水平不行。之前和朋友开玩笑说，当我能做出课设的时候 ，我就是“科技博主”。但是当我做不出来的时候，我就想一些其他的，一些所谓的“哲学问题”吧，我在世界中的位置？我是谁？为什么和他人沟通这么难？我就成了一个“情感博主”。似乎有时候人是需靠摔锅给队友或者“这件事不是因为我做不好，而是因为我不做”这种虚幻的错觉才活得下去，哎，我必须不停的提醒自己<strong>“I am a powerful、independent person.‘’</strong>而且我老想着蜘蛛侠里边的台词<strong>‘’ If you are nothing without the suit, then you shouldn’t have it.‘’</strong>要是没有好队友就做不出课设，那这课设我本身也就做不出来Orz。</p><p>而且事情都堆在一起的时候，人就会很烦。人的心智带宽是有限的，一旦心智带宽被占得死死的时候，整个人就会很累，是没办法用多余的心智带宽来约束自己的。六月份几乎都睡得挺晚的，有时候，我想睡早点，但一躺到床上，脑子里就想一些有的没的，很难睡着。于是我都故意睡得晚一些，睡觉前哪怕是不做正事，只是看手机，我也要晚点睡觉。因为不想听见自己心中真正的声音就老想着用一些噪音来把它覆盖掉。而且我还故意欺骗自己：<strong>”偏执的保持健康的生活习惯这一点本身就很不健康。“</strong>Orz</p><blockquote><p>这学期也算是过完了，似乎每个月都有事情在忙，三月、四月上在准备英语竞赛，四月下、五月开始学数学，考了三门试，六月有两门考试和俩课设。希望接下来自己能无所畏惧吧！</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 课程设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Verilog </tag>
            
            <tag> dsp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数学建模竞赛和大三上的总结</title>
      <link href="/2019/02/24/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E5%92%8C%E5%A4%A7%E4%B8%89%E4%B8%8A%E7%9A%84%E6%80%BB%E7%BB%93/"/>
      <url>/2019/02/24/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E5%92%8C%E5%A4%A7%E4%B8%89%E4%B8%8A%E7%9A%84%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>这篇博文主要是谈谈自己在2018年九月份的全国大学生数学建模竞赛CUCUM和2019年1月份的数学建模美赛MCMICM中的一些收获以及对于大三上学期的一些感悟与总结吧。</p><p>这是一篇客观存在比较少，主观虚无比较多的博文（就是没什么干货，光有我自己的一些干扯）。</p><span id="more"></span><h3 id="国赛"><a href="#国赛" class="headerlink" title="国赛"></a>国赛</h3><p>夏天的时候突然想参加数学建模竞赛，拉拢几位兄台一起比赛，结果有的有事要忙，有的没兴趣。最终我们队伍由一位上一届的计科的学姐和同届的一位能动同学以及来自电信的我三个人桃园结义组成，确实我们不是同年同月同日生，但却在做不出数模题的时候想要同年同月同日原地哭唧唧。</p><p>数学建模是基于一个实际的问题，用数学的方法对问题进行描述、分析的过程。数学建模的队伍一般有三个人组成，三个人分别主要负责建模、编程、写作。前期我们的任务分工是计科学姐当仁不让地负责编程工作，至于我和能动同学谁来写作、谁来建模最终<strong>由掷硬币得出</strong>，我来当写手，能动同学来当建模手。</p><h4 id="数学建模的思维"><a href="#数学建模的思维" class="headerlink" title="数学建模的思维"></a>数学建模的思维</h4><p>暑假的时候我们三个人都学习了数学建模的一些基本知识，建模建模最重要的当然是在于模型本身啦，三个人的思路相互补充才能建立相对比较完善的模型。这里不能不说姜启源、谢金星的《数学模型》这本书非常棒，书中内容来源于现实问题，关于这些问题建立的模型总能给我一种在揭示世界运行原理的感觉。</p><p>比较有意思的是对于很多问题<strong>我们的直觉型思考和真正通过数学模型的分析求解会有很大的偏差</strong>。这本书在B站有相应的课程，谢金星老师讲的很好。印象挺深刻的是一个关于<strong>公平席位分配问题</strong>。</p><blockquote><p>假设某学校有3个系共200名学生，其中甲系有100名学生，乙系有60名学生，丙系有40名学生。假如学生代表会议设20个席位，公平又简单的席位分配原则是按学生人数比例分配，显然席位分配情况为<strong>甲: 乙: 丙  =100 : 60: 40 = 10: 6: 4</strong>。</p><p>现在丙系有6名同学转入甲、乙系，三系人数比为<strong>甲: 乙: 丙  =103 : 63: 34 = 51.5%: 31.5%: 17%</strong>。按比例分配的话那么三系的席位比应为10.3: 6.3: 3.4，但由于这个实际问题是不存在小数情况的，于是在取得整数的19席分配完毕后，三系同意按照所谓惯例把第20席位给比例分配中小数部分最大的丙系，于是三系席位分配情况仍为<strong>甲: 乙: 丙 = 10: 6: 4</strong></p><p>有20个席位的代表会议在表决提案时可能会出现10: 10的情况，于是会议决定下一届增加一席，共21席。三系人数比仍为甲: 乙: 丙  =103 : 63: 34 = 51.5%: 31.5%: 17%。按比例分配三系的席位比应为10.815: 6.615: 3.570，参照惯例的话三系席位分配情况为<strong>甲: 乙: 丙 = 11: 7: 3</strong></p><p>这个结果显然对丙系是非常不公平的，系人数比例不变，总席位增加1，而丙系却由4席减为3席。</p></blockquote><p>问题出现在所谓的按照惯例分配上，即把按整数比例分配席位后的剩余席位按照小数部分的大小来分配。</p><p>实际上由A.Hamilton提出的这种<strong>比例+惯例</strong>，称之为<strong>最大剩余法</strong>或最大分数法的席位分配方法在美国国会1850——1900年的众议员席位分配中（按人口比例每个州应分得几个席位）多次被采用。它被质疑的一个理由就是出现上述例子中的所谓的<strong>席位悖论</strong>——总席位增加反而可能导致某洲席位的减少。</p><p><strong>最大剩余法</strong>还有一个重大缺陷是所谓的<strong>人口悖论</strong>，某洲的人口增加较多反而导致该州的席位减少。假如例子中甲、乙、丙三系人数分别变为114、64、34的话，按最大剩余法21席的分配结果将是11，6，4。结果就是甲系人数加1，席位数不变；乙席人数加1，席位数减1；丙系人数不变，席位加1。</p><p>为了优化看似公平的席位分配实则有很大bug的方案，美国国会把这个问题交给了哈佛大学数学系某知名教授（名字忘了），教授给出了250多种的优化后的方案。核心思想是建立衡量分配不公平程度的指标——相对不公平度，制定席位分配原则时应使它们尽可能小。</p><p>谢金星老师在课里边说：没有绝对的公平，只是看你站在哪一方。这或许就是所谓的利益相关？？？</p><p>最后老师还提出了一个挺意味深长的问题——得到的席位多就一定有权利吗？</p><p>学习怎么分析问题、建立数学模型能使人学会相对冷静、深入的思考吧。但是在实际生活中，碰到某些目标和执念的时候，后天所训练出的理性很快就没了，人一下子就会被直觉型思维带出来的各种情绪所主导。所以我在想怎么才能在面对具体事情的时候还能做到客观的用理性来分析呢？或者做到尽量不受干扰呢？</p><h4 id="竞赛环境下的紧张"><a href="#竞赛环境下的紧张" class="headerlink" title="竞赛环境下的紧张"></a>竞赛环境下的紧张</h4><p>由于我是写手，暑假的时候，我就学习了怎么使用latex。我们组在假期进行了1.5次的赛前模拟。第一次模拟的时候，我们选的是2007年A题，关于中国人口预测的一个题目。我们想了两天，实在是无从下手，然后放弃，这算是0.3次吧。然后又做了一次，我们选择了2008年B题，关于怎么评价高校学费合不合理，情况比第一次好点，但最后还是无从下手，算是0.4次吧。第三次模拟我们选了2009年B题，关于病床调度分配的，我们想到了排队论、cpu调度原理什么的，但可能因为模型不好，代码逻辑有问题，出的结果有负数，显然不对，这次模拟就算是0.8次吧。</p><p>在真正的比赛中我们选择了关于高温工作环境下隔热服传热情况分析的A题。礼拜四晚上比赛开始，礼拜五我和能动的同学各有两节课，能动同学他刚好有《传热学》这门课，因为我是写手我也必须清楚原理才能在论文中说明白。礼拜五晚上我在B站速成传热学。</p><p>坦白讲，这题挺刁难人的，礼拜六，建模手的式子还是没有完全出来。我想起来了之前计算方法、数理方程相关的内容又一起编了一编。最终我们找到了一篇重要的文献，结合了一下里边的思路，才勉强做出了一个我们没有很明白的式子。礼拜六下午整个人还特别想吐╯︿╰</p><p>礼拜六晚上我忙着开始写论文，学姐找相关求解的方法。我从零点睡到一点半，学姐从三点睡到五点半，礼拜天早晨我们才完成赛前定的时间节点计划中礼拜五晚上应该做的东西。</p><p>这个时候我有点想放弃了，学姐跟我说<strong>现在放弃，最后一定会后悔</strong>，我们一定要完成。于是礼拜天三个人肝了一天。最后晚上的时候学姐跑完程序有急事先走了，我和建模手一起弄论文。我忙着排版，写文献引用，所以建模手最后也参与了写作，他完成了摘要、优缺点、结论。不过我们论文中第三问没出结果。</p><p>最后我们21：52才上交作品，离结束也就只有8分钟了。其实最后极限操作的时候，建模手在我旁边安慰我，但我整个人有点火，跟他说不要讲话。</p><p>交完后才发现论文里一个标号有点问题，我们俩一起回去的时候，我有点低落，感觉准备了一些时间，还熬了这么几天结果就制造了一篇我们自己都不认可的学术垃圾。又是这位建模手佛系同学跟我讲<strong>太阳明天照常升起</strong>。确实，或许是我稍微有了一些得失心吧。</p><p>我们三个以为我们应该是当分母了，10月份的时候我们组被预推国一，最后结果得了国二，我们组确实挺意外的。坦白讲，我甚至有点羞愧，因为感觉自己真的做得烂，感觉自己是个骗子（fraud、fraud、fraud  Orz），不应该得奖，内心不安。但是我们组确实准备的挺久的，得奖或许是因为同行衬托？？？</p><h3 id="美赛"><a href="#美赛" class="headerlink" title="美赛"></a>美赛</h3><p>比完国赛没多久，我就问俩队友还美赛吗，学姐要考研，到时候可能会准备复试就说不参加了。于是我在一个群里看到有个人问有没有人做美赛，于是我们就组队了。</p><p>本来我想换成编程，能动同学来写作，新队友机械同学来建模的。但因为我上学期的课程太紧张了，没时间看算法了，机械同学做过matlab课设，而且我latex用的比较熟练，于是机械同学就担任编程手，我和能动同学的任务分配不变。</p><p>寒假留校，主楼暖气低温运行真的挺冷的，食堂吃的简直不想了。</p><p>美赛我们做的A题如何生态养龙。我们使用了微分方程模型，关于具体的细节我们争论了挺久的才得出了比较优化的结果。比赛1月29号上午结束，28号晚上我在集中住宿的寝室通了一宵肝论文，到早上6：30才去睡觉。就是那种我觉得我一点能写完论文，过了会我看了下时间一点半了，我觉得两点能写完，过了一会两点半了，我觉得三点能写完，过了一会天亮了。</p><p>论文中需要提交一封信件，这封信件还有由能动同学写的。</p><p>29号上午，我们一起揪了一会儿错误。机械同学要去赶火车，提前走了。我和能动同学一起给官方发提交邮件。</p><p>不过第二天我发现我们的论文里还是有点小瑕疵的，算是遗憾吧。</p><p>关于两个比赛，感觉美赛更开放，不过美赛官方有点坑，C题题目改了多次，又加上当时美国政府罢工，有一道题给的网站连接进不去。美赛之前出现了两个中国、visa卡盗刷的事件，而且现在美赛97%的队伍都来自于中国大陆了，感觉美赛有点不靠谱呀。</p><h4 id="记忆点"><a href="#记忆点" class="headerlink" title="记忆点"></a>记忆点</h4><p>比赛中大家的争论最终碰撞出了思维的火花，感觉比赛重在三人的合作，最强的队伍不一定有三个最强的人，但是一定有很棒的配合。</p><p>两次比赛中队友都跟我说让我 不要着急、不要着急、不要着急… 确实，我会有容易着急，其实对自己内耗也有点大，最好能改改。</p><h3 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h3><p>其实这篇博客应该在春节之前写完的，今天才写完主要是因为拖延。。。（他宛如戏台上的老将军，背上插满了flag.jpg）还有就是正如<a href="http://iszff.top/2018/09/02/qq好友空间说说爬虫/#more">本人的上篇博文——QQ好友空间爬虫</a>，又要开学了，再不写可能就emmmm…</p><p>上学期确实过的比较tough，之前一位朋友发了一条说说，是王安石《游褒禅山记》中的内容：夫夷以近，则游者众；险以远，则至者少。而世之奇伟、瑰怪，非常之观，常在于险远，而人之所罕至焉，故非有志者不能至也。</p><p>写的真好，希望自己这学期能够多一些客观存在，少一些主观虚无吧。</p><p>另外就是今天修了图床，从七牛云换成了腾讯cos,还解决了其余几个让人有点懊恼的小问题。</p><p>想起国赛结束那天晚上，我回寝室插插销，我的插座被烧坏了。美赛结束第二天，去海边，结果踩到了石油，擦不掉。或许我们向宇宙发出讯号，宇宙会有回应。我把这些小事联系起来其实就特别主观虚无，但我还是挺感谢自己依然有些天真古怪的念头。希望宇宙再给我一些回应吧。</p>]]></content>
      
      
      <categories>
          
          <category> 数学建模 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学建模 </tag>
            
            <tag> 生活总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>qq好友空间说说爬虫</title>
      <link href="/2018/09/02/qq%E5%A5%BD%E5%8F%8B%E7%A9%BA%E9%97%B4%E8%AF%B4%E8%AF%B4%E7%88%AC%E8%99%AB/"/>
      <url>/2018/09/02/qq%E5%A5%BD%E5%8F%8B%E7%A9%BA%E9%97%B4%E8%AF%B4%E8%AF%B4%E7%88%AC%E8%99%AB/</url>
      
        <content type="html"><![CDATA[<p>这篇日志主要记录、总结自己写的一个小爬虫，算是自己学习python的小练习吧。</p><p>这个爬虫主要实现了爬取qq好友们2018年的说说、提取关键信息并且存入excel文件的功能。</p><p><strong>该爬虫使用的python版本是<a href="https://www.python.org/">python3</a>。</strong></p><p><strong>该爬虫的使用第三方库有<a href="https://pypi.org/project/selenium/">selenium</a>、<a href="http://docs.python-requests.org/zh_CN/latest/user/quickstart.html">requests</a>、<a href="https://docs.python.org/3.5/library/re.html">re</a>、<a href="https://pypi.org/project/openpyxl/">openpyxl</a>、<a href="https://docs.python.org/3.5/library/time.html">time</a>。</strong></p><p>其中，selenium库用于登陆访问页面，requests库用于向网页发送请求、re库用于匹配、提取返回包含说说信息的内容，openpyxl库用于储存信息，time库用于计算程序运行时间。</p><p>程序源码放在了GitHub上<a href="https://github.com/iszff">iszff</a>/<a href="https://github.com/iszff/QQ-Crawler"><strong>QQ-Crawler</strong></a>。</p><span id="more"></span><h3 id="爬虫思路"><a href="#爬虫思路" class="headerlink" title="爬虫思路"></a>爬虫思路</h3><div id="flowchart-0" class="flow-chart"></div><h3 id="使用selenium登陆空间"><a href="#使用selenium登陆空间" class="headerlink" title="使用selenium登陆空间"></a>使用selenium登陆空间</h3><p><a href="https://www.seleniumhq.org/">selenium</a>是一个自动化测试工具，它可以寻找、定位、选择网页中的元素，向网页提交一些信息，如输入用户名、密码，也可以获取页面中的信息，如返回当前页面的url、网页源码。</p><p>感觉测试攻城狮<a href="https://me.csdn.net/u011541946">Anthony_tester </a>的<a href="https://blog.csdn.net/u011541946/article/category/6788788">Python+Selenium自动化测试从零到框架设计系列 </a>教程写的很棒,适合入门。</p><h4 id="进入好友空间说说页面、登陆并获取信息："><a href="#进入好友空间说说页面、登陆并获取信息：" class="headerlink" title="进入好友空间说说页面、登陆并获取信息："></a>进入好友空间说说页面、登陆并获取信息：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login_in</span>(<span class="params">url, qq, password</span>):</span></span><br><span class="line">    driver = webdriver.Chrome()</span><br><span class="line">    driver.set_page_load_timeout(<span class="number">10</span>)</span><br><span class="line">    driver.get(url)</span><br><span class="line">    time.sleep(<span class="number">6</span>)</span><br><span class="line">    driver.switch_to.frame(<span class="string">&#x27;login_frame&#x27;</span>)<span class="comment">#通过观察网页源码，发现账号和密码的输入框在&lt;frame&gt;标签下，需要跳入框架中，否则定位不到</span></span><br><span class="line">    driver.find_element_by_xpath(<span class="string">&#x27;//*[@id=&quot;switcher_plogin&quot;]&#x27;</span>).click()<span class="comment">#点击选择</span></span><br><span class="line">    driver.find_element_by_xpath(<span class="string">&#x27;//*[@id=&quot;u&quot;]&#x27;</span>).clear()</span><br><span class="line">    driver.find_element_by_xpath(<span class="string">&#x27;//*[@id=&quot;u&quot;]&#x27;</span>).send_keys(qq)<span class="comment">#输入账号</span></span><br><span class="line">    driver.find_element_by_xpath(<span class="string">&#x27;//*[@id=&quot;p&quot;]&#x27;</span>).clear()</span><br><span class="line">    driver.find_element_by_xpath(<span class="string">&#x27;//*[@id=&quot;p&quot;]&#x27;</span>).send_keys(password)<span class="comment">#输入密码</span></span><br><span class="line">    driver.find_element_by_xpath(<span class="string">&#x27;//*[@id=&quot;login_button&quot;]&#x27;</span>).click()</span><br><span class="line">    time.sleep(<span class="number">6</span>)</span><br><span class="line">    </span><br><span class="line">    html = driver.page_source<span class="comment">#返回页面内容，根据内容判断内否进入空间</span></span><br><span class="line">    fw = re.findall(<span class="string">r&#x27;主人设置了权限，您可通过以下方式访问&#x27;</span>,html)<span class="comment">#判断好友空间是否限制权限</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(fw)!=<span class="number">0</span>:</span><br><span class="line">        driver.delete_all_cookies()</span><br><span class="line">        driver.quit()</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:  </span><br><span class="line">        cookies = driver.get_cookies()<span class="comment">#获取cookie</span></span><br><span class="line">        cookie = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> elem <span class="keyword">in</span> cookies:</span><br><span class="line">           cookie[elem[<span class="string">&#x27;name&#x27;</span>]] = elem[<span class="string">&#x27;value&#x27;</span>]</span><br><span class="line">        driver.delete_all_cookies()    </span><br><span class="line"></span><br><span class="line">        hashes=<span class="number">5381</span></span><br><span class="line">        <span class="keyword">for</span> letter <span class="keyword">in</span> cookie[<span class="string">&#x27;p_skey&#x27;</span>]:</span><br><span class="line">            hashes += (hashes &lt;&lt; <span class="number">5</span>) + <span class="built_in">ord</span>(letter)</span><br><span class="line">        gtk=<span class="built_in">str</span>(hashes &amp; <span class="number">0x7fffffff</span>)   </span><br><span class="line">    </span><br><span class="line">        g_qzonetoken = re.findall(<span class="string">r&#x27;window\.g\_qzonetoken \= \(function\(\)\&#123; try\&#123;return \&quot;.*?\&quot;&#x27;</span>,html)</span><br><span class="line">        qzonetoken=g_qzonetoken[<span class="number">0</span>].split(<span class="string">&#x27;&quot;&#x27;</span>)[-<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">        driver.quit()</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> cookie, gtk, qzonetoken  </span><br><span class="line">    </span><br></pre></td></tr></table></figure><blockquote><p>通过观察浏览器控制台中的请求头，发现请求连接中有g_tk和qzonetoken这两项</p><p>而且两项都是动态变化的，每次刷新都会变化，不过经过多次…尝试，似乎qzonetoken的变化影响不大</p><p>qzonetoken包含在页面的源代码中，可以通过正则匹配直接获得</p><p>在js内容找出可以找出一个g_tk的加密算法（向大佬们低头）</p></blockquote><h3 id="使用requests向网页发送请求"><a href="#使用requests向网页发送请求" class="headerlink" title="使用requests向网页发送请求"></a>使用requests向网页发送请求</h3><p><a href="http://docs.python-requests.org/zh_CN/latest/user/quickstart.html">requests</a>可以通过传递参数向网络发送<a href="https://baike.baidu.com/item/HTTP请求/10882159?fr=aladdin">http请求</a>,可以获得不同类型的<a href="http://docs.python-requests.org/zh_CN/latest/api.html#requests.Response">Response</a>内容。</p><p><a href="http://docs.python-requests.org/zh_CN/latest/user/quickstart.html">官方文档</a>写的很不错。</p><h4 id="构造链接"><a href="#构造链接" class="headerlink" title="构造链接"></a>构造链接</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">urlOrd = &#x27;https://user.qzone.qq.com/proxy/domain/taotao.qq.com/cgi—bin/emotion_cgi_msglist_v6?uin=&#x27;</span><br><span class="line">sec=&#x27;&amp;ftype=0&amp;sort=0&amp;pos=&#x27;</span><br><span class="line">third=&#x27;&amp;num=20&amp;replynum=100&amp;g_tk=&#x27;</span><br><span class="line">forth=&#x27;&amp;callback=_preloadCallback&amp;code_version=1&amp;format=jsonp&amp;need_private_comment=1&amp;qzonetoken=&#x27;</span><br><span class="line">fifth=&#x27;&amp;g_tk=&#x27;</span><br><span class="line"></span><br><span class="line">for ye in range(400):</span><br><span class="line">    pos = str(ye*20)</span><br><span class="line">    emotionurl=urlOrd+ friend_qq + sec + pos + third + gtk + forth + qzonetoken+ fifth + gtk</span><br></pre></td></tr></table></figure><blockquote><p>观察确定请求url格式，构造请求url</p></blockquote><h4 id="对网页请求"><a href="#对网页请求" class="headerlink" title="对网页请求"></a>对网页请求</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def get_emotion(url,cookie):</span><br><span class="line">    header= &#123;&#x27;accept-encoding&#x27;: &#x27;gzip, deflate, br&#x27;, &#x27;accept-language&#x27;: &#x27;zh-CN,zh;q=0.8&#x27;, &#x27;host&#x27;: &#x27;h5.qzone.qq.com&#x27;, &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36&#x27;, &#x27;accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&#x27;, &#x27;connection&#x27;: &#x27;keep-alive&#x27;&#125;</span><br><span class="line">    r = requests.get(url, headers=header, cookies=cookie) </span><br><span class="line">    return r.text #返回文本信息</span><br></pre></td></tr></table></figure><h4 id="获取好友信息"><a href="#获取好友信息" class="headerlink" title="获取好友信息"></a>获取好友信息</h4><p>获取好友信息的爬虫思路和获取说说页面响应的思路基本一致，只是url有所改变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">enter_qzone</span>(<span class="params">my_qq, my_password, save_path</span>):</span><span class="comment">#进入网页 获取cookies 构造链接</span></span><br><span class="line">    url = <span class="string">&#x27;https://user.qzone.qq.com/&#x27;</span>+my_qq+<span class="string">&#x27;/311&#x27;</span></span><br><span class="line">    (cookie, gtk, qzonetoken) = login_in(url, my_qq, my_password)</span><br><span class="line"></span><br><span class="line">    urlOrd = <span class="string">&#x27;https://user.qzone.qq.com/proxy/domain/r.qzone.qq.com/cgi-bin/tfriend/friend_hat_get.cgi?hat_seed=1&amp;uin=&#x27;</span></span><br><span class="line">    second =<span class="string">&#x27;&amp;fupdate=1&amp;g_tk=&#x27;</span></span><br><span class="line">    third = <span class="string">&#x27;&amp;qzonetoken=&#x27;</span></span><br><span class="line">    fourth = <span class="string">&#x27;&amp;g_tk=&#x27;</span></span><br><span class="line">    haturl= urlOrd + my_qq + second + gtk + third + qzonetoken + fourth + gtk</span><br><span class="line"></span><br><span class="line">    textpage = get_friendhat(haturl, cookie)</span><br><span class="line">    process_save(textpage,save_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_friendhat</span>(<span class="params">url,cookie</span>):</span><span class="comment">#发送请求 获取响应</span></span><br><span class="line">    header= &#123;<span class="string">&#x27;accept-encoding&#x27;</span>: <span class="string">&#x27;gzip, deflate, br&#x27;</span>, <span class="string">&#x27;accept-language&#x27;</span>: <span class="string">&#x27;zh-CN,zh;q=0.8&#x27;</span>, <span class="string">&#x27;host&#x27;</span>: <span class="string">&#x27;h5.qzone.qq.com&#x27;</span>, <span class="string">&#x27;user-agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36&#x27;</span>, <span class="string">&#x27;accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&#x27;</span>, <span class="string">&#x27;connection&#x27;</span>: <span class="string">&#x27;keep-alive&#x27;</span>&#125;</span><br><span class="line">    r = requests.get(url, headers=header, cookies=cookie)</span><br><span class="line">    <span class="keyword">return</span> r.text</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_save</span>(<span class="params">text,save_path</span>):</span><span class="comment">#提取信息并储存</span></span><br><span class="line">    friend=re.findall(<span class="string">&#x27;\&quot;.*?\&quot;\:\&#123;\n\&quot;realname\&quot;\:\&quot;.*?\&quot;\&#125;&#x27;</span>,text)</span><br><span class="line">    frNum = <span class="built_in">len</span>(friend)</span><br><span class="line"></span><br><span class="line">    Name = []</span><br><span class="line">    <span class="keyword">for</span> elem <span class="keyword">in</span> friend:</span><br><span class="line">        n =elem.split(<span class="string">&#x27;&quot;realname&quot;:&quot;&#x27;</span>)[<span class="number">1</span>].split(<span class="string">&#x27;&quot;&#125;&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">        Name.append(n)</span><br><span class="line">    QQ = []</span><br><span class="line">    <span class="keyword">for</span> elem <span class="keyword">in</span> friend:</span><br><span class="line">        q = elem.split(<span class="string">&#x27;&quot;&#x27;</span>)[<span class="number">1</span>]</span><br><span class="line">        QQ.append(q)</span><br><span class="line"></span><br><span class="line">    wb = load_workbook(save_path)</span><br><span class="line">    ws = wb.get_sheet_by_name(<span class="string">&quot;friend_info&quot;</span>)</span><br><span class="line">    wc_name=ws[<span class="string">&#x27;A2&#x27;</span>:<span class="string">&#x27;A&#x27;</span>+<span class="built_in">str</span>(frNum+<span class="number">2</span>)]</span><br><span class="line">    wc_qq=ws[<span class="string">&#x27;B2&#x27;</span>:<span class="string">&#x27;B&#x27;</span>+<span class="built_in">str</span>(frNum+<span class="number">2</span>)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(frNum):</span><br><span class="line">        wc_name[i][<span class="number">0</span>].value = Name[i]</span><br><span class="line">        wc_qq[i][<span class="number">0</span>].value = QQ[i]</span><br><span class="line"></span><br><span class="line">    wb.save(save_path)</span><br></pre></td></tr></table></figure><h3 id="使用正则表达式re库提取信息"><a href="#使用正则表达式re库提取信息" class="headerlink" title="使用正则表达式re库提取信息"></a>使用正则表达式re库提取信息</h3><p>观察响应信息的内容，不难发现它的表达方式是json格式。</p><p>可以直接使用Requests库中内置的JSON解码器<code>r.json()</code></p><blockquote><p>当时写代码的时候…没想起来（￣。。￣），直接暴力的用正则对文本内容进行了正则匹配，不过也达到了提取信息的目的</p></blockquote><p>这里使用的方法是对返回的一个很长很长的字符串的文本信息进行正则匹配，这一块就没什么好说的了，只要对正则的使用比较熟练就很方便。</p><h4 id="正则表达式的常用操作符"><a href="#正则表达式的常用操作符" class="headerlink" title="正则表达式的常用操作符"></a>正则表达式的常用操作符</h4><div class="table-container"><table><thead><tr><th>操作符</th><th>说明</th></tr></thead><tbody><tr><td>.</td><td>表示任何单个字符</td></tr><tr><td>[ ]</td><td>字符集，对单个字符给出取值范围</td></tr><tr><td><sup><a href="#fn_ " id="reffn_ "> </a></sup></td><td>非字符集，对单个字符给出排除范围</td></tr><tr><td>*</td><td>前一个字符出现0次或无限次扩展</td></tr><tr><td>+</td><td>前一个字符出现1次或无限次扩展</td></tr><tr><td>？</td><td>前一个字符0次或1次扩展</td></tr><tr><td>&#124;</td><td>左右表达式任取其一</td></tr><tr><td>{m}</td><td>扩展前一个字符m次</td></tr><tr><td>{m,n}</td><td>扩展签一个字符m至n次（含n）</td></tr><tr><td>^</td><td>匹配字符串开头</td></tr><tr><td>$</td><td>匹配字符串结尾</td></tr><tr><td>()</td><td>分组标记，内部只能使用&#124;操作符</td></tr><tr><td>\d</td><td>数字</td></tr><tr><td>\w</td><td>单词字符</td></tr></tbody></table></div><blockquote><p>要注意正则表达式默认的是<strong>贪婪匹配</strong>,即<strong>趋于最大匹配长度</strong>,使用<code>*?</code>可以实现最小匹配</p><p>另一点需要注意的地方就是转义字符的问题</p></blockquote><h4 id="re库的函数"><a href="#re库的函数" class="headerlink" title="re库的函数"></a>re库的函数</h4><div class="table-container"><table><thead><tr><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>re.search()</td><td>在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象</td></tr><tr><td>re.match()</td><td>从一个字符串的开始位置起匹配正则表达式，返回match对象</td></tr><tr><td>re.findall()</td><td>搜索字符串，一列表类型返回全部能匹配的字符串</td></tr><tr><td>re.split()</td><td>将一个字符串按照正则表达式匹配结果进行分割，返回列表</td></tr><tr><td>re.finditer()</td><td>搜索字符串，返回匹配结果的迭代类型，每个迭代元素是match对象</td></tr><tr><td>re.sub()</td><td>在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串</td></tr></tbody></table></div><blockquote><p>re库本生就有很多好用的函数，当时写代码的时候也没怎么想起来（￣。。￣）</p></blockquote><p>当时在返回信息中并没有找到发说说时刻这个信息，created_time对用的是一串数字，通过我咳咳…细心的观察发现这一串数字是秒数，发说说的具体时间是按照秒数计算的。通过对几条说说的观察算出<code>1514649600</code>是2018年1月1日00:00对应的秒数。当时还有专门去算一下对应的<code>0</code>代表的是哪个时间点，算出来是1970年1月1日08:00，一开始还以为是马化腾生日(+_+)。后来也算是偶然知道了<a href="https://baike.baidu.com/item/unix时间戳/2078227">unix时间戳</a>是从1970年1月1日（UTC/GMT的午夜）开始所经过的秒数，很多语言后来也是这样如java、python。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">h = postTime.split(&#x27;,&quot;created_time&quot;:&#x27;)[1].split(&#x27;,&quot;editMask&quot;&#x27;)[0]</span><br><span class="line">    hour = ( int(h) - 1514649600 ) %86400 /3600#算出一天中发说说的时刻</span><br><span class="line">    ssInfo.append(hour) </span><br></pre></td></tr></table></figure><h3 id="使用openpyxl将信息存入excel文件中、"><a href="#使用openpyxl将信息存入excel文件中、" class="headerlink" title="使用openpyxl将信息存入excel文件中、"></a>使用openpyxl将信息存入excel文件中、</h3><p><a href="https://openpyxl.readthedocs.io/en/stable/">openpyxl</a>提供了很多方便操作.xlsx文件的函数，通过Sheet、cell的操作可以访问、读写单元格。</p><p>本来是想用数据库的，但是并不会数据库，配置文件弄了半天也没弄好，就选择操作excel文件。</p><blockquote><p>存储信息量太大的话最好不要存在一个文件中，加载会变慢</p></blockquote><p>最终爬取了200多位好友2018年的说说，共5000多条。</p><p>储存在excel中的<a href="https://iszffbucket-1258695789.cos.ap-shanghai.myqcloud.com/Blog/QQ_Crawler/resultsSheets.jpg">结果</a>。</p><h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><p>可视化使用了<a href="https://matplotlib.org/">matplotlib</a>。</p><p>对好友空间是否对自己开放情况进行了统计，绘制饼状图<a href="https://iszffbucket-1258695789.cos.ap-shanghai.myqcloud.com/Blog/QQ_Crawler/accessablePie.jpg">accessablePie</a>。</p><p>按照月份对好友们发说说条数绘制柱状图<a href="https://iszffbucket-1258695789.cos.ap-shanghai.myqcloud.com/Blog/QQ_Crawler/monthHist.jpg">monthHist</a>。</p><p>按照小时对好友们发说说条数绘制柱状图<a href="https://iszffbucket-1258695789.cos.ap-shanghai.myqcloud.com/Blog/QQ_Crawler/hourHist.jpg">hourHist</a>。</p><p>本文参照了<a href="https://www.zhihu.com/people/biao.me/activities">标标</a>的专栏文章<a href="https://zhuanlan.zhihu.com/p/38634589">爬取QQ空间</a>。</p><h3 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h3><p>其实代码写得挺烂的，异常处理做的也不好，跑起来遇到了异常才对应的写异常处理。</p><p>另外就是网速要好o(≧口≦)o，不然requests就会报错，校网实在不能打，所以我选择去网吧(≧∇≦)。</p><p>算是一个上学期的一个小总结，也算是这学期的开始吧。</p><p>因为代码是上学期最后写的，这篇总结一直拖到了这学期开学才写，再不写就要上课了Orz。</p><p>希望大三一切都顺利吧，至少能够顺心意。</p><p><script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: 开始op1=>operation: 登陆好友空间说说页面、获取cookieop2=>operation: 构造请求、获取返回内容op3=>operation: 处理返回内容、提取信息e=>endst->op1->op2->op3->e</textarea><textarea id="flowchart-0-options" style="display: none">{"0":{"scale":"1,"},"1":{"line-width":"2,"},"2":{"line-length":"50,"},"3":{"text-margin":"10,"},"4":{"font-size":12},"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script></p>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据处理 </tag>
            
            <tag> python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>新建博客</title>
      <link href="/2018/05/27/%E6%96%B0%E5%BB%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>/2018/05/27/%E6%96%B0%E5%BB%BA%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<h1 id="新建博客"><a href="#新建博客" class="headerlink" title="新建博客"></a>新建博客</h1><blockquote><p>此博客主要用于记录个人成长和学习生活。</p><p>在学习生活中自己常常遇到一些问题，需要总结、记录一下。</p><p>Orz</p></blockquote><h1 id="添加对公式支持"><a href="#添加对公式支持" class="headerlink" title="添加对公式支持"></a>添加对公式支持</h1><p> <a href="https://blog.csdn.net/weixin_45511189/article/details/115798563">hexo博客next主题添加对数学公式的支持_阳光洒在杨树上的博客-CSDN博客_hexo 数学公式</a></p><h1 id="Hexo-NexT主题配置搜索"><a href="#Hexo-NexT主题配置搜索" class="headerlink" title="Hexo NexT主题配置搜索"></a>Hexo NexT主题配置搜索</h1><p><a href="https://www.jianshu.com/p/fdd8e4e36c57">Hexo NexT主题配置搜索功能教程</a></p><p><a href="https://blog.csdn.net/aoman_hao/article/details/86713171">Hexo博客Next主题站内搜索模块相关，解决搜索无效、一直loading的问题</a></p><p><a href="https://www.sqlsec.com/2017/12/hexosearch.html">Hexo 博客无法搜索的终极解决方法 | 国光 (sqlsec.com)</a></p><p><a href="https://blog.csdn.net/weixin_45877759/article/details/107141789">(21条消息) hexo的next主题添加搜索功能_杂货店的阿猿的博客-CSDN博客</a>此方法无效</p>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> blog </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
