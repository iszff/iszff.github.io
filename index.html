<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>iszff&#39; Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="记录">
<meta property="og:type" content="website">
<meta property="og:title" content="iszff&#39; Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="iszff&#39; Blog">
<meta property="og:description" content="记录">
<meta property="og:locale">
<meta property="article:author" content="iszff">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="iszff&#39; Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">iszff&#39; Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-”风格造成的关键点检测误差“" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/" class="article-date">
  <time datetime="2021-12-31T12:03:53.000Z" itemprop="datePublished">2021-12-31</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/">风格造成的人脸关键点检测误差</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>由于图片拍摄时候外界环境,比如光线变化等会造成图片”风格”的不同,不同风格的同一张人脸经过landmark detector 可能会获得不同的地标,这并不是由于人脸结构本身造成。对于此问题的研究目前有两篇具有代表性的文章,主要思路都是通过数据增强,对图片进行某种风格变换,再去训练landmark detector,使得检测器对于风格变化具有鲁棒性。从而提高总体landmark检测准确度。</p>
<h2 id="2018CVPR-Style-Aggregated-Network-for-Facial-Landmark-Detection"><a href="#2018CVPR-Style-Aggregated-Network-for-Facial-Landmark-Detection" class="headerlink" title="[2018CVPR]Style Aggregated Network for Facial Landmark Detection"></a>[2018CVPR]Style Aggregated Network for Facial Landmark Detection</h2><blockquote>
<p>Xuanyi Dong1, Yan Yan1, Wanli Ouyang2, Yi Yang1∗<br>1University of Technology Sydney,2The University of Sydney<br>{xuanyi.dong,yan.yan-3}@student.uts.edu.au;<br>wanli.ouyang@sydney.edu.au; yi.yang@uts.edu.au</p>
</blockquote>
<ol>
<li><p>评价：提出了一种对图像风格差异不敏感的风格聚合网络(style - aggregation Network, SAN)人脸地标检测方法。在face landmark detection领域第一个明确了图像风格变化问题造成可能会带来检测失误。</p>
</li>
<li><p>针对问题：不同图像风格差异较大的问题。除了人脸本身的方差外，图像风格的内在方差，如灰度与彩色图像、亮与暗、强烈与暗淡等。landmark检测器对于不同风格的同一张人脸图片有不同的输出。例：三张图片内容完全相同。唯一不同的是形象风格。使用训练好的面部标志检测器来定位面部标志。zoom部分显示不同风格图像上相同面部标志的预测位置之间的偏差。</p>
<p><img src="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211229210439162.png" alt="image-20211229210439162"></p>
</li>
<li><p>本文的目的：提高对图像风格的大方差的鲁棒性。</p>
</li>
<li><p>实现的方法：</p>
<p><img src="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211229212141043.png" alt="image-20211229212141043"></p>
</li>
</ol>
<p>SAN架构概述。网络由两部分组成。第一个是样式聚合的脸生成模块，该模块将输入图像转换成不同的样式，然后将它们组合成样式聚合的脸。</p>
<p>二是人脸地标预测模块。该模块将原始图像和样式聚合图像作为输入，得到两个互补的特征，然后将两个特征进行融合，级联生成热图预测。“fc”意味着fully-convolution。</p>
<p>5.方法简介</p>
<p>数据增强（风格变化）：通过将300-W和AFLW转换成不同的风格，发布了两个新的人脸标志物检测数据集:300W-Styles(≈12000 images)和AFLW- styles(≈80000 images)。</p>
<p>通过生成式对抗模块将原始人脸图像转换为风格聚合图像，使用风格聚合图像使得人脸图像对环境变化更具鲁棒性。</p>
<p>风格互补训练：将原始人脸图像与风格聚合的人脸图像作为双流输入landmark 检测器，二流输入可以互补。</p>
<blockquote>
<p>假如测试阶段移除gan呢，相当于多模态训练，单模态测试？</p>
</blockquote>
<p>在基准数据集AFLW方法表现良好。代码可在GitHub上公开获取:<a target="_blank" rel="noopener" href="https://github.com/D-X-Y/SAN">https://github.com/D-X-Y/SAN</a></p>
<p>6.个人的思考</p>
<p>相当于做了数据增强，增加了数据量。</p>
<h2 id="2019ICCV-Aggregation-via-Separation-Boosting-Facial-Landmark-Detector-with-Semi-Supervised-Style-Translation"><a href="#2019ICCV-Aggregation-via-Separation-Boosting-Facial-Landmark-Detector-with-Semi-Supervised-Style-Translation" class="headerlink" title="[2019ICCV]Aggregation via Separation: Boosting Facial Landmark Detector with Semi-Supervised Style Translation"></a>[2019ICCV]Aggregation via Separation: Boosting Facial Landmark Detector with Semi-Supervised Style Translation</h2><blockquote>
<p>Shengju Qian1, Keqiang Sun2, Wayne Wu2,3, Chen Qian3, Jiaya Jia1,4<br>1The Chinese University of Hong Kong2Tsinghua University<br>3SenseTime Research4Y ouTu Lab, Tencent<br>{sjqian, leojia}@cse.cuhk.edu.hk, skq17@mails.tsinghua.edu.cn,{wuwenyan, qianchen}@sensetime.com</p>
</blockquote>
<ol>
<li><p>评价：</p>
</li>
<li><p>针对问题：鉴于任何人脸图像都可以被分解成光线、纹理和图像环境的风格空间，以及一个风格不变的结构空间，本文的关键想法是利用每个个体的风格和形状空间。[2018CVPR]Style Aggregated Network for Facial Landmark Detection中显式地研究了图像风格带来的畸变现象。</p>
</li>
<li><p>思路：</p>
<p>在实践中，图像内容是指对象、语义和边缘特征，而风格可以是颜色和纹理。</p>
<p>基于人脸地标检测的目的，即通过过滤不受约束的“风格”，回归“人脸内容”，即人脸几何的主成分。定义“style”是指的是图像背景、光线、质量、是否存在眼镜等阻碍探测器识别人脸几何形状的因素。</p>
</li>
<li><p>实现的方法：</p>
<p>利用风格迁移和解纠缠表征学习disentangled representation learning来处理人脸对齐问题，因为风格迁移的目的是在保留内容的同时改变风格。在不使用额外知识的情况下，增加人脸地标检测的训练。</p>
</li>
<li><p>方法简介：</p>
<p>a. 不是直接生成图像来做数据增强，而是首先将人脸图像映射到结构和风格的空间中。</p>
<p>b. 为了保证这两个空间的解纠缠，设计了一个条件变分自编码器模型，该模型将Kullback-Leiber (KL)散度损失和skip连接分别用于风格和结构的紧凑表示。通过分解这些特征，在现有的面部几何图形之间执行视觉风格转换。根据现有的人脸结构，将戴眼镜、质量较差、在模糊或强光下的人脸进行相应的风格，用于进一步训练人脸地标探测器，形成一个较为通用和健壮的人脸几何识别系统。</p>
</li>
</ol>
<p><img src="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211231173635145.png" alt="image-20211231173635145"></p>
<p>框架由两部分组成。一个是学习面部外观和结构的解离化表示，而另一个可以是任何面部标志检测器。</p>
<p>在第一阶段，提出了条件变化自动编码器，用于学习风格和结构之间的分离表示。</p>
<p>在第二阶段，在从其他人脸转换到风格后，带有结构的“风格化”图像可用于提高训练性能和风格不变检测器。</p>
<ol>
<li><p>个人的思考</p>
<p>与第一篇文章中类似，相当于做了数据增强，增加了数据量。不同的是不是显示地改变光线或是rgb图转换成灰度，而是隐式地对于风格特征和结构特征进行了分离，在把一张图片风格迁移到其他风格。</p>
</li>
<li><p>实验效果</p>
<p><img src="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/coding\stylealign结果\test_00075000.png" alt></p>
</li>
</ol>
<p>   <img src="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/coding\stylealign结果\test_01000000.png" alt></p>
<p>   上面两张图 给出了在batch_size=8的情况下，第75个epoch和1000000个 epoch数据增强 的效果。</p>
<p>   最上面第一行是第一列各个图像地关键点，也就是人脸地结构特征。在第二行到第九行中，第一列为原始图像，之后地第二列到第九列为各个原始图像保持原始landmark结构的情况下，生成的具有风格迁移地人脸。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/" data-id="ckxv6sclq00006oup4vsf2wiw" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag">文献阅读</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-人脸关键点检测论文梳理" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/12/28/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/" class="article-date">
  <time datetime="2021-12-28T12:26:26.000Z" itemprop="datePublished">2021-12-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/12/28/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/">人脸关键点检测论文梳理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="常用数据集"><a href="#常用数据集" class="headerlink" title="常用数据集"></a>常用数据集</h1><p><img src="/2021/12/28/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211228205403494.png" alt="image-20211228205403494"></p>
<h1 id="3FabRec-Fast-Few-shot-Face-alignment-by-Reconstruction"><a href="#3FabRec-Fast-Few-shot-Face-alignment-by-Reconstruction" class="headerlink" title="3FabRec: Fast Few-shot Face alignment by Reconstruction"></a>3FabRec: Fast Few-shot Face alignment by Reconstruction</h1><blockquote>
<p>2020CVPR</p>
<p>Bjorn Browatzki and Christian Wallraven* ¨ Dept. of Artificial Intelligence, Korea University, Seoul browatbn@korea.ac.kr, wallraven@korea.ac.kr</p>
</blockquote>
<p>•<strong>人脸关键点定位的**</strong>监督学习<strong><strong>方法需要</strong></strong>大量训练数据<strong><strong>，在特定数据集上由于参数量多会</strong></strong>过拟合**</p>
<p>•<strong>此文**</strong>使用半监督学习方法<strong><strong>在于首先从大量的</strong></strong>无标签图像<strong><strong>要生成</strong></strong>隐藏的人脸知识** </p>
<p>••two-stage architecture 3FabRec达到sota的表现</p>
<p>•在极小训练集（10张图片）可以保持同样的精度</p>
<p>•插入迁移层在解码器上增加少量参数，推断过程在GPU上可以达到数百FPS</p>
<p>•精确关键的人脸定位对于之后的人脸处理很重要。挑战在于克服各种模糊、遮挡、视觉不可见，实现低定位错误，对于人脸变化由高鲁棒性，保证高定位精度。</p>
<p>•多数方法都使用了高度协调的有监督的学习方案，并且几乎总是在要测试的特定数据集上进行特殊优化，从而增加了对该数据集进行过度拟合的可能性。</p>
<p>•不同数据集中的注释可能是不精确且不一致的。</p>
<p>用已经存在的标定好的人脸数据集和其他任务的人脸数据集，学习潜在人脸形状知识可以使得跨数据集间更好的泛化能力.</p>
<h1 id="LUVLi-Face-Alignment-Estimating-Landmarks’-Location-Uncertainty-and-Visibility-Likelihood"><a href="#LUVLi-Face-Alignment-Estimating-Landmarks’-Location-Uncertainty-and-Visibility-Likelihood" class="headerlink" title="LUVLi Face Alignment: Estimating Landmarks’ Location, Uncertainty, and Visibility Likelihood"></a>LUVLi Face Alignment: Estimating Landmarks’ Location, Uncertainty, and Visibility Likelihood</h1><blockquote>
<p>2020 CVPR</p>
<p>Abhinav Kumar∗,1 , Tim K. Marks∗,2 , Wenxuan Mou∗,3 , Ye Wang2 , Michael Jones2 , Anoop Cherian2 , Toshiaki Koike-Akino2 , Xiaoming Liu4 , Chen Feng5 abhinav3663@gmail.com, tmarks@merl.com, wenxuanmou@gmail.com, [ywang, mjones, cherian, koike]@merl.com, liuxm@cse.msu.edu, cfeng@nyu.edu 1University of Utah, 2Mitsubishi Electric Research Labs (MERL), 3University of Manchester, 4Michigan State University, 5New York University</p>
</blockquote>
<p>•现代人脸对齐方法在预测面部标志的位置方面已经非常准确，但通常不会估算其预测位置的不确定性，也无法预测标志是否可见。</p>
<p>•本文提出了一种用深度网络联合预测标志位置、不确定度、标值可见性的新颖框架。</p>
<p>•把以上三点用混合随机变量建模，用提出的Location,Uncertainty, and Visibility Likelihood (LUVLi) loss来训练</p>
<p>•用了全新带标签的68点人脸数据集，数据中还包含了每个标记是否没有被遮挡、是否出现了自遮挡、外部遮挡。</p>
<p>•联合评估的方法产生了对于预测标定点不确定度的精确估计，并且在多个标准人脸配准数据集上达到了SOTA</p>
<p>•对于标定点不确定的估计可以自动合格判定人脸配准是否失败</p>
<p><strong>贡献</strong></p>
<p>•这是引入用于面对齐的参数不确定性估计概念的第一项工作。</p>
<p>•提出了一个端到端可训练模型，用于联合估计地标位置，不确定性和可见性可能性（LUVLi），建模为混合随机变量。</p>
<p>•使用多元高斯和多元拉普拉斯概率分布比较我们的模型。</p>
<p>•算法在多个面部对齐数据集上产生准确的不确定性估计和最新的地标定位结果。</p>
<p>•发布一个新的数据集，其中将以手动方式标记各种姿势中超过19; 000张面部图像上68个地标的位置，其中每个地标也被标记为三种可见性类别之一。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/12/28/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/" data-id="ckxv6sclr00016oup79cmcuo3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag">文献阅读</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-EyediapRGBDgaze" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/12/10/EyediapRGBDgaze/" class="article-date">
  <time datetime="2021-12-10T07:07:56.000Z" itemprop="datePublished">2021-12-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/12/10/EyediapRGBDgaze/">EyediapRGBDgaze</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>Funes Mora K A, Monay F, Odobez J M. Eyediap: A database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras[C]//Proceedings of the Symposium on Eye Tracking Research and Applications. 2014: 255-258.</p>
</blockquote>
<ol>
<li><p>评价：用于开发和评估来自RGB和RGB- d相机的注视估计算法的数据库</p>
</li>
<li><p>针对的问题：缺乏一个共同的基准来评估从RGB和RGB- d数据的凝视估计任务。</p>
</li>
<li><p>本文的目的：引入一个新的数据库和一个共同的框架来克服缺乏一个共同的benchmark这一局限性，用于注视估计方法的训练和评价。可以评估算法鲁棒性 。</p>
<p>i)头部姿势变化;</p>
<p>ii)人变化;</p>
<p>iii)环境和感知条件的变化和</p>
<p>iv)目标类型:屏幕或3D对象</p>
</li>
</ol>
<h1 id="数据集采集设置"><a href="#数据集采集设置" class="headerlink" title="数据集采集设置"></a>数据集采集设置</h1><h2 id="Set-up"><a href="#Set-up" class="headerlink" title="Set-up"></a>Set-up</h2><p><img src="/2021/12/10/EyediapRGBDgaze/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211210155014440.png" alt="image-20211210155014440"></p>
<p>Kinect:这种消费设备提供标准(RGB)和VGA分辨率(640×480)和30fps的深度视频流。</p>
<p>高清摄像头:Kinect设计的视场更大，对用户移动性的限制更少，但这对于基于VGA分辨率的眼球追踪来说是个问题。因此，我们也使用了全高清相机(1920x1080)， 25fps记录场景。</p>
<p>led:两个摄像头都可见的5个led用于同步RGB-D和HD流。</p>
<p>平面屏幕:使用一个24英寸的屏幕来显示一个可视目标。</p>
<p>小球:使用直径为4cm的小球作为视觉目标，有两个目的:在3D环境中作为视觉目标，在RGB和深度数据中都具有识别力，从而可以精确跟踪其3D位置(见第3节)。</p>
<p>如图1所示，摄像头位于电脑屏幕正下方，这样可以从下方观察参与者的眼睛，尽量减少眼皮遮挡。参与者被要求在一定距离内坐在装置前面，这取决于视觉目标的类型(见下一段)，并凝视指定的视觉目标。没有在说话活动、面部表情等方面给出指示。</p>
<h2 id="Recording-sessions"><a href="#Recording-sessions" class="headerlink" title="Recording sessions"></a>Recording sessions</h2><p>为了评估注视估计算法的不同方面，我们设计了一组记录会话，每个会话的特征是四个主要变量的组合，影响注视估计的准确性:视觉目标，头部姿势，参与者和记录条件。这些措施说明如下:</p>
<h3 id="Discrete-screen-target"><a href="#Discrete-screen-target" class="headerlink" title="Discrete screen target"></a>Discrete screen target</h3><p>离散目标(DS)屏幕上,一个小圆匀画每1.1秒随机位置在计算机屏幕上,</p>
<h3 id="Continuous-screen-target"><a href="#Continuous-screen-target" class="headerlink" title="Continuous screen target"></a>Continuous screen target</h3><p>屏幕连续目标(CS),圆圈沿着一个随机2 s轨道移动,以获得注视运动更平稳的例子。</p>
<h3 id="3D-floating-target"><a href="#3D-floating-target" class="headerlink" title="3D floating target"></a>3D floating target</h3><p>3D浮动目标(FT):一个直径4厘米的球挂在一根细线上，这根细线附着在一根棍子上，在摄像机和参与者之间的3D区域内移动。与屏幕上的目标相比，参与者离相机的距离更大(1.2米而不是80-90厘米)，以便有足够的空间让目标移动。</p>
<h2 id="头部姿势。"><a href="#头部姿势。" class="headerlink" title="头部姿势。"></a>头部姿势。</h2><p>为了评估方法对头部姿势的稳健性，要求参与者保持注视视觉目标，同时</p>
<p>(i)面对屏幕保持近似静止的头部姿势(静态情况，S);</p>
<p>(ii)进行头部运动(平移和旋转translation and rotation)，以引入头部姿势变化(Mobile case, M)。</p>
<h2 id="记录的条件"><a href="#记录的条件" class="headerlink" title="记录的条件"></a>记录的条件</h2><p>对于参与者12、13和14，在两种不同的条件下(记为A或B)，一些进程被记录了两次:不同的日子、光照和与相机间的距离</p>
<blockquote>
<p>总结。记录了94个2 - 3分钟的会话，总共获得了超过4小时的数据。每个会话由字符串“P-C-T-H”表示，它指的是参与者 participan id P=(1-16)，记录条件conditions C=(A或B)，使用的目标target T=(DS, CS或FT)和头部姿势 head pose H=(S或M)。录音示例如图2所示。</p>
</blockquote>
<p><img src="/2021/12/10/EyediapRGBDgaze/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211215204745435.png" alt="image-20211215204745435"></p>
<p>记录数据样本使用:a-c) RGB-D相机;d-e)高清相机，原始图像的640×480pixels patch显示用于与VGA分辨率数据进行比较。在这些例子中，参与者注视的是:a,d)头部保持静止的屏幕目标;b)静态头姿的浮动目标;c,e)移动头部时浮动目标。</p>
<h1 id="Data-processing"><a href="#Data-processing" class="headerlink" title="Data processing"></a>Data processing</h1><p>除了原始数据本身，还提供了额外的信息，这些信息对推导地面真相测量至关重要，或者只是对开发数据集和运行实验很有用。如何估计它们的更多细节可以在[Funes Mora et al. 2014]中找到。</p>
<h2 id="RGB-D-sensor-calibration"><a href="#RGB-D-sensor-calibration" class="headerlink" title="RGB-D sensor calibration"></a>RGB-D sensor calibration</h2><p>RGB-D传感器校准。提供了RGB-D立体集成的校准参数，这些参数是使用开源校准工具箱获得的[Herrera C. et al. 2012]。这允许结合RGB-D数据到一个有纹理的3D表面。</p>
<h2 id="RGB-D-to-screen-calibration"><a href="#RGB-D-to-screen-calibration" class="headerlink" title="RGB-D to screen calibration"></a>RGB-D to screen calibration</h2><p>RGB-D屏幕校准。提供摄像机坐标系统(3D)和2D屏幕坐标之间的校准。</p>
<h2 id="RGB-D-and-HD-camera-synchrony-and-calibration"><a href="#RGB-D-and-HD-camera-synchrony-and-calibration" class="headerlink" title="RGB-D and HD camera synchrony and calibration"></a>RGB-D and HD camera synchrony and calibration</h2><p>RGB-D和高清相机同步和校准。由于使用了5个led，高清数据与RGB-D视频流同步[Funes Mora等人，2014]。实现了两摄像机间的标准立体标定。</p>
<h2 id="Head-pose-and-eyes-tracking"><a href="#Head-pose-and-eyes-tracking" class="headerlink" title="Head pose and eyes tracking."></a>Head pose and eyes tracking.</h2><p>对于每个参与者，我们使用[Funes Mora和Odobez 2012]中描述的方法，将一个3D Morphable Model [Paysan et al. 2009]与深度数据拟合，从而创建一个与他/她特定面部形状相对应的3D网格。此外，给定这个模板，我们使用迭代最近点(ICP)算法跟踪3D头部姿势，然后从该算法得到眼球在相机3D空间中的近似位置。</p>
<h2 id="Floating-target-tracking"><a href="#Floating-target-tracking" class="headerlink" title="Floating target tracking"></a>Floating target tracking</h2><p>漂浮目标跟踪。对于使用一个球作为可视目标的记录会话，我们提供了球在每个时间步t的3D中心，使用彩色滤波和ICP拟合计算。</p>
<h2 id="Manual-annotations"><a href="#Manual-annotations" class="headerlink" title="Manual annotations"></a>Manual annotations</h2><p>手工注释。进一步的手工注释，当数据被认为是不可靠的注视估计(和评价)提供。这对应于眨眼的时刻，或者当这个人分心的时候(没有看目标)。手动注释是针对涉及屏幕目标的正面会话进行的。鉴于这些情况的发生率很低，我们目前没有在其他会话中对其进行标记(它将被视为噪音)。不过，如果需要，还可以做进一步的注释。</p>
<h1 id="Considered-tasks"><a href="#Considered-tasks" class="headerlink" title="Considered tasks"></a>Considered tasks</h1><p>文章描述了用于评估注视估计算法的准确性及其对不同变量(如头姿、光照条件等)的鲁棒性的不同评估基准。首先总结评估协议框架的主要元素，包括性能度量，然后列出一组基准。</p>
<h2 id="Evaluation-protocol-and-measures"><a href="#Evaluation-protocol-and-measures" class="headerlink" title="Evaluation protocol and measures"></a>Evaluation protocol and measures</h2><p>本节介绍实验描述中涉及的概念:什么被理解为注视估计算法;训练、测试和评估集的定义 train, test and evaluation sets;性能测量。</p>
<h3 id="Gaze-estimation-algorithm注释估计算法"><a href="#Gaze-estimation-algorithm注释估计算法" class="headerlink" title="Gaze estimation algorithm注释估计算法"></a>Gaze estimation algorithm注释估计算法</h3><p>注视估计的输出取决于应用程序。这里考虑两个非常常见的情况:一个3D gaze ray单位向量;或屏幕坐标系Screen coordinates上的二维像素，这通常用于基于屏幕的HCI应用程序。提供额外信息(摄像机-屏幕校准)，可以从3D凝视射线推断屏幕坐标[Funes Mora等人，2014]。</p>
<p>训练数据集的真值包含3d视线估计p和2d视线估计s，给出一定的参数，p可以从s中计算处理。考虑不同的方法来收集这些训练样本:时间，即数据对应于一个较大的视频的部分;或结构化:训练数据以结构化的方式收集，以满足注视估计算法的特定要求(例如，获取最接近屏幕上预定义数量的特定真值的样本)。</p>
<p>测试集假设它是帧索引范围内较大视频的时间序列。</p>
<p>验证集将用于计算算法性能。这是测试数据的一个子集，通过移除数据或地面真相被破坏的样本，由于眨眼和分心，极端的头部姿势，影响眼睛的可见性(例如，鼻子遮挡)。</p>
<h2 id="Predefined-experimental-protocols"><a href="#Predefined-experimental-protocols" class="headerlink" title="Predefined experimental protocols"></a>Predefined experimental protocols</h2><p>为了在不同的实验条件下比较算法和它们的优点，定义了一组协议，它们的区别主要在于用于训练和测试算法的数据库。注意，该数据集有两种主要类型的视觉目标:3D浮动目标(FT)和屏幕目标(CS或DS)。因此，定义的评价方案可以根据首选的视觉目标而变化。总结如下。更多细节见[Funes Mora et al. 2014]。</p>
<p>Protocol 1:注视估计的准确性。在本方案中，我们评估了一个算法$H$的准确性，在所有参数的最小变化下，不注视变化。更准确地说，对于一个session s，其中唯一的变化是注视本身，我们将训练集定义为s的前半部分。测试集定义为s的后半部分。这种实验的结果是平均角误差$\epsilon$◦。可以根据视觉目标的类型导出相关会话。</p>
<p>Protocol 2:头姿变化的鲁棒性。这里的目的是测量由于头部姿势的变化，凝视的准确性下降了多少。首先进行静态头姿(S)实验，然后进行头姿变化(M)实验，然后报告两种情况下的平均误差以及算法对头姿变化的敏感性。</p>
<p>Protocol 3:个人依赖。目的是评估一种方法推广到不可见用户的效果。这可以通过 leave-one-person-out留一法实验装置来实现。</p>
<p>Protocol 4:环境变化。最后，本案例的目标是研究一种方法在不同条件下的泛化特性。为此，可以对参与者12、13和14进行实验，在不同的设置和光照条件下记录会话。</p>
<h1 id="Evaluation-protocol-example"><a href="#Evaluation-protocol-example" class="headerlink" title="Evaluation protocol example"></a>Evaluation protocol example</h1><p>为了说明数据集的用法，这里我们详细描述了与其中一个基准相关的数据:3D浮动目标的注视估计精度协议(Protocol 1)，并使用RGBD流作为注视估计算法的输入数据。注意，这种配置是数据集中最具挑战性的情况之一，对于这种情况，典型的眼睛图像大小为≈14×10pixels。</p>
<h2 id="头部姿势和3D目标跟踪"><a href="#头部姿势和3D目标跟踪" class="headerlink" title="头部姿势和3D目标跟踪"></a>头部姿势和3D目标跟踪</h2><p>在表1中，我们显示了每个会话对应的帧总数，以及我们能够成功估计头部姿势或视觉目标位置的帧数。注意，头部姿势的召回率很高(99.9%)，因为在本方案中，涉及近额和静态头部姿势的人。由于目标在相机的视野之外，或者太靠近传感器导致缺失深度数据，可视目标位置的召回率较低(69%)。尽管如此，这些数字表明有大量的凝视标记数据可供实验使用(每次记录大约100秒)。</p>
<p><img src="/2021/12/10/EyediapRGBDgaze/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211217152023265.png" alt="image-20211217152023265"></p>
<p>表1:第1 - 3行:记录的视频帧数(Total)，头部姿态或浮动目标位置估计成功。第四到第六行:Protocol 1的训练、测试和评估集的大小。</p>
<p>表2:协议1的注视角度误差比较。“头”的情况下，只使用估计的头部方向作为凝视预测。</p>
<p>Protocol sets每个会话被平均分为两个时间区域:前半部分是训练集，后半部分是测试集。测试集被筛选以定义评估集(下一节将描述标准)。表1显示了每组样本的数量(仅考虑已知头姿和目标位置的样本)</p>
<p>目光估计方法。我们实现了一种基于RGB-D的方法[Funes Mora和Odobez 2012]，该方法依靠RGB-D数据将眼睛图像的视点矫正为典型的头部姿势。我们将这种方法称为位校正自适应线性回归(PRALR)。对每个参与者，从训练集中提取42个样本的注视外观模型。这些样本有规律地分布在注视偏航值之间的±40◦和注视仰角值之间的±30◦。由于PR-ALR是一种基于插值的方法，我们只考虑凝视外观模型中使用的数据的凸壳内的测试样本，即评价集仅由地面真值测量遵循相同的偏航和仰角凝视标准的测试样本组成。</p>
<p>估计精度。为了证明数据的可变性，我们计算了在假设参与者注视前方时获得的注视角度误差，即假设注视方向是由头部姿势方向给出的。实验结果如表2所示。注意，这两种方法都输出3D凝视光线，我们使用相同的评估集，因此这些结果可以直接进行比较。“头部”案例显示的角度误差提供了数据中凝视的大变异性的证据。采用PR-ALR注视估计算法后，注视估计的精度得到了显著提高。</p>
<p>然而，与文献报道的结果相比，误差仍然很高，这主要是由于低分辨率(~ 14×10每只眼睛像素)和低对比度(例如参与者7-A是黑色皮肤)。此外，异常值(眨眼、分心等)还没有从评估集3中剔除。注意，这是我们的数据中最具挑战性的场景之一，这个实验足以演示如何使用数据、定义的协议以及如何描述一个实验。</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们描述了一个新的数据集，用于开发和评估来自RGB或RGB- d数据的注视估计算法，解决了社区对标准化基准的需求。</p>
<p>数据库丰富多样，因为它代表了这项任务的主要挑战。大多数变量(头部姿势，人，条件和目标类型)已经被系统地隔离，目标是恰当地描述注视估计在准确性和稳健性方面的不利条件。</p>
<p>描述了记录方法和所记录数据的摘要。我们还列出了提供给用户的额外信息，如设置校准，目标位置，头部和眼睛跟踪信息。我们更详细地描述了一个作为使用实例的实验。我们相信该数据库对研究人员具有很高的价值，因为它将有助于促进注视估计技术在较少约束条件下的发展。</p>
<h1 id="数据集组织结构"><a href="#数据集组织结构" class="headerlink" title="数据集组织结构"></a>数据集组织结构</h1><p><img src="/2021/12/10/EyediapRGBDgaze/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211217165829214.png" alt="image-20211217165829214"></p>
<h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>Data下文件夹命名格式为“P-C-T-H”，指的是参与者 participan id P=(1-16)，记录条件conditions C=(A或B)，使用的目标target T=(DS, CS或FT)和头部姿势 head pose H=(S或M)。</p>
<p><img src="/2021/12/10/EyediapRGBDgaze/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211230120945151.png" alt="image-20211230120945151"></p>
<p><img src="/2021/12/10/EyediapRGBDgaze/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211217180544505.png" alt="image-20211217180544505"></p>
<p>每个文件夹下包含三个视频，3个视频采集设备的参数文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">rgb_vga_calibration.txt</span><br><span class="line"></span><br><span class="line">[resolution]</span><br><span class="line">640;480</span><br><span class="line">[intrinsics]</span><br><span class="line">522.335571;0.000000;323.681061</span><br><span class="line">0.000000;522.346497;269.110931</span><br><span class="line">0.000000;0.000000;1.000000</span><br><span class="line">[R]</span><br><span class="line">1.000000;-0.000000;-0.000000</span><br><span class="line">-0.000000;-1.000000;-0.000000</span><br><span class="line">-0.000000;-0.000000;-1.000000</span><br><span class="line">[T]</span><br><span class="line">0.000000</span><br><span class="line">0.000000</span><br><span class="line">1.000000</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">rgb_hd_calibration.txt</span><br><span class="line"></span><br><span class="line">[resolution]</span><br><span class="line">1920;1080</span><br><span class="line">[intrinsics]</span><br><span class="line">1485.921875;0.000000;940.767761</span><br><span class="line">0.000000;1497.773804;530.065613</span><br><span class="line">0.000000;0.000000;1.000000</span><br><span class="line">[R]</span><br><span class="line">0.946361;-0.081335;-0.312705</span><br><span class="line">-0.077775;-0.996685;0.023863</span><br><span class="line">-0.313610;0.001738;-0.949550</span><br><span class="line">[T]</span><br><span class="line">0.218694</span><br><span class="line">-0.016453</span><br><span class="line">1.015923</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">depth_calibration.txt</span><br><span class="line"></span><br><span class="line">[resolution]</span><br><span class="line">640;480</span><br><span class="line">[intrinsics]</span><br><span class="line">588.194946;0.000000;310.267731</span><br><span class="line">0.000000;585.303101;248.230743</span><br><span class="line">0.000000;0.000000;1.000000</span><br><span class="line">[R]</span><br><span class="line">0.999919;-0.003616;-0.012239</span><br><span class="line">-0.003528;-0.999968;0.007239</span><br><span class="line">-0.012265;-0.007195;-0.999899</span><br><span class="line">[T]</span><br><span class="line">-0.028705</span><br><span class="line">-0.001298</span><br><span class="line">1.002781</span><br><span class="line">[k_coefficients]</span><br><span class="line">3.1199302673;-0.0028519365</span><br><span class="line">[alpha]</span><br><span class="line">1.1090897322;0.0013763016</span><br><span class="line">[beta] #该项目后面的参数跟的很多很多</span><br><span class="line">0.0000000000;0.0000000000;0.0000000000;0.0000000000;0.0000000000;0.0000000000;0.0000000000;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">screen_coordinates.txt</span><br><span class="line"></span><br><span class="line">Screen target coordinates: 2D coordinates (x,y); 3D coordinates (x,y,z)</span><br><span class="line">0;413.0000;477.0000;0.1165;-0.1441;-0.0632</span><br><span class="line">1;416.0000;474.0000;0.1154;-0.1451;-0.0630</span><br><span class="line">2;419.0000;472.0000;0.1143;-0.1457;-0.0629</span><br><span class="line">3;422.0000;469.0000;0.1132;-0.1467;-0.0627</span><br><span class="line">4;425.0000;467.0000;0.1121;-0.1473;-0.0626</span><br><span class="line">5;429.0000;465.0000;0.1107;-0.1480;-0.0625</span><br><span class="line">6;433.0000;463.0000;0.1092;-0.1486;-0.0625</span><br><span class="line">...</span><br><span class="line">#用于记录目标的坐标</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">head_pose.txt</span><br><span class="line"></span><br><span class="line">Head pose: (Rotation matrix (9 values, row by row);Translation(3 values, x,y,z))</span><br><span class="line">0;0.9997;0.0236;-0.0082;-0.0231;0.9980;0.0588;0.0096;-0.0586;0.9982;-0.0046;0.1084;0.3938</span><br><span class="line">1;0.9997;0.0252;-0.0050;-0.0249;0.9980;0.0578;0.0064;-0.0577;0.9983;-0.0047;0.1090;0.3943</span><br><span class="line">2;0.9995;0.0316;-0.0085;-0.0312;0.9988;0.0387;0.0097;-0.0384;0.9992;-0.0049;0.1102;0.3943</span><br><span class="line">3;0.9996;0.0250;-0.0150;-0.0244;0.9987;0.0441;0.0161;-0.0438;0.9989;-0.0049;0.1091;0.3948</span><br><span class="line">4;0.9997;0.0249;-0.0019;-0.0248;0.9985;0.0485;0.0032;-0.0484;0.9988;-0.0049;0.1090;0.3939</span><br><span class="line">5;0.9998;0.0176;0.0042;-0.0178;0.9991;0.0388;-0.0035;-0.0388;0.9992;-0.0046;0.1103;0.3919</span><br><span class="line">6;0.9997;0.0251;0.0051;-0.0253;0.9990;0.0362;-0.0042;-0.0363;0.9993;-0.0046;0.1107;0.3887</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">eye_tracking.txt</span><br><span class="line"></span><br><span class="line">Eyes tracking: Kinect RGB eyeball center left and right (x_l,y_l,x_r,y_r); Kinect depth eyeball center left and right (x_l,y_l,x_r,y_r); HD camera eyeball center left and right (x_l,y_l,x_r,y_r); 3D position of the eyeballs, left and right (x_l, y_l, z_l, x_r, y_r, z_r)</span><br><span class="line">0;286.1482;120.0683;351.1937;121.9867;306.9435;85.2708;379.7931;86.8933;725.6400;164.9603;876.9300;143.2760;-0.0376;0.1493;0.4766;0.0275;0.1472;0.4773</span><br><span class="line">1;286.3311;119.3493;351.4520;121.4472;307.1886;84.4702;380.1107;86.2908;725.2724;163.1687;876.9486;141.8322;-0.0374;0.1499;0.4773;0.0278;0.1476;0.4778</span><br><span class="line">2;285.9384;119.3542;351.1345;121.8086;306.7900;84.4777;379.8091;86.6937;723.7910;163.4062;875.4026;142.9983;-0.0377;0.1497;0.4780;0.0274;0.1470;0.4786</span><br><span class="line">3;285.1989;120.1658;350.4309;122.0558;305.9690;85.3842;379.0494;86.9733;722.2474;165.7260;873.3084;143.9852;-0.0385;0.1488;0.4780;0.0267;0.1467;0.4791</span><br><span class="line">4;286.2832;120.0966;351.3980;122.2475;307.1400;85.2999;380.0429;87.1811;725.2728;165.0547;877.1600;143.8994;-0.0374;0.1491;0.4774;0.0277;0.1469;0.4776</span><br><span class="line">5;287.0309;120.2445;351.9596;122.0580;307.8890;85.4603;380.5585;86.9690;728.2791;164.9782;880.3108;142.9601;-0.0368;0.1494;0.4759;0.0284;0.1476;0.4757</span><br><span class="line">6;287.5150;120.6915;352.0477;122.9920;308.2452;85.9573;380.4743;88.0112;732.2335;165.5833;883.7582;144.9835;-0.0365;0.1498;0.4729;0.0286;0.1475;0.4726</span><br></pre></td></tr></table></figure>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2><h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><p>1.depth数据的处理，depth相机参数的使用。</p>
<p>2.depth相机参数中的k_coefficients、alpha、beta是啥？</p>
<h1 id="数据预处理程序坐标转化部分"><a href="#数据预处理程序坐标转化部分" class="headerlink" title="数据预处理程序坐标转化部分"></a>数据预处理程序坐标转化部分</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">head_rot = head[<span class="number">1</span>:<span class="number">10</span>]</span><br><span class="line"><span class="comment"># Head pose: (Rotation matrix (9 values, row by row)</span></span><br><span class="line">head_rot = np.array(head_rot).reshape([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line"><span class="comment"># reshape</span></span><br><span class="line">head1 = cv2.Rodrigues(head_rot)[<span class="number">0</span>].T[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#此处的T[0]不知道是啥</span></span><br><span class="line">head2d = dpc.HeadTo2d(head1)</span><br><span class="line"><span class="comment">#HeadTo2d 计算Rodrigues，返回航偏角和俯仰角</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(head2d, end=<span class="string">&quot;------&quot;</span>)</span><br><span class="line"><span class="comment">#输出航偏角和俯仰角</span></span><br><span class="line"></span><br><span class="line">head_rot = np.dot(cam_rot, head_rot) </span><br><span class="line"><span class="comment">#相机旋转X头部旋转,基于任意坐标系为参考的成像几何https://www.zhihu.com/people/zhang-xiao-he-8-15,相当于选择世界坐标系</span></span><br><span class="line">head1 = cv2.Rodrigues(head_rot)[<span class="number">0</span>].T[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#此处计算将前面的计算值覆盖掉了</span></span><br><span class="line">head2d = dpc.HeadTo2d(head1)</span><br><span class="line"><span class="comment">#HeadTo2d 计算Rodrigues，返回航偏角和俯仰角</span></span><br><span class="line"><span class="built_in">print</span>(head2d, end=<span class="string">&quot;------&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># rotate the head into camera coordinate system</span></span><br><span class="line">head_trans = np.array(head[<span class="number">10</span>:<span class="number">13</span>])*<span class="number">1000</span></span><br><span class="line"></span><br><span class="line">head_trans = np.dot(cam_rot, head_trans)</span><br><span class="line">head_trans = head_trans + cam_trans</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the 3d coordinates of origin.</span></span><br><span class="line">anno = anno_info[index]</span><br><span class="line">anno = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">eval</span>, anno.strip().split(<span class="string">&quot;;&quot;</span>)))</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(anno) != <span class="number">19</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[Error anno]&quot;</span>)</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line">anno = np.array(anno)</span><br><span class="line"></span><br><span class="line">left3d = anno[<span class="number">13</span>:<span class="number">16</span>]*<span class="number">1000</span></span><br><span class="line">left3d = np.dot(cam_rot, left3d) + cam_trans</span><br><span class="line">right3d = anno[<span class="number">16</span>:<span class="number">19</span>]*<span class="number">1000</span></span><br><span class="line">right3d = np.dot(cam_rot, right3d) + cam_trans</span><br><span class="line"></span><br><span class="line">face3d = (left3d + right3d)/<span class="number">2</span></span><br><span class="line">face3d = (face3d + head_trans)/<span class="number">2</span></span><br><span class="line"></span><br><span class="line">left2d = anno[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">right2d = anno[<span class="number">3</span>:<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the 3d coordinates of target</span></span><br><span class="line">target = target_info[index]</span><br><span class="line">target = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">eval</span>,target.strip().split(<span class="string">&quot;;&quot;</span>)))</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(target) != <span class="number">6</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[Error target]&quot;</span>)</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line">target3d = np.array(target)[<span class="number">3</span>:<span class="number">6</span>]*<span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># target3d = target3d.T - cam_trans</span></span><br><span class="line"><span class="comment"># target3d = np.dot(np.linalg.inv(cam_rot), target3d)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize the left eye image</span></span><br><span class="line">norm = dpc.norm(center = face3d,</span><br><span class="line">                gazetarget = target3d,</span><br><span class="line">                headrotvec = head_rot,</span><br><span class="line">                imsize = (<span class="number">224</span>, <span class="number">224</span>),</span><br><span class="line">                camparams = camera)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Acquire essential info</span></span><br><span class="line">im_face = norm.GetImage(frame)</span><br><span class="line">gaze = norm.GetGaze(scale=scale)</span><br><span class="line">head = norm.GetHeadRot(vector=<span class="literal">False</span>)</span><br><span class="line">head = cv2.Rodrigues(head)[<span class="number">0</span>].T[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">origin = norm.GetCoordinate(face3d)</span><br><span class="line">rvec, svec = norm.GetParams()</span><br><span class="line"></span><br><span class="line">gaze2d = dpc.GazeTo2d(gaze)</span><br><span class="line">head2d = dpc.HeadTo2d(head)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Crop Eye Image</span></span><br><span class="line">left2d = norm.GetNewPos(left2d)</span><br><span class="line">right2d = norm.GetNewPos(right2d)</span><br><span class="line"><span class="comment">#   im_face = ipt.circle(im_face, left2d, 2)</span></span><br><span class="line"><span class="comment">#im_face = ipt.circle(im_face, [left2d[0]+10, left2d[1]], 2)</span></span><br><span class="line"><span class="comment"># cv2.imwrite(&quot;eye.jpg&quot;, im_face)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">im_left = norm.CropEyeWithCenter(left2d)</span><br><span class="line">im_left = dpc.EqualizeHist(im_left)</span><br><span class="line">im_right = norm.CropEyeWithCenter(right2d)</span><br><span class="line">im_right = dpc.EqualizeHist(im_right)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/12/10/EyediapRGBDgaze/" data-id="ckx01sfd500003wupdq7o5gxf" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/" rel="tag">实验记录</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-web3-0" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/11/19/web3-0/" class="article-date">
  <time datetime="2021-11-19T12:06:24.000Z" itemprop="datePublished">2021-11-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/11/19/web3-0/">web3.0</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>1.什么是web3.0？</p>
<p>web1.0类似于博客，有专门的人边界、pgc发布信息，用户消费信息。</p>
<p>web2.0类似于博客、微博，用户可以read和write，user generate content。</p>
<p>web3.0不仅仅能write，还可以own。现阶段比如微博网红相比于平台只能拥有价值网络的一小部分。web3.0用户类似于股东。</p>
<p>公司以前创始人所有-&gt;创始人+股东-&gt;创始人+股东+员工持股，相当于一个范式的转变。</p>
<p>web3.0user就是investor。</p>
<blockquote>
<p>追星需要投入时间、感情 和金钱，用爱发电。但最后收益的是经济公司。web3.0更多的是付出的人获得收益。</p>
</blockquote>
<p>用户、创始人、员工。</p>
<blockquote>
<p>初始的用户可能对于一个公司的作用很大，但享受到的收益大。</p>
</blockquote>
<p>基于protocol web3.0公司核心团队拿15%-30%已经很多了。</p>
<p>2.web3.0公司治理结构如何保持稳定？</p>
<p>在相当长一个时间段需要一个领导人强势引导。民主会影响效率。这一点需要摸索迭代。</p>
<p>deo decentralized autonomous organization。</p>
<p>3.什么时候对于web3.0感兴趣？</p>
<p>16、17九四时间，比特币冲入2万刀。</p>
<p>看人才净流入、净流出。</p>
<p>杰克·多西、Fred Wilson。</p>
<p>但巴菲特、芒格的反对意见。</p>
<p>defi、普惠式金融。</p>
<p>4.怎么学习web3.0</p>
<p>如果写到教科书里，那就不用学了？</p>
<p>a.钱这东西到底是啥？我们为什么要投资？为什么要攒钱？</p>
<p>b.虚拟货币的发展?为啥中本聪要提出比特币？eth解决了啥？defi解决了啥？有什么新变化？解决了华尔街解决的哪些问题？defi1？defi2。</p>
<blockquote>
<p>了解每个人创新的脉络</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://digitalnative.substack.com/p/chain-reactions-how-creators-web3">https://digitalnative.substack.com/p/chain-reactions-how-creators-web3</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/11/19/web3-0/" data-id="ckx01sfdb00023wup3k499kpv" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Few-Shot-Adaptive-Gaze-Estimation" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/11/18/Few-Shot-Adaptive-Gaze-Estimation/" class="article-date">
  <time datetime="2021-11-18T03:30:41.000Z" itemprop="datePublished">2021-11-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/11/18/Few-Shot-Adaptive-Gaze-Estimation/">Few-Shot Adaptive Gaze Estimation</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>Seonwook Park12*, Shalini De Mello1*, Pavlo Molchanov1, Umar Iqbal1, Otmar Hilliges2, Jan Kautz1<br>1NVIDIA,2ETH Zürich</p>
<p>2019ICCV</p>
</blockquote>
<p>(1) 评价：少样本自适应注视估计。</p>
<p>(2)针对问题：</p>
<p>a.由于个体解剖学之间的差异，基于大量数据训练出的注视估计网络限制了对于个体注视估计的准确性。</p>
<blockquote>
<p>每个人眼睛的生理结构不同，即使处于同一位置的人盯着同一物体，所获得的视线也可能有差异。眼睛类似于相机，每一双眼睛的内参都不一眼。</p>
</blockquote>
<p>b.过度参数化的神经网络并不适合从少数例子中学习，因为它们会很快过度拟合。</p>
<p>(3) 本文的目的：使用小样本自适应凝视估计网络，再少量校准样本情况下处理个性化的凝视估计。</p>
<p>(4)实现的方法：通过一个解耦的编码-解码器架构，以及使用元学习训练的高度适应性的凝视估计器，获得<strong>旋转（rotation aware）感知的凝视潜在表征</strong>。</p>
<p><img src="/2021/11/18/Few-Shot-Adaptive-Gaze-Estimation/faze.png" alt="image-20211118144150719"></p>
<blockquote>
<p>FAZE framework在给定一组具有地面真实注视方向信息的训练图像的基础上，首先学习一种为注视估计任务量身定制的潜在特征表示。考虑到这些特征，然后学习一个适应性强的注视估计网络adaptable gaze estimation network AdaGEN。使用元学习可以很容易地适应一个强大的个人特定的注视估计网络Person-specific gaze estimation network(PS-GEN)，只需很少的校准数据。</p>
</blockquote>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p><a target="_blank" rel="noopener" href="https://github.com/NVlabs/few_shot_gaze">https://github.com/NVlabs/few_shot_gaze</a></p>
<p>The bash script should be self-explanatory and can be edited to replicate the final FAZE model evaluation procedure, given that hardware requirements are satisfied (8x GPUs, where each are Tesla V100 GPUs with 32GB of memory)</p>
<p>需要显卡资源多Orz</p>
<p>We also provide a realtime demo that runs with live input from a webcam in the <code>demo/</code> folder. Please check the separate <a target="_blank" rel="noopener" href="https://github.com/NVlabs/few_shot_gaze/blob/master/demo/README.md">demo instructions</a> for details of how to setup and run it.、</p>
<p>给了实时demo。</p>
<hr>
<p>\1)    评价：贡献创新点。</p>
<p>\2)    针对问题：啥情况啥场景。</p>
<p>\3)    本文的目的：可以做到啥。</p>
<p>\4)    实现的方法：</p>
<p>\5)    方法简介</p>
<p>\6)    方法优化</p>
<p>\7)    方法总结‘</p>
<p>\8)    文章存在的问题</p>
<p>\9)    个人的思考</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/11/18/Few-Shot-Adaptive-Gaze-Estimation/" data-id="ckw4ebw5t0000ogup5l87ay1e" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag">文献阅读</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Appearance-Based-Gaze-Estimation-Using-Dilated-Convolutions" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/11/08/Appearance-Based-Gaze-Estimation-Using-Dilated-Convolutions/" class="article-date">
  <time datetime="2021-11-08T06:19:15.000Z" itemprop="datePublished">2021-11-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/11/08/Appearance-Based-Gaze-Estimation-Using-Dilated-Convolutions/">Appearance-Based Gaze Estimation Using Dilated-Convolutions</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>2018 AACV</p>
<p>Zhaokang Chen[0000−0003−0237−0358]and Bertram E. Shi[0000−0001−9167−7495]<br>The Hong Kong University of Science and Technology, Hong Kong SAR<br>{zchenbc,eebert}@ust.hk</p>
</blockquote>
<hr>
<p> \1)  评价：贡献创新点。</p>
<p>\2)    针对问题：啥情况啥场景。</p>
<p>\3)    本文的目的：可以做到啥。</p>
<p>\4)    实现的方法：</p>
<p>\5)    方法简介</p>
<p>\6)    方法优化</p>
<p>\7)    方法总结‘</p>
<p>\8)    文章存在的问题</p>
<p>\9)    个人的思考</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/11/08/Appearance-Based-Gaze-Estimation-Using-Dilated-Convolutions/" data-id="ckvtao51q00017cupccd6by18" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag">文献阅读</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/" class="article-date">
  <time datetime="2021-11-07T01:52:13.000Z" itemprop="datePublished">2021-11-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/">Multiview Multitask Gaze Estimation With Deep Convolutional Neural Networks</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>2018 TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</p>
<p>Dongze Lian, Lina Hu, Weixin Luo, Y anyu Xu, Lixin Duan, Jingyi Y u,Member , IEEE, and Shenghua Gao.</p>
</blockquote>
<p>(1) 评价：基于深度卷积神经网络的<strong>多视图多任务</strong>注视估计</p>
<p>(2)  针对问题：现有的许多方法都是基于单个摄像机的，大多数方法只关注注视点估计或注视方向估计。</p>
<p>(3) 本文的方法：</p>
<p>a.分析了注视点估计和注视方向估计之间的密切关系，并采用<strong>部分共享卷积神经网络结构</strong>来同时估计注视方向和注视点。</p>
<p>b.引入了一种新的<strong>多视角注视跟踪数据集</strong>，该数据集由<strong>不同被试的多视角注视图像组成</strong>。</p>
<p>c.对于注视方向的预测，提出在左右眼注视方向上引入共面约束。</p>
<p>对于注视点的估计，提出引入一个跨视图池模块。</p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211107213710618.png" alt="image-20211107213710618"></p>
<blockquote>
<p>四流输入、四流输出？怎么共享参数?</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://datasets.d2.mpi-inf.mpg.de/MPIIGaze/MPIIGaze.tar.gz">https://datasets.d2.mpi-inf.mpg.de/MPIIGaze/MPIIGaze.tar.gz</a></p>
<h1 id="数据集ShanghaiTechGaze"><a href="#数据集ShanghaiTechGaze" class="headerlink" title="数据集ShanghaiTechGaze"></a>数据集ShanghaiTechGaze</h1><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211112160847091.png" alt="image-20211112160847091"></p>
<p>设备：使用27英寸的苹果iMac机器作为显示设备。屏幕的宽/高分别为59.77厘米和33.62厘米。然后，屏幕底部部署了三台GoPro Hero 4相机，以捕捉参与者的图像。两台相邻摄像机之间的距离为18.22厘米。</p>
<p>环境：然后，在正常照明条件下，将系统固定在房间的书桌上。为了避免其他移动的物体/人或噪音造成的干扰，房间内保持安静和空无一人，只有一名待命助手除外。</p>
<p>使用大型iMac的第一个原因是，希望在水平和垂直方向上预测更大范围的注视点。相比之下，GazeCapture只预测了手机或平板电脑屏幕内的点。因此，数据集比GazeCapture更具挑战性。</p>
<p>使用iMac的第二个原因是，它的视网膜屏幕有更高的分辨率，以保证像素精度的地面真实;同时，减少了数据采集过程中的眼睛疲劳。</p>
<p>采集过程：要求参与者自由地坐在屏幕前。然后，在灰色背景的屏幕上随机显示一个半径为8像素的白点(作为ground truth)，让参与者用鼠标点击。这样的点击动作可以帮助参与者将注意力吸引到这个点上。然后，记录光标的坐标和动作的时间戳，之后显示光标位置上的蓝点。同时，我们计算白点和蓝点之间的距离。如果距离超过一定的阈值(在我们的设置中是8像素)，则有可能参与者没有盯着白点，因此该数据样本将被丢弃。在采集过程中，GoPro相机被设置为视频模式。根据点击动作的时间戳，我们可以从视频中提取参与者的图像帧。对于每个参与者，在记录数据之前，要求他/她先点击9个点，熟悉数据采集系统。接下来，50个点将在每个环节依次显示给参与者，以进行数据采集。当参与者成功点击上一个点后，屏幕会闪烁，下一个点会显示出来。每个参与者被要求点击12期(总共点击600个点)。在两个周期之间，我们设置了1分钟的休息时间，以避免眼睛疲劳。</p>
<p>我们总共招募了137名学生参与者进行数据收集(年龄在20 — 24岁之间，男性98名，女性39名)。所有参与者视力正常或矫正至正常。在去除白点与蓝点之间距离大于阈值的数据样本后，为每个参与者保留约450-600个点及其对应的眼睛和面孔图像。最后ShanghaiTechGaze数据集由233 796张图像组成。我们进一步使用100个参与者对应的图像作为训练集，其余37个参与者对应的图像作为测试集。</p>
<h2 id="数据组织格式"><a href="#数据组织格式" class="headerlink" title="数据组织格式"></a>数据组织格式</h2><p>—|dataset</p>
<p>——|annotations</p>
<p>———|txtfile</p>
<p>————|test_txt</p>
<p>—————|leftcamera</p>
<p>——————|eyelocation.txt    (images/face_landmarks/leftcamera/00109/00000.mat-images/face_landmarks/leftcamera/00147/00599.mat)21301rows</p>
<p>——————|lefteye.txt    (images/Single_eyes/leftcamera/00109/00000_left.jpg-images/Single_eyes/leftcamera/00147/00599_left.jpg)</p>
<p>——————|righteye.txt    (images/Single_eyes/leftcamera/00109/00000_right.jpg)</p>
<p>—————|middlecamera</p>
<p>——————|eyelocation.txt</p>
<p>——————|lefteye.txt</p>
<p>——————|righteye.txt</p>
<p>—————|rightcamera</p>
<p>——————|eyelocation.txt</p>
<p>——————|lefteye.txt</p>
<p>——————|righteye.txt</p>
<p>—————|gt.txt    (images/coordinate/00109/00000.mat-images/coordinate/00147/00599.mat)</p>
<p>————|train_txt</p>
<p>(images/coordinate/00004/00000.mat-images/coordinate/00108/00599.mat)56631rows</p>
<p>——|images</p>
<p>———|coordinate </p>
<p>————|candidate_index</p>
<p>—————|00000.mat-00599.mat 存放2维数据，真值。</p>
<p>———|face_landmarks（eyelocation，candidate_index，00000.mat-00599.mat）</p>
<p>————|leftcamera存放24维数据</p>
<p>————|middlecamera</p>
<p>————|rightcamera</p>
<p>———|Single_eyes（eye_patch，candidate_index，00000_left.jpg-00599_left.jpg，00000_right.jpg-00599_right.jpg）</p>
<p>————|leftcamera</p>
<p>————|middlecamera</p>
<p>————|rightcamera</p>
<blockquote>
<p>眼睛的landmart位置信息和gt使用.mat格式存储。</p>
</blockquote>
<h1 id="实验code"><a href="#实验code" class="headerlink" title="实验code"></a>实验code</h1><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/mmcodeStruct.png" alt="image-20211111170143842"></p>
<blockquote>
<p>查看mat格式。能不能清晰的读出来，是否需要下载matlab?</p>
<p>dataloader的__getitem__函数在enumerate部分</p>
</blockquote>
<blockquote>
<p>123行 eyelocation = sio.loadmat(eyelocation_name)[‘eyelocation’]是个24个数字</p>
<p>eyelocation_name = ’/data/ShanghaiTechGaze/images/face_landmarks/leftcamera/00061/00157.mat’</p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211116170548765.png" alt="image-20211116170548765"></p>
</blockquote>
<blockquote>
<p>124行gt = sio.loadmat(gt_name)[‘xy_gt’]是两个数字</p>
<p>gt_name = ‘/data/ShanghaiTechGaze/images/coordinate/00061/00157.mat’</p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211116170844442.png" alt="image-20211116170844442"></p>
<p>数据Normalization 是因为是左眼吗所以这么处理。   </p>
<p>gt[0] -= W_screen / 2 宽</p>
<p>​    gt[1] -= H_screen / 2 高</p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211116171147547.png" alt="image-20211116171147547"></p>
<p>第151行 data, target = (input[‘le’], input[‘re’], input[‘eyelocation’]), input[‘gt’]</p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211116172803224.png" alt="image-20211116172803224"></p>
</blockquote>
<h2 id="multi-view-gaze-master-code-Train-Single-View-ST-py"><a href="#multi-view-gaze-master-code-Train-Single-View-ST-py" class="headerlink" title="multi-view-gaze-master/code/Train_Single_View_ST.py"></a>multi-view-gaze-master/code/Train_Single_View_ST.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GazeImageDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, txt_file, txt_dir, transform=<span class="literal">None</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        </span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">train_loader, model, criterion, optimizer, epoch</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">test_loader, model, criterion, epoch, minimal_error</span>):</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AverageMeter</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, val, n=<span class="number">1</span></span>):</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_error</span>(<span class="params">output, target</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_checkpoint</span>(<span class="params">state, filename=<span class="string">&#x27;checkpoint.pth.tar&#x27;</span></span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adjust_learning_rate</span>(<span class="params">optimizer, epoch</span>):</span></span><br></pre></td></tr></table></figure>

<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/%E5%AE%9E%E9%AA%8C%E5%8F%82%E6%95%B0.png" alt="image-20211115131555168"></p>
<h2 id="multi-view-gaze-master-code-network-gazenet-py"><a href="#multi-view-gaze-master-code-network-gazenet-py" class="headerlink" title="multi-view-gaze-master/code/network/gazenet.py"></a>multi-view-gaze-master/code/network/gazenet.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选对应的resnet</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">single_view</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line"><span class="comment">#在resnet.py中设置好resnet</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#训练过程，向前传递参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, *<span class="built_in">input</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> self.view == <span class="string">&#x27;single&#x27;</span>:</span><br><span class="line">        out = self.single_view(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<p>128,512.  128,512. 128,128. 128,1152.</p>
</blockquote>
<h2 id="single-eye-训练思路"><a href="#single-eye-训练思路" class="headerlink" title="single eye 训练思路"></a>single eye 训练思路</h2><p>在每一个epoch里分别训练和测试，每完成一个epoch训练后，保存权重。</p>
<p>每一次训练，每一个batch（即每一个iteration）中，左右眼分别过Resnet34，得到长度为512输出，24维landmark过线性层得到长度为128的输出，之后这3个输出concatenate成1152的输出。再过fc（两次linear），输出。</p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211116212707500.png" alt="image-20211116212707500"></p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211117164429587.png" alt="image-20211117164429587"></p>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h2 id="路径出错"><a href="#路径出错" class="headerlink" title="路径出错"></a>路径出错</h2><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks%5CSTpy_101th_row.png" alt="image-20211112150649867"></p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211119155759306.png" alt="image-20211119155759306"></p>
<blockquote>
<p>眼睛的位置是瞳仁的中心？还是眼周做平均</p>
</blockquote>
<hr>
<p><a target="_blank" rel="noopener" href="https://github.com/dongzelian/multi-view-gaze">dongzelian/multi-view-gaze: Multi-view gaze estimation (github.com)</a></p>
<p>\1)    评价：贡献创新点。</p>
<p>\2)    针对问题：啥情况啥场景。</p>
<p>\3)    本文的目的：可以做到啥。</p>
<p>\4)    实现的方法：</p>
<p>\5)    方法简介</p>
<p>\6)    方法优化</p>
<p>\7)    方法总结‘</p>
<p>\8)    文章存在的问题</p>
<p>\9)    个人的思考</p>
<p>简要的评价，任务，方法的简要描述。关注文章的动机。 </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/" data-id="ckvtao51l00007cupd8enfwjb" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-gaze综述文章" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/" class="article-date">
  <time datetime="2021-11-03T03:08:18.000Z" itemprop="datePublished">2021-11-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/">gaze综述文章</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>人的眼睛由视网膜、瞳孔、角膜、巩膜、虹膜等组成，眼睛检测是对眼睛进行定位并测量眼睛注视的过程。有几种用于注视跟踪的方法，如基于形状的、基于特征的、基于外观的和混合的方法。</p>
<p>基于特征的凝视估计技术是通过检测眼睛的一组不同的特征，对光线和视点的差异不敏感，虹膜和瞳孔需要清晰才能正确检测，包括瞳孔的轮廓，没有角膜反射。</p>
<p>此外，必须应用三维和基于几何的方法。相比之下，基于外观的凝视估计技术是通过提取图像内容并将其映射到屏幕上的位置来实现的。因此，寻找关注点、相关特征和个人变化是这项技术的关键步骤。这种技术的例子是流形的，包括灰度单元、交叉比、高斯插值和变形模型。</p>
<h1 id="眼动跟踪技术"><a href="#眼动跟踪技术" class="headerlink" title="眼动跟踪技术"></a>眼动跟踪技术</h1><p>四种眼动跟踪技术已经成为该领域和开发新的眼动跟踪应用的研究重点。他们是巩膜搜索线圈技术、红外眼图(IOG)、眼电图(EOG)、眼视频图(VOG)scleral search coil technique, infrared oculography (IOG), electrooculography (EOG), and video oculography (VOG).。</p>
<h2 id="红外眼图-IOG"><a href="#红外眼图-IOG" class="headerlink" title="红外眼图(IOG)"></a>红外眼图(IOG)</h2><p>红外眼膜照相术(IOG)技术测量从巩膜反射的红外光的强度，这提供了关于眼睛位置的各种信息。一副眼镜就能发出光。该方法主要依赖于光和瞳孔检测算法。为了解决头部运动灵敏度的问题，在应用该技术时，使用红外光源包括一个参考点，<strong>即角膜反射或闪烁</strong>。</p>
<p>红外技术产生的干扰比EOG少。红外眼图(IOG)、眼电图(EOG)。</p>
<p>IOG技术的一个优势是它能够平滑地处理眨眼。然而，它无法量化扭转运动。</p>
<h2 id="Electrooculography"><a href="#Electrooculography" class="headerlink" title="Electrooculography"></a>Electrooculography</h2><p>眼动电波图测量技术，眼电描记法。</p>
<p>眼电描记术(Electrooculography, EOG)是一种实用廉价的人机交互技术。在这种方法中，传感器被连接到眼睛周围的区域，通过<strong>测量眼睛旋转时的波动来检测电场</strong>。</p>
<p><strong>水平和垂直的眼球运动是用电极分离记录的</strong>。然而，这种信号可以在没有眼球运动的情况下被改变。图5显示了带有传感器的可穿戴EOG设备。</p>
<p>EOG不是一种日常使用的方法;它的应用将有利于医学领域和实验室。该方法与眼球运动呈线性相关，可以使用头部运动进行跟踪(Chennamma &amp; Yuan, 2013;Sorate et al.， 2017)。EOG眼凝视界面结构简单，易于使用。然而，由于眼球漂移和眼部疾病，EOG的使用受到限制(Tsui, Jia, Gan， &amp; Hu, 2007;Constable等人，2017)。针对这个问题已经开发了一个解决方案。用户可以使用指针和切换器轻松地打开/关闭呼叫设备，或将1位信号转发到具有高度牢固和精细度的PC (Tsui, Jia, Gan, Hu， &amp; Yuan, 2007)。</p>
<h2 id="Video-Oculography"><a href="#Video-Oculography" class="headerlink" title="Video Oculography"></a>Video Oculography</h2><p>一个典型的装置包括一个记录眼睛运动的摄像机和一个保存和分析目光数据的电脑。VOG方法可以使用可见光或红外光。VOG是一种非侵入式系统，可以远程执行眼球追踪。</p>
<p>基于使用的摄像机数量，有两种实现视频眼动跟踪的方法:第一种方法使用单个摄像机，而另一种方法使用多个摄像机(Majaranta &amp; Bulling, 2014)。</p>
<p>两种类型的眼动跟踪器，远程或头部安装，如果在HCI系统中使用，由于头部位置的变化有一个主要缺点。</p>
<p>对于远程跟踪器，这可以通过使用<strong>两个立体摄像头或一个广角摄像头来解决</strong>，以搜索前面的人，另一个指向人的脸，并放大。图6显示了一个使用两个摄像机的VOG示例。</p>
<blockquote>
<p>克服头部位置变化？怎么克服？</p>
</blockquote>
<p>单相机系统:单相机系统捕捉固定在单点上的有限视场和高分辨率图像。这种方法<strong>使用红外光源产生角膜反射</strong>，将作为注视估计的参考点。</p>
<p>当头部移动时，闪烁明显改变其位置，瞳孔-闪烁矢量对于眼睛和头部运动保持恒定。一些商业系统使用一个摄像头和一个红外光。</p>
<p>这种方法的主要困难是要充分捕捉高分辨率图像所需的视野有限。通过在设置中添加多个光源可以获得更好的结果(Chennamma &amp; Yuan, 2013;Sorate et al.， 2017)。</p>
<p><strong>多相机眼动仪</strong>:需要大视场的自由头部运动。通过广角镜头相机或可移动的窄角镜头相机来实现这一目标。一个摄像头用于眼睛，另一个摄像头用于捕捉头部位置。所有从这些相机收集到的数据都被结合起来，用来估计凝视点。眼动仪系统使用两个摄像机组成一个<strong>立体视觉系统来校准瞳孔中心的计算</strong>，这是使用<strong>三维坐标</strong>实现的。</p>
<p>从这些摄像机生成的视频，如VOG，被引入来寻找解决基于图像的眼球追踪技术面临的许多问题的方法。其中一些问题包括由于头部运动而难以发现眼睛，由于眨眼而眼睛模糊，或睫毛弯曲。VOG有助于提高眼动跟踪系统的准确性和观察眼动障碍。视频系统可以操作简单，允许头部运动和完全远程记录。然而，视频录制系统价格昂贵，需要更多的存储和具有高计算能力的设备。此外，在记录闭眼时，无法测量眼睛扭转。</p>
<h1 id="使用机器学习的眼球追踪"><a href="#使用机器学习的眼球追踪" class="headerlink" title="使用机器学习的眼球追踪"></a>使用机器学习的眼球追踪</h1><h1 id="眼动追踪技术的方法-知乎-zhihu-com"><a href="#眼动追踪技术的方法-知乎-zhihu-com" class="headerlink" title="眼动追踪技术的方法 - 知乎 (zhihu.com)"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/159057692">眼动追踪技术的方法 - 知乎 (zhihu.com)</a></h1><p>在早期眼动追踪中，通常是直接观察的，这种方式非常的粗略。在最初的自动化系统追踪中，使用的是一种与眼睛直接接触的搜索方法，通过<strong>将一个橡胶吸盘吸在眼球表面</strong>，发明了第一个可以记录数据的眼动仪，这种眼动仪非常的不方便，且不舒适。</p>
<p>1963最早的眼动追踪电子系统之一是<strong>电磁巩膜搜索线圈</strong>。它们被嵌入到专用的隐形眼镜中，并且将导线连接到记录设备。根据眼睛运动产生的感应电压来记录眼动轨迹，该系统具有很高的精度和速度，但是在进行实验时是侵入式的，且需要先麻醉人的眼睛，将实验用的设备吸附早眼球上，通常的实验环境是在法拉第笼中，该方法对参与者的眼睛影响较大，有一定的生理伤害。</p>
<p>双眼浦肯（Purkinje）成像系统（DPI）等眼动追踪设备虽然没有物理接触到眼睛</p>
<h2 id="非侵入式眼动技术主要采用的追踪方法主要有1-巩膜一虹膜边缘法、2-瞳孔追踪方法、3-瞳孔一角膜反射法。"><a href="#非侵入式眼动技术主要采用的追踪方法主要有1-巩膜一虹膜边缘法、2-瞳孔追踪方法、3-瞳孔一角膜反射法。" class="headerlink" title="非侵入式眼动技术主要采用的追踪方法主要有1.巩膜一虹膜边缘法、2.瞳孔追踪方法、3.瞳孔一角膜反射法。"></a>非侵入式眼动技术主要采用的追踪方法主要有1.巩膜一虹膜边缘法、2.瞳孔追踪方法、3.瞳孔一角膜反射法。</h2><h3 id="1-巩膜一虹膜边缘法"><a href="#1-巩膜一虹膜边缘法" class="headerlink" title="1.巩膜一虹膜边缘法"></a>1.巩膜一虹膜边缘法</h3><p>利用红外光照射人眼，在眼睛附近安装的两只红外光敏管用来接收巩膜和虹膜边缘处两部分反射的红外光。当眼球向一侧运动时，虹膜就转向这边，这一侧的光敏管所接受的红外线就会减少;而另一侧的巩膜反射部分增加，导致这边的光敏管所接受的红外线增加。利用这个差分信号就能无接触的测出眼动。这种方法的水平精度较高，垂直精度较低、干扰大、头部误差大。</p>
<p>利用红外线差分信号。</p>
<blockquote>
<p>在现在大多数基于视频的眼动跟踪系统都包括一个红外摄像机，红外光（IR）照明器以及用于瞳孔中心检测和伪影排除的眼动追踪算法，图像处理和数据收集由专用硬件处理或者由计算机（Host PC）或软件处理。基于红外的照明具有多个优点：参与者在很大程度上看不到照明，并且可以通过波长过滤来自人造光源的伪影。</p>
</blockquote>
<h3 id="2-瞳孔追踪方法"><a href="#2-瞳孔追踪方法" class="headerlink" title="2.瞳孔追踪方法"></a>2.瞳孔追踪方法</h3><p>现在眼动仪中常用的两种近红外眼动追踪技术：明瞳和暗瞳。它们的差异基于照明源相对于光学系统的位置。</p>
<p>如果照明与光路同轴，则当光从视网膜反射时，眼睛将充当反射器，从而产生类似于红眼的明亮瞳孔效果。</p>
<p>如果照明源偏离光路，则瞳孔会变暗，因为来自视网膜的回射被定向为远离相机。</p>
<p><img src="https://pic3.zhimg.com/80/v2-fc125c419a01a7baa7e9ab0c93ef4a86_720w.jpg" alt="img"></p>
<h3 id="3-瞳孔-角膜反射追踪方法"><a href="#3-瞳孔-角膜反射追踪方法" class="headerlink" title="3.瞳孔-角膜反射追踪方法"></a>3.瞳孔-角膜反射追踪方法</h3><p>首先利用眼摄像机拍摄眼睛图像，接着通过图像处理得到瞳孔中心位置。然后把角膜反射点（黄色斑点）作为眼摄像机和眼球的相对位置的基点，根据图像处理得到的瞳孔中心即可以得到视线向量坐标，从而确定人眼注视点。</p>
<p><img src="https://pic4.zhimg.com/v2-c047bc5b78bca574f785d4c2dc55aea7_r.jpg" alt="preview"></p>
<blockquote>
<p>怎么得到角膜反射点？</p>
<p>使用rgb的数据集是怎么label的，做一次实验就知道了。</p>
</blockquote>
<p>在找到较好的瞳孔-角膜反射点后，通过一些校准程序，找出瞳孔与角膜反射点间组成的向量与屏幕注视点之间的映射函数，然后通过检测瞳孔-角膜向量的变化量，实时跟踪出人在屏幕中所凝视的兴趣点。</p>
<p>由于人眼形状，大小，结构，存在个体差异，眼睛球面上的点在摄像机参照系中的投影点位置和眼睛转动角度之间存在非线性关系，并且视线估计方向与真实视线方向有模型误差，所以视线跟踪系统需要校准环节。</p>
<p><img src="https://pic1.zhimg.com/v2-141ec7bc86c26ee912167377e25dc6fc_r.jpg" alt="preview"></p>
<p>校准点可以是一点，也可以是3点、5点、9点以及13点，校准点数依据实验任务的不同而不同，该算法在每个目标的眼睛位置（减去CR）和注视位置之间创建数学转换，然后创建一个矩阵来覆盖整个校准区域，并在每个点之间进行插值。使用的校准点越多，在整个视场中的精度就越高，越均匀。</p>
<p>在EyeLink系统中，除了校准过程外，还有一个过程是验证过程。因为在大多数时候校准是需要受试者一定的配合和能力的，因此在大多数情况下，需要进行验证校准过程产生的误差。比如：以下9点校准。</p>
<p><img src="https://pic3.zhimg.com/80/v2-d1e2edb5067000110a05c7ff670440de_720w.jpg" alt="img"></p>
<p>当平均值小于1度，最大值小于1.5度时，会显示绿色，说明是GOOD；当平均值小于1.5度，最大值小于2度时，会显示灰色，说明是FAIR；当平均值大于1.5度，最大值大于2度时，会显示红色，说明是POOR，在大多数情况下，需要调整到GOOD情况下，才能被允许进行实验数据的收集，对于大多数实验来说，超过1度被认为是校准失败，需要再次校准。</p>
<h2 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h2><p>1、瞳孔遮挡</p>
<p>在当前的眼动仪中，很明确的需要瞳孔无遮挡的视线，浓厚的眼睫毛有可能会被误认为瞳孔，从而使眼动追踪无法继续进行，当然，在绝大多数时候，即使瞳孔被部分遮挡，当前的眼动仪也可以根据算法确定中心，但是在某个点上，如果被遮挡足够多，眼动追踪也会无法继续进行。我们目前依然保持着较高的信心，这种情况发生的事件很小。</p>
<p>2、眼部化妆</p>
<p>简单来说，做眼动实验前，要求受试者不要化眼部妆，而且不要带美瞳，它会造成红外的照射不能完全捕捉瞳孔区域。</p>
<p>3、眼睛度数</p>
<p>一般建议是超过800度以上的或者是有严重的散光的被试，将会被排除在外。任何眼镜都会使拍摄的眼睛变形，有可能会减少一些反射的红外照明光源，在现代的眼动仪中，能追踪到低度数的眼动，这是可以的，但是也需要注意，在进行实验时，尽可能的不戴有黑色镜框的眼镜，它可能会遮挡或被误认为瞳孔区域进而眼动追踪无法继续。</p>
<h1 id="Vision-based-Gaze-Estimation-A-Review"><a href="#Vision-based-Gaze-Estimation-A-Review" class="headerlink" title="Vision-based Gaze Estimation: A Review"></a>Vision-based Gaze Estimation: A Review</h1><p>从元学习、因果推理、解纠缠表征和无约束注视估计的社会注视行为等方面指出了注视估计未来的研究方向和挑战。</p>
<h2 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h2><p>头部姿势可以粗略地决定注视的方向。然而，头部姿势不足以描述人的注视，因为眼睛可以旋转。为了更好地描述人眼注视，除了考虑头部姿态外，还需要从眼睛区域中提取与注视相关的特征。</p>
<p>因此，鲁棒的凝视估计需要从头部姿态和凝视中提取具有代表性的信息，以多粒度描述人眼的注视。</p>
<p>头部姿势变化的一个影响是眼睛位置的变化。为了提取高度相关的注视特征，需要从原始图像中裁剪出眼睛区域或人脸区域，以去除噪声背景。人脸地标通常作为参考点来裁剪所需区域。然后，可以提取凝视特征，但仍然会出现外观变化。头部运动的另一个影响是，当考虑头部运动时，眼睛的外观发生了显著变化。</p>
<blockquote>
<p>头的运动影响的是,眼睛的位置和外观。</p>
</blockquote>
<p><img src="/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211105101710286.png" alt="image-20211105101710286"></p>
<blockquote>
<p>使用UnityEye生成的不同头部姿势(偏斜:30°，0°，-30°)下，眼睛外观随相同的注视角度变化。例如，从左到右，虹膜中心与眼角之间的相对位置变化明显</p>
</blockquote>
<p>凝视和头姿在现有数据集中的分布不均匀，这也导致了凝视和头姿之间的过拟合。假设没有提取姿态不敏感特征，当训练样本和测试样本不服从独立同分布假设时，训练模型将无法估计注视量。</p>
<p><img src="/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211104112538152.png" alt></p>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><h3 id="5-Y-Sugano-Y-Matsushita-and-Y-Sato-“Learning-bysynthesis-for-appearance-based-3d-gaze-estimation-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-CVPR-2014-pp-1821–-1828"><a href="#5-Y-Sugano-Y-Matsushita-and-Y-Sato-“Learning-bysynthesis-for-appearance-based-3d-gaze-estimation-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-CVPR-2014-pp-1821–-1828" class="headerlink" title="[5] Y. Sugano, Y. Matsushita, and Y. Sato, “Learning-bysynthesis for appearance-based 3d gaze estimation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 1821– 1828."></a>[5] Y. Sugano, Y. Matsushita, and Y. Sato, “Learning-bysynthesis for appearance-based 3d gaze estimation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 1821– 1828.</h3><p>通过训练一个随机回归森林模型来<strong>学习眼睛区域和眼睛注视点之间的映射函数</strong>。具体来说，回归森林模型中的每棵回归树都针对每一个冗余聚类进行训练。来重建眼睛区域。</p>
<h3 id="6-类似的聚类方案也用于处理头部姿态的方差。"><a href="#6-类似的聚类方案也用于处理头部姿态的方差。" class="headerlink" title="[6]类似的聚类方案也用于处理头部姿态的方差。"></a>[6]类似的聚类方案也用于处理头部姿态的方差。</h3><p>但是，为每个集群训练多个模型会增加计算复杂度。</p>
<h3 id="7-R-Ranjan-S-D-Mello-and-J-Kautz-“Light-weight-head-pose-invariant-gaze-tracking-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-Workshops-CVPRW-Los-Alamitos-CA-USA-IEEE-Computer-Society-jun-2018-pp-2237–-22-378"><a href="#7-R-Ranjan-S-D-Mello-and-J-Kautz-“Light-weight-head-pose-invariant-gaze-tracking-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-Workshops-CVPRW-Los-Alamitos-CA-USA-IEEE-Computer-Society-jun-2018-pp-2237–-22-378" class="headerlink" title="[7] R. Ranjan, S. D. Mello, and J. Kautz, “Light-weight head pose invariant gaze tracking,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Los Alamitos, CA, USA: IEEE Computer Society, jun 2018, pp. 2237– 22 378."></a>[7] R. Ranjan, S. D. Mello, and J. Kautz, “Light-weight head pose invariant gaze tracking,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Los Alamitos, CA, USA: IEEE Computer Society, jun 2018, pp. 2237– 22 378.</h3><p>受聚类方法的启发，提出了一种新的卷积神经网络模型即由5个卷积层(conv1-conv5)和1个全连接层(fc1)组成的卷积神经网络模型。与训练多元回归模型不同的是，该CNN模型<strong>根据梯度更新conv1-conv5和fc1的权值来学习注视表示</strong>，然后在fc1之后<strong>利用学习到的注视表示</strong>和来自相应<strong>头部姿态聚类的头部姿态向量</strong>进一步更新多个CNN流。每个流在fc1之后有两个完全连接的层。该方案充分利用有限的数据学习基本的注视表示，然后根据不同的头部姿态调整注视表示。</p>
<h2 id="人脸正面化"><a href="#人脸正面化" class="headerlink" title="人脸正面化"></a>人脸正面化</h2><blockquote>
<p>基于人脸正面化的方法的主要思想是重建一个三维的人脸模型。人脸可以被转换为正面，这样就可以消除头部的方差。</p>
<p>虽然人脸frontalization方法可以生成一个正面人脸，但如果没有捕获部分眼睛区域，眼睛外观仍然不完整，降低了凝视估计。</p>
</blockquote>
<h3 id="8-Q-Shi-S-Nobuhara-and-T-Matsuyama-“3d-face-reconstruction-and-gaze-estimation-from-multi-view-video-using-symmetry-prior-”-Ipsj-Transactions-on-Computer-Vision-and-Applications-vol-4-pp-149–160-2012"><a href="#8-Q-Shi-S-Nobuhara-and-T-Matsuyama-“3d-face-reconstruction-and-gaze-estimation-from-multi-view-video-using-symmetry-prior-”-Ipsj-Transactions-on-Computer-Vision-and-Applications-vol-4-pp-149–160-2012" class="headerlink" title="[8] Q. Shi, S. Nobuhara, and T. Matsuyama, “3d face reconstruction and gaze estimation from multi-view video using symmetry prior,” Ipsj Transactions on Computer Vision and Applications, vol. 4, pp. 149–160, 2012."></a>[8] Q. Shi, S. Nobuhara, and T. Matsuyama, “3d face reconstruction and gaze estimation from multi-view video using symmetry prior,” Ipsj Transactions on Computer Vision and Applications, vol. 4, pp. 149–160, 2012.</h3><p>Qun等人[8]提出了一种基于<strong>对称先验知识的三维重建方法用于注视估计</strong>。该方法首先从多个摄像机的三维网格数据和视频中<strong>检测出人脸区域。然后提取人脸的关键点，得到对称平面。</strong>其次，在对称平面的基础上，<strong>利用对称先验知识重建三维人脸模型</strong>，然后利用超解绘制方法绘制出虚拟正面人脸图像。最后，在虹膜直径等于眼球半径的假设下，<strong>将二维虹膜中心映射到三维眼球模型，得到三维虹膜位置和三维眼球中心</strong>。因此，凝视方向就是从三维眼球中心到三维虹膜中心的连线。</p>
<h3 id="9-K-A-Funes-Mora-and-J-Odobez-“Gaze-estimation-from-multimodal-kinect-data-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-Workshops-CVPRW-2012-pp-25–30-10-K-A-Funes-Mora-and-J-M-Odobez-“Gaze-estimation-in-the-3d-space-using-rgb-d-sensors-”-International-Journal-of-Computer-Vision-vol-118-no-2-pp-194–-216-2016"><a href="#9-K-A-Funes-Mora-and-J-Odobez-“Gaze-estimation-from-multimodal-kinect-data-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-Workshops-CVPRW-2012-pp-25–30-10-K-A-Funes-Mora-and-J-M-Odobez-“Gaze-estimation-in-the-3d-space-using-rgb-d-sensors-”-International-Journal-of-Computer-Vision-vol-118-no-2-pp-194–-216-2016" class="headerlink" title="[9] K. A. Funes Mora and J. Odobez, “Gaze estimation from multimodal kinect data,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2012, pp. 25–30. [10] K. A. Funes-Mora and J. M. Odobez, “Gaze estimation in the 3d space using rgb-d sensors,” International Journal of Computer Vision, vol. 118, no. 2, pp. 194– 216, 2016."></a>[9] K. A. Funes Mora and J. Odobez, “Gaze estimation from multimodal kinect data,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2012, pp. 25–30. [10] K. A. Funes-Mora and J. M. Odobez, “Gaze estimation in the 3d space using rgb-d sensors,” International Journal of Computer Vision, vol. 118, no. 2, pp. 194– 216, 2016.</h3><p>提出首先在离线阶段基于三维变形模型学习一个针对特定人群的三维网格模型，然后在在线阶段根据深度数据估计头部姿态。通过头部姿态和三维网格数据，可以进一步生成正面。最后，凝视可以从裁剪的眼睛图像学习，然后映射回WCS坐标系。</p>
<h3 id="11-R-S-Ghiass-and-O-Arandjelovic-“Highly-accurate-gaze-estimation-using-a-consumer-rgb-d-sensor-”-in-Proceedings-of-the-Twenty-Fifth-International-Joint-Conference-on-Artificial-Intelligence-ser-IJCAI16-AAAI-Press-2016-p-33683374"><a href="#11-R-S-Ghiass-and-O-Arandjelovic-“Highly-accurate-gaze-estimation-using-a-consumer-rgb-d-sensor-”-in-Proceedings-of-the-Twenty-Fifth-International-Joint-Conference-on-Artificial-Intelligence-ser-IJCAI16-AAAI-Press-2016-p-33683374" class="headerlink" title="[11] R. S. Ghiass and O. Arandjelovic, “Highly accurate gaze estimation using a consumer rgb-d sensor,” in Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, ser. IJCAI16. AAAI Press, 2016, p. 33683374."></a>[11] R. S. Ghiass and O. Arandjelovic, “Highly accurate gaze estimation using a consumer rgb-d sensor,” in Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, ser. IJCAI16. AAAI Press, 2016, p. 33683374.</h3><p>Reza Shoja和Ognjen[11]提出了一个准确的头部姿势估计。采用基于人的变形模型，采用迭代最近点算法在深度空间中估计头部姿态。然后，通过变换和重渲染得到合成的人脸。最后，从合成图像的外观特征中回归凝视方向。该方法在被试头部运动时的平均误差为8.9°</p>
<h3 id="12-L-A-Jeni-and-J-F-Cohn-“Person-independent-3d-gaze-estimation-using-face-frontalization-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-Workshops-CVPRW-Los-Alamitos-CA-USA-IEEE-Computer-Society-jul-2016-pp-792–800"><a href="#12-L-A-Jeni-and-J-F-Cohn-“Person-independent-3d-gaze-estimation-using-face-frontalization-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-Workshops-CVPRW-Los-Alamitos-CA-USA-IEEE-Computer-Society-jul-2016-pp-792–800" class="headerlink" title="[12] L. A. Jeni and J. F. Cohn, “Person-independent 3d gaze estimation using face frontalization,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Los Alamitos, CA, USA: IEEE Computer Society, jul 2016, pp. 792–800."></a>[12] L. A. Jeni and J. F. Cohn, “Person-independent 3d gaze estimation using face frontalization,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Los Alamitos, CA, USA: IEEE Computer Society, jul 2016, pp. 792–800.</h3><p>检测人脸上<strong>密集的标记，并使用级联回归进行二维到三维的桥接。</strong>然后，通过求解优化问题进行三维面重建。在人脸正面化后，从6个眼睛计数器标记和1个瞳孔标记中提取二值特征，训练线性支持向量回归器，将注视特征映射到注视方向。在UT-Multiview数据集上的平均误差为4.2867°。</p>
<h2 id="特征融合"><a href="#特征融合" class="headerlink" title="特征融合"></a>特征融合</h2><p>特征融合通过考虑多模型特征来提高特征表示能力，从而提高分类或回归的性能。因此，<strong>头部姿态向量与注视特征的融合</strong>也是对抗头部姿态方差的有效方法。</p>
<p>采用了简单的连续融合凝视特征和头部姿态的方案，简单明了。</p>
<h3 id="13-R-Jafari-and-D-Ziou-“Gaze-estimation-using-kinect-ptz-camera-”-in-2012-IEEE-International-Symposium-on-Robotic-and-Sensors-Environments-Proceedings-2012-pp-13–18-14-R-Jafari-and-D-Ziou-“Eye-gaze-estimation-under-various-head-positions-and-iris-states-”-Expert-Systems-with-Applications-vol-42-no-1-pp-510–518-2015"><a href="#13-R-Jafari-and-D-Ziou-“Gaze-estimation-using-kinect-ptz-camera-”-in-2012-IEEE-International-Symposium-on-Robotic-and-Sensors-Environments-Proceedings-2012-pp-13–18-14-R-Jafari-and-D-Ziou-“Eye-gaze-estimation-under-various-head-positions-and-iris-states-”-Expert-Systems-with-Applications-vol-42-no-1-pp-510–518-2015" class="headerlink" title="[13] R. Jafari and D. Ziou, “Gaze estimation using kinect/ptz camera,” in 2012 IEEE International Symposium on Robotic and Sensors Environments Proceedings, 2012, pp. 13–18. [14] R. Jafari and D. Ziou, “Eye-gaze estimation under various head positions and iris states,” Expert Systems with Applications, vol. 42, no. 1, pp. 510–518, 2015"></a>[13] R. Jafari and D. Ziou, “Gaze estimation using kinect/ptz camera,” in 2012 IEEE International Symposium on Robotic and Sensors Environments Proceedings, 2012, pp. 13–18. [14] R. Jafari and D. Ziou, “Eye-gaze estimation under various head positions and iris states,” Expert Systems with Applications, vol. 42, no. 1, pp. 510–518, 2015</h3><p>Jafari和Ziou使用<strong>kinect估计头部姿势</strong>，并使用主动高清摄像机捕捉眼睛区域[13,14]。然后，从眼睛区域提取<strong>虹膜位移矢量和参考点</strong>。最后，<strong>结合虹膜位移向量、头部位置和头部姿态</strong>，采用变分贝叶斯多项式logistic回归估计凝视方向。</p>
<h3 id="15-X-Zhang-Y-Sugano-M-Fritz-and-A-Bulling-“Appearance-based-gaze-estimation-in-the-wild-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-CVPR-2015-pp-4511–4520"><a href="#15-X-Zhang-Y-Sugano-M-Fritz-and-A-Bulling-“Appearance-based-gaze-estimation-in-the-wild-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-CVPR-2015-pp-4511–4520" class="headerlink" title="[15] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Appearance-based gaze estimation in the wild,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 4511–4520"></a>[15] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Appearance-based gaze estimation in the wild,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 4511–4520</h3><p>Zhang等人[15]提出利用CNN从眼睛图像中提取凝视表征。具体来说，它首先通过旋转和平移相机到眼睛中心的眼睛区域。然后，将归一化后的眼睛图像和头部姿态矢量输入Lenet模型，利用眼睛姿态矢量和头部姿态矢量进行回归分析。该方法在MPIIGaze数据集上的平均误差为6.3°。</p>
<h3 id="16-X-Zhang-Y-Sugano-M-Fritz-and-A-Bulling-“Mpiigaze-Real-world-dataset-and-deep-appearance-based-gaze-estimation-”-IEEE-Trans-Pattern-Anal-Mach-Intell-vol-PP-no-99-pp-1–1-2017"><a href="#16-X-Zhang-Y-Sugano-M-Fritz-and-A-Bulling-“Mpiigaze-Real-world-dataset-and-deep-appearance-based-gaze-estimation-”-IEEE-Trans-Pattern-Anal-Mach-Intell-vol-PP-no-99-pp-1–1-2017" class="headerlink" title="[16] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Mpiigaze: Real-world dataset and deep appearance-based gaze estimation,” IEEE Trans Pattern Anal Mach Intell, vol. PP, no. 99, pp. 1–1, 2017."></a>[16] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Mpiigaze: Real-world dataset and deep appearance-based gaze estimation,” IEEE Trans Pattern Anal Mach Intell, vol. PP, no. 99, pp. 1–1, 2017.</h3><p>采用VGG-16模型代替Lenet模型，[16]进一步提高了性能，平均误差为5.5°。在MPIIGaze数据集中，右眼水平翻转。</p>
<h3 id="18-J-Lemley-A-Kar-A-Drimbarean-and-P-Corcoran-“Convolutional-neural-network-implementation-for-eyegaze-estimation-on-low-quality-consumer-imaging-systems-”-IEEE-Transactions-on-Consumer-Electronics-vol-65-no-2-pp-179–187-2019"><a href="#18-J-Lemley-A-Kar-A-Drimbarean-and-P-Corcoran-“Convolutional-neural-network-implementation-for-eyegaze-estimation-on-low-quality-consumer-imaging-systems-”-IEEE-Transactions-on-Consumer-Electronics-vol-65-no-2-pp-179–187-2019" class="headerlink" title="[18] J. Lemley, A. Kar, A. Drimbarean, and P. Corcoran, “Convolutional neural network implementation for eyegaze estimation on low-quality consumer imaging systems,” IEEE Transactions on Consumer Electronics, vol. 65, no. 2, pp. 179–187, 2019."></a>[18] J. Lemley, A. Kar, A. Drimbarean, and P. Corcoran, “Convolutional neural network implementation for eyegaze estimation on low-quality consumer imaging systems,” IEEE Transactions on Consumer Electronics, vol. 65, no. 2, pp. 179–187, 2019.</h3><p>然而，Lemley等人[18]证明，将两只眼睛都视为地面真理而不是将一只眼睛翻转到另一只眼睛，可以提高性能。此外，通过将5x5卷积替换为3x3卷积，并使用relu激活以及融合眼睛图像和头部姿态向量作为CNN的输入，进一步将MPIIGaze数据集上的注视估计平均误差提高到3.65°。</p>
<h3 id="17-H-Sun-C-Yang-and-S-Lai-“A-deep-learning-approach-to-appearance-based-gaze-estimation-under-head-pose-variations-”-in-2017-4th-IAPR-Asian-Conference-on-Pattern-Recognition-ACPR-2017-pp-935–940"><a href="#17-H-Sun-C-Yang-and-S-Lai-“A-deep-learning-approach-to-appearance-based-gaze-estimation-under-head-pose-variations-”-in-2017-4th-IAPR-Asian-Conference-on-Pattern-Recognition-ACPR-2017-pp-935–940" class="headerlink" title="[17] H. Sun, C. Yang, and S. Lai, “A deep learning approach to appearance-based gaze estimation under head pose variations,” in 2017 4th IAPR Asian Conference on Pattern Recognition (ACPR), 2017, pp. 935–940."></a>[17] H. Sun, C. Yang, and S. Lai, “A deep learning approach to appearance-based gaze estimation under head pose variations,” in 2017 4th IAPR Asian Conference on Pattern Recognition (ACPR), 2017, pp. 935–940.</h3><p>Sun等人[17]提出了一种多流CNN模型，该模型由4个并行的VGG模型组成，分别从眼睛和面部区域提取凝视特征和头部姿态。最终的凝视输出是所有流的平均值。结果是UT-Multiview数据集上的平均误差为6°。</p>
<h3 id="19-D-Lian-Z-Zhang-W-Luo-L-Hu-M-Wu-Z-Li-J-Yu-and-S-Gao-“Rgbd-based-gaze-estimation-via-multitask-cnn-”-in-Proceedings-of-the-AAAI-Conference-on-Artificial-Intelligence-vol-33-no-01-2019-pp-2488–-2495"><a href="#19-D-Lian-Z-Zhang-W-Luo-L-Hu-M-Wu-Z-Li-J-Yu-and-S-Gao-“Rgbd-based-gaze-estimation-via-multitask-cnn-”-in-Proceedings-of-the-AAAI-Conference-on-Artificial-Intelligence-vol-33-no-01-2019-pp-2488–-2495" class="headerlink" title="[19] D. Lian, Z. Zhang, W. Luo, L. Hu, M. Wu, Z. Li, J. Yu, and S. Gao, “Rgbd based gaze estimation via multitask cnn,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 2488– 2495."></a>[19] D. Lian, Z. Zhang, W. Luo, L. Hu, M. Wu, Z. Li, J. Yu, and S. Gao, “Rgbd based gaze estimation via multitask cnn,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 2488– 2495.</h3><p>Lian等人[19]提出了一种基于多流多任务的CNN模型。该模型从RGB图像和经过GAN精炼的深度图中，从裁剪过的眼睛图像和全人脸的头部姿态信息中学习凝视特征。然后，学习到的头部姿势信息与凝视特征连接在一起。在EYEDIAP和ShanghaiTechGaze数据集上的平均误差分别为4.8°和38.7 mm。</p>
<h2 id="几何特性的方法"><a href="#几何特性的方法" class="headerlink" title="几何特性的方法"></a>几何特性的方法</h2><blockquote>
<p>使用眼球模型的几何知识和不同坐标系之间的转换来补偿头部运动引起的偏差的研究。</p>
</blockquote>
<p><img src="/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211104112608127.png" alt="image-20211104112608127"></p>
<p><img src="/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211104112700336.png" alt="image-20211104112700336"></p>
<p><img src="/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211104112735277.png" alt="image-20211104112735277"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/" data-id="ckvnk2k0600028sup75tadjch" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-gaze大佬主页" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/10/31/gaze%E5%A4%A7%E4%BD%AC%E4%B8%BB%E9%A1%B5/" class="article-date">
  <time datetime="2021-10-31T07:06:34.000Z" itemprop="datePublished">2021-10-31</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/10/31/gaze%E5%A4%A7%E4%BD%AC%E4%B8%BB%E9%A1%B5/">gaze大佬主页</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Xucong Zhang<a target="_blank" rel="noopener" href="https://scholar.google.de/citations?user=lDfmDk4AAAAJ&amp;hl=en">https://scholar.google.de/citations?user=lDfmDk4AAAAJ&amp;hl=en</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/cvlab-uob/Awesome-Gaze-Estimation">cvlab-uob/Awesome-Gaze-Estimation: Awesome Curated List of Eye Gaze Estimation Paper (github.com)</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/10/31/gaze%E5%A4%A7%E4%BD%AC%E4%B8%BB%E9%A1%B5/" data-id="ckvnk2jzy00018sup7ix71wh3" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-gaze最新文献调研" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/10/29/gaze%E6%9C%80%E6%96%B0%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94/" class="article-date">
  <time datetime="2021-10-29T05:30:13.000Z" itemprop="datePublished">2021-10-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/10/29/gaze%E6%9C%80%E6%96%B0%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94/">gaze最新文献调研</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Vulnerability-of-Appearance-based-Gaze-Estimation"><a href="#Vulnerability-of-Appearance-based-Gaze-Estimation" class="headerlink" title="Vulnerability of Appearance-based Gaze Estimation"></a>Vulnerability of Appearance-based Gaze Estimation</h1><blockquote>
<p>2021预印本</p>
<p>Mingjie Xu1 Haofei Wang2 Yunfei Liu1 Feng Lu1, 2, *<br>1State Key Laboratory of VR Technology and Systems, School of CSE, Beihang University<br>2Peng Cheng Laboratory, Shenzhen, China</p>
</blockquote>
<p>评价：基于外观的凝视估计的脆弱性</p>
<p>针对问题：利用噪声对原始图像进行干扰会混淆凝视估计模型，基于机器学习的方法存在脆弱性。</p>
<p>本文的目的：尽管扰动后的图像在视觉上与原始图像相似，但凝视估计模型输出的凝视方向是错误的。本文研究了基于外观的注视估计的脆弱性。</p>
<p>从多个方面系统地描述了该漏洞的特性。</p>
<ul>
<li>基于像素的对抗攻击pixel-based adversarial attack、</li>
<li>基于补丁的对抗攻击patch-based adversarial attack</li>
<li>防御策略 defense strategy等。</li>
</ul>
<p>实现的方法：通过在原始输入中加入对抗性扰动，研究了是否有可能改变预测的注视方向，甚至输出一个特定的注视方向。</p>
<p><img src="/2021/10/29/gaze%E6%9C%80%E6%96%B0%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211029144448019.png" alt="image-20211029144448019"></p>
<p>refer</p>
<blockquote>
<p>眼神凝视是人类交流的重要渠道之一。它表示人眼分型[17,24]、认证[13,19]、显著性预测[31]、目标检测[7]等过程中感兴趣的区域 the region of interest during eye typing<br>[17,24], authentication [13,19], saliency prediction [31],<br>object detection 。</p>
</blockquote>
<p>观察到，当我们只攻击“鼻子”或“嘴”时，系统会产生很大的角度误差。图4(b)和图4(c)显示，发作后注意区域仅为“鼻子”或“嘴”。如果同时攻击“鼻子”和“嘴”，则角度误差会大大降低，攻击后注意区域也会同时转移到“鼻子”和“嘴”。</p>
<p>但是这个角误差还是比通过攻击其他部分来完成那个大。注意，不是所有的注意力区域都在“眼睛”上，这导致了这样的结果。如果我们同时攻击“眼睛”和“鼻子”，或者同时攻击“眼睛”和“嘴巴”，平均角度误差就会比只攻击“鼻子”和“嘴巴”低得多。在这种情况下，注意力区域在“眼睛”和其他部分。</p>
<p>Yiwei Bao, Yihua Cheng, Y unfei Liu, and Feng Lu. Adaptive<br>feature fusion network for gaze tracking in mobile tablets. In<br>25th International Conference on Pattern Recognition (ICPR),<br>2020.</p>
<p>Yihua Cheng, Shiyao Huang, Fei Wang, Chen Qian, and Feng<br>Lu. A coarse-to-fine adaptive network for appearance-based<br>gaze estimation. InProceedings of the AAAI Conference on<br>Artificial Intelligence, volume 34, pages 10623–10630, 2020.</p>
<p>Seonwook Park, Shalini De Mello, Pavlo Molchanov, Umar<br>Iqbal, Otmar Hilliges, and Jan Kautz. Few-shot adaptive gaze<br>estimation. InProceedings of the IEEE/CVF International<br>Conference on Computer Vision, pages 9368–9377, 2019.</p>
<p>Tobias Fischer, Hyung Jin Chang, and Yiannis Demiris. RT-<br>GENE: Real-Time Eye Gaze Estimation in Natural Environ-<br>ments. InEuropean Conference on Computer Vision, pages<br>339–357, September 2018.</p>
<p>Yihua Cheng, Xucong Zhang, Feng Lu, and Y oichi Sato.<br>Gaze estimation by exploring two-eye asymmetry.IEEE<br>Transactions on Image Processing, 29:5259–5272, 2020.</p>
<hr>
<p>\1)    评价：贡献创新点。</p>
<p>\2)    针对问题：啥情况啥场景。</p>
<p>\3)    本文的目的：可以做到啥。</p>
<p>\4)    实现的方法：</p>
<p>\5)    方法简介</p>
<p>\6)    方法优化</p>
<p>\7)    方法总结‘</p>
<p>\8)    文章存在的问题</p>
<p>\9)    个人的思考</p>
<p>简要的评价，任务，方法的简要描述。关注文章的动机。 </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/10/29/gaze%E6%9C%80%E6%96%B0%E6%96%87%E7%8C%AE%E8%B0%83%E7%A0%94/" data-id="ckvnk2jzx00008sup7jshg09b" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Blog/">Blog</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/">数学建模</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1/">课程设计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86/">面试经历</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Verilog/" rel="tag">Verilog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/" rel="tag">blog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dsp/" rel="tag">dsp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%9E%E9%AA%8C/" rel="tag">实验</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/" rel="tag">实验记录</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/" rel="tag">数学建模</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" rel="tag">数据处理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag">文献阅读</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%94%9F%E6%B4%BB%E6%80%BB%E7%BB%93/" rel="tag">生活总结</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="tag">论文阅读</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Verilog/" style="font-size: 10px;">Verilog</a> <a href="/tags/blog/" style="font-size: 10px;">blog</a> <a href="/tags/dsp/" style="font-size: 10px;">dsp</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/%E5%AE%9E%E9%AA%8C/" style="font-size: 10px;">实验</a> <a href="/tags/%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/" style="font-size: 10px;">实验记录</a> <a href="/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/" style="font-size: 10px;">数学建模</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" style="font-size: 10px;">数据处理</a> <a href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" style="font-size: 20px;">文献阅读</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 10px;">爬虫</a> <a href="/tags/%E7%94%9F%E6%B4%BB%E6%80%BB%E7%BB%93/" style="font-size: 10px;">生活总结</a> <a href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" style="font-size: 10px;">论文阅读</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">October 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/">风格造成的人脸关键点检测误差</a>
          </li>
        
          <li>
            <a href="/2021/12/28/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/">人脸关键点检测论文梳理</a>
          </li>
        
          <li>
            <a href="/2021/12/10/EyediapRGBDgaze/">EyediapRGBDgaze</a>
          </li>
        
          <li>
            <a href="/2021/11/19/web3-0/">web3.0</a>
          </li>
        
          <li>
            <a href="/2021/11/18/Few-Shot-Adaptive-Gaze-Estimation/">Few-Shot Adaptive Gaze Estimation</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 iszff<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>