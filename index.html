<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>iszff&#39; Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="记录">
<meta property="og:type" content="website">
<meta property="og:title" content="iszff&#39; Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="iszff&#39; Blog">
<meta property="og:description" content="记录">
<meta property="og:locale">
<meta property="article:author" content="iszff">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="iszff&#39; Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">iszff&#39; Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-人脸关键点检测的轻量化网络" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/" class="article-date">
  <time datetime="2022-01-02T13:59:43.000Z" itemprop="datePublished">2022-01-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/">人脸关键点检测的轻量化网络</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本篇文章文章主要介绍PFLD: A Practical Facial Landmark Detector这篇文章。</p>
<h2 id="2019CVPR-PFLD-A-Practical-Facial-Landmark-Detector"><a href="#2019CVPR-PFLD-A-Practical-Facial-Landmark-Detector" class="headerlink" title="[2019CVPR]PFLD: A Practical Facial Landmark Detector"></a>[2019CVPR]PFLD: A Practical Facial Landmark Detector</h2><blockquote>
<p>2019CVPR Xiaojie Guo1, Siyuan Li1, Jinke Y u1, Jiawan Zhang1, Jiayi Ma2, Lin Ma3, Wei Liu3, and Haibin Ling4</p>
<p>1Tianjin University2Wuhan University3Tencent AI Lab4Temple University</p>
</blockquote>
<h2 id="关键点检测四大挑战"><a href="#关键点检测四大挑战" class="headerlink" title="关键点检测四大挑战"></a>关键点检测四大挑战</h2><p>•<strong>挑战1</strong> ——<strong>局部变化。</strong> <strong>表情、局部极端光照(如高光和阴影)和遮挡</strong>会给人脸图像带来部分变化和干扰。一些区域的关键点可能会偏离它们的正常位置，甚至消失。</p>
<p>•<strong>挑战2</strong>——<strong>全局变化。</strong> <strong>姿态和成像质量</strong>是全局影响图像中人脸外观的两个主要因素，当人脸的全局结构被错误估计时，会导致很大一部分关键点定位出现偏差。</p>
<p>•<strong>挑战3 <strong>—— <strong>数据不平衡。</strong>在浅层学习和深层学习中，一个</strong>可用的数据集在其类/属性之间呈现不平衡的分布</strong>是很常见的。这种不平衡很可能使算法/模型不能正确地表示数据的特征，从而在不同的属性上提供不令人满意的精度。</p>
<p>原因：在现实生活中，获得完美的脸几乎不可能。换句话说，人脸经常暴露在不受控制甚至不受约束的环境中。在不同的光照条件下，外观有很大的姿态、表情和形状变化，有时伴有部分遮挡。</p>
<p><img src="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220103114028361.png" alt="image-20220103114028361"></p>
<p>上图提供了几个这样的示例。此外，为数据驱动方法提供足够的训练数据也是模型性能的关键。假设出于数据平衡的考虑，在不同的条件下捕捉几个人的面孔可能是可行的，但这种收集方式不现实。</p>
<p>上述挑战大大增加了精确检测的难度，要求检测器具有鲁棒性。</p>
<p>•<strong>挑战4</strong> ——<strong>模型效率</strong>。模型大小和计算要求。机器人、AR和视频聊天等任务需要在配备有限计算和内存资源的平台(如智能手机或嵌入式产品)上实时执行。这一点特别要求关键点检测器具有小的模型尺寸和快速的处理速度。</p>
<h2 id="方法简介"><a href="#方法简介" class="headerlink" title="方法简介"></a>方法简介</h2><p><img src="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220103170252735.png" alt="image-20220103170252735"></p>
<p>•与局部变化相比，全局变化极大地影响整个地标集。为了增强鲁棒性，使用网络上分支来估计每个人脸样本的几何信息，并随后正则化地标定位。辅助网络(上分支)可以输出目标角度，估计三维旋转信息，包括偏航yaw、俯仰pitch和滚转角roll。有了这三个欧拉角，就可以确定头部的姿态。</p>
<p>•预测地关键点的骨干网(下分支)中使用了MobileNet代替了传统卷积，骨干网的负荷大大减少，从而加快了速度。此外，网络可以通过根据用户的需求调整mobilenet的宽度参数来压缩，使模型更小、更快。</p>
<p>上下分支网络设计有效的应对了挑战2和挑战4。</p>
<p>扩大感受野，更好地捕捉人脸的全局结构，增加了一个多尺度全连通层，用于精确定位图像中的关键点。</p>
<h2 id="针对于几何变化和数据不平衡问题设计的loss函数"><a href="#针对于几何变化和数据不平衡问题设计的loss函数" class="headerlink" title="针对于几何变化和数据不平衡问题设计的loss函数"></a>针对于几何变化和数据不平衡问题设计的loss函数</h2><p><img src="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220103174622027.png" alt="image-20220103174622027"></p>
<p>第m个输入的第n个地标，N是每个检测人脸预定义的标定点数量，M表示每个过程中训练图片的个数。$d$代表估计值与真值之间的偏差。</p>
<p><img src="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220103174654579.png" alt="image-20220103174654579"></p>
<p>公式(2)表示地面实况和估计的偏航、俯仰和滚转角之间的偏离角。显然，随着偏离角度的增加，惩罚力度会增加。此外，将样本分为一个或多个属性类，包括侧面、正面、抬头、低头、表情和遮挡。C表示类。</p>
<p>加权参数$ω_c^n$根据属于c类的样本的分数进行调整(论文中采用倒数)。这样设计loss，对于数量少的样本惩罚系数会增大，对于数量多的样本惩罚系数会减小。</p>
<p>很容易得到式(2)中的$\sum_{c=1}^C \sum_{k=1}^K(1−cosθ^k_n)$作为式(1)中的$\gamma_n$。式中$θ^1、θ^2、θ^3$(K=3)表示地真值与估计的偏航角、俯仰角、滚转角的偏差角，当误差大时惩罚系数大，当误差小时惩罚系数小。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220103200847245.png" alt="image-20220103200847245"></p>
<p><img src="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220103200931893.png" alt="image-20220103200931893"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/" data-id="ckxyn55qj0000dkupf5wre0xo" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag">文献阅读</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-人脸关键点检测简介" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%AE%80%E4%BB%8B/" class="article-date">
  <time datetime="2022-01-01T02:45:18.000Z" itemprop="datePublished">2022-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%AE%80%E4%BB%8B/">人脸关键点检测简介</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>人脸关键点检测也被称为人脸关键点定位，旨在定位人脸上预定义的关键点，关键点位于五官和人脸边界上。关键点能够反映各个部位的脸部特征，随着技术的发展和对精度要求的增加，人脸关键点的数量经历了从最初的5个点到如今超过200个点的发展历程。下图分别是68点和98点关键点的标注形式。</p>
<p><img src="https://img-blog.csdnimg.cn/20190103210914208.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://wywu.github.io/projects/LAB/support/WFLW_annotation.png" alt="img"></p>
<p>提取人脸关键点可以获得五官的定位，可以对人脸获得五官patch，可以辅助人脸识别、活体检测等任务。或者是可以对人脸进行对齐，所以人脸关键点定位的相关论文也可以使用face alignment进行检索，计算检测到的人脸关键点到目标平均脸的形变参数，可以用于3维人脸重建。</p>
<h1 id="人脸关键点检测的评价指标："><a href="#人脸关键点检测的评价指标：" class="headerlink" title="人脸关键点检测的评价指标："></a>人脸关键点检测的评价指标：</h1><h2 id="Inter-Ocular-Normalization"><a href="#Inter-Ocular-Normalization" class="headerlink" title="Inter-Ocular Normalization"></a>Inter-Ocular Normalization</h2><p>使用左右眼角的距离对于关键点误差进行归一化，其中$d$为左右眼角的距离，$x$和$x^*$分别为关键点和关键点真值。<br>$$<br>e_i = \frac{∣∣x_i−x_i^∗∣∣_2}{d}<br>$$</p>
<h2 id="Inter-Pupil-Normalization"><a href="#Inter-Pupil-Normalization" class="headerlink" title="Inter-Pupil Normalization"></a>Inter-Pupil Normalization</h2><p>使用瞳间距对于关键点误差进行归一化的公式于使用左右眼角的距离对于关键点误差进行归一化的公式形式一样，不过$d$换成了瞳间距。如果在真值中瞳仁没有标注，就使用眼周关键点求平均值获得眼睛中心的坐标值，计算瞳间距。</p>
<h2 id="Normalization-Mean-Error"><a href="#Normalization-Mean-Error" class="headerlink" title="Normalization Mean Error"></a>Normalization Mean Error</h2><p>归一化平均错误率计算关键点检测平均误差，$N$为整张图片上关键点的个数。<br>$$<br>e = \frac{∑ _{i=1}^N ∣∣x_i−x_i^∗∣∣ _2}{N∗d}<br>$$</p>
<h2 id="Failure-Rate-FR"><a href="#Failure-Rate-FR" class="headerlink" title="Failure Rate (FR)"></a>Failure Rate (FR)</h2><p>是评估定位质量的另一个指标。对于一幅图像，如果NME大于阈值，则认为预测失败。通常使用0.1作为阈值。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%AE%80%E4%BB%8B/" data-id="ckxvubcnk0000esupbg27bsyh" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag">文献阅读</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-人脸关键点检测loss设计" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/" class="article-date">
  <time datetime="2022-01-01T02:29:59.000Z" itemprop="datePublished">2022-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/">人脸关键点检测对heatmap的loss设计</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文主要介绍人脸关键点检测中使用的heatmap。并且会介绍Adaptive Wing Loss这篇文章中针对于heatmap损失函数的设计，以及它的实验结果。</p>
<h2 id="heatmap介绍"><a href="#heatmap介绍" class="headerlink" title="heatmap介绍"></a>heatmap介绍</h2><p><em>参考了[2016CVPRW]Deep Alignment Network: A convolutional neural network for robust face alignment 3.3. Landmark heatmap</em></p>
<p>landmark heatmap 是landmark位置的强度最高，强度随着距离最近的地标的距离增大而减小的图像。<br>$$<br>H(x,y)=\frac{1}{1+min_{s_i}}||(x,y)-s_i||<br>$$<br>$H$是热图，$s_i$是第i个landmark，为了提高性能，可以只在每个地标周围以16为半径的圆圈内计算热图值。</p>
<p>上面的公式中使用了范数，也可在的每个关键点处绘制一个高斯分布来生成的热图。</p>
<h2 id="2019ICCV-Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression"><a href="#2019ICCV-Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression" class="headerlink" title="[2019ICCV]Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression"></a>[2019ICCV]Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression</h2><blockquote>
<p>Xinyao Wang1,2Liefeng Bo2Li Fuxin1</p>
<p>1Oregon State University 2JD Digits<br>{wangxiny, lif}@oregonstate.edu,{xinyao.wang3, liefeng.bo}@jd.com</p>
</blockquote>
<ol>
<li><p>评价：提出了一种新的loss函数，称为Adaptive Wing loss，它具有适应性，这种适应性对前景像素损失的惩罚更大，而对背景像素损失惩罚较小。提出了加权损失图(Weighted Loss Map)，为前景像素和“困难的背景像素”分配较高的权重，以帮助训练过程更多地关注对地标定位至关重要的像素。为了进一步提高面对齐精度，引入了边界预测和基于边界坐标的CoordConv。</p>
</li>
<li><p>针对问题：基于深度网络的热图回归已成为人脸关键点定位的主流方法之一。然而，对于热图回归中损失函数的研究却很少。</p>
</li>
<li><p> 本文的目的：分析了人脸关键点检测问题热图回归的理想损失函数性质，并根据此设计了Adaptive Wing loss。该模型在像素级对真值热图进行更加准确的回归，然后利用预测的热图来推断关键点的位置。</p>
</li>
<li><p>热图分析：</p>
<p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220101171407947.png" alt="image-20220101171407947"></p>
<p>在热图中前景像素(具有正值的像素)，特别是每一个高斯分附近的像素，对于准确定位关键点至关重要。这些像素上即使很小的预测误差也会导致预测偏离正确的模式。这些像素的预测误差即使很小，也会导致预测偏离正确的模式。</p>
<p>相反，准确预测背景像素(零值像素)的值就不那么重要了，因为这些像素上的小误差在大多数情况下不会影响地标的预测。</p>
<p>然而，困难背景像素(图1背景像素接近前景像素区域的像素)的预测精度也很重要，因为它们经常被错误地回归为前景像素，可能导致预测不准确。</p>
</li>
<li><p>heatmap regression中的常用的MSE loss</p>
<p>a. 均方误差对小误差不敏感，影响了对高斯分布模式的正确定位。</p>
<p>b. 在训练过程中，MSE loss对于所有的所有的像素都具有相同的损失函数和相等的权值，但是在热图上，背景像素绝对优于前景像素。</p>
<p>由于a)和b)，使用MSE损失训练的模型所得的结果与ground truth热图相比，前景像素强度会被降低，会获得一张模糊和膨胀的热图。这种低质量的热图可能会导致对面部关键点的错误估计。</p>
<p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220101175929474.png" alt="image-20220101175929474"></p>
</li>
<li><p>网络结构</p>
<p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220101200537435.png" alt="image-20220101200537435"></p>
</li>
<li><p>损失函数的基本原理</p>
<p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220101201422715.png" alt="image-20220101201422715"></p>
<p>•其中，N是训练样本的总数，H、W和C分别是热图的高度、宽度和通道。</p>
<p>•从稳健统计中引入一个概念。影响力是一个启发式工具，在稳健统计中用来研究估计量的性质。</p>
<p>•在收敛时，在收敛时，所有误差的影响必须相互平衡。因此，具有较大梯度幅值的像素的正误差(影响较大)需要用影响较小的许多像素上的负误差来平衡。与梯度较小的错误相比，梯度较大的错误也会在训练中更被关注。如何对小误差有一个相对较高的关注是设计的出发点之一。</p>
</li>
<li><p>不同loss对于大梯度和小梯度误差影响力分析以及连续性分析</p>
<p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220101204052130.png" alt="image-20220101204052130"></p>
<p>a. 常用于热图回归的均方误差损失函数。均方误差损失的梯度是线性的，因此误差小的像素影响小。该属性可能导致训练收敛，而许多像素仍有小误差。因此，用均方误差损失训练的模型倾向于预测模糊和扩大的热图。</p>
<p>b. L1loss具有恒定的梯度，因此误差小的像素与误差大的像素具有相同的影响。然而，L1损失的梯度在零点不是连续的，具有正误差的像素的数量必须恰好等于具有负误差的像素的数量。</p>
<p>c.当误差较大时具有恒定梯度，当误差较小时具有大梯度的wing损失。因此，误差小的像素将被放大。</p>
<p>仍然不能克服其梯度在零点时的不连续性。不适用于热图回归。</p>
<p>因为在所有背景像素上计算wing损失，背景像素上的小误差具有不成比例的影响。训练一个在这些像素上输出零或小梯度的神经网络是非常困难的。</p>
<p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220101204617486.png" alt="image-20220101204617486"></p>
</li>
<li><p>理想的loss设计 </p>
<p>•误差较大</p>
<p>•loss函数具有恒定的影响，这样它将对不准确的注释和遮挡具有鲁棒性。</p>
<p>•误差小</p>
<p>•对于前景像素，影响应该开始增加，以便训练能够集中于减少这些误差。当误差非常接近零时，这种影响应该会迅速减小，这样这些“足够好”的像素将不再被关注。减少正确估计的影响有助于网络保持收敛，而不是像L1和wing loss那样振荡。</p>
<p>•对于背景像素，梯度应该表现得更类似于均方误差损失，即随着训练误差的减小，梯度将逐渐减小到零，因此当误差较小时，影响将相对较小。该属性减少了背景像素上的训练的关注，稳定了训练过程。</p>
</li>
</ol>
<ol start="10">
<li>Adaptive Wing loss设计</li>
</ol>
<p>   <img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220101210145674.png" alt="image-20220101210145674"></p>
<p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220101210329410.png" alt="image-20220101210329410"></p>
<p>​    损耗函数在零点平滑。</p>
<ol start="11">
<li><p>区分前后景像素</p>
<p>在具有64 × 64热图和7×7高斯大小的面部标志定位的典型设置中，前景像素仅构成所有像素的1.2%。为这种不平衡的数据分配相同的权重可能会使训练过程收敛缓慢，从而导致较差的性能。为了进一步建立网络聚焦前景像素和困难背景像素(接近前景像素的背景像素)的能力，引入加权损失图来平衡不同类型像素的损失。前景像素和困难背景像素1，其他像素0。</p>
<p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220101210634320.png" alt="image-20220101210634320"></p>
</li>
</ol>
<h2 id="adapt-wing-loss论文测试效果"><a href="#adapt-wing-loss论文测试效果" class="headerlink" title="adapt_wing_loss论文测试效果"></a>adapt_wing_loss论文测试效果</h2><p><em>此论文只给了测试代码以及预训练好的模型</em></p>
<p>真值绿色，红色为预测值。</p>
<p>下图为nme &gt; 0.1的失败案例。失败的原因是图片本身有遮挡，或者图片清晰度不高，或者是人脸为大角度姿势。</p>
<p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/coding\result_save10\47_Matador_Bullfighter_Matador_Bullfighter_47_772_2485.jpg"></p>
<p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/coding\result_save10\59_peopledrivingcar_peopledrivingcar_59_202_2488.jpg" alt="59_peopledrivingcar_peopledrivingcar_59_202_2488"></p>
<p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/coding\result_save10\61_Street_Battle_streetfight_61_708_2489.jpg" alt="61_Street_Battle_streetfight_61_708_2489"></p>
<p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/coding\result_save10\20_Family_Group_Family_Group_20_667_2486.jpg"></p>
<p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/coding\result_save10\32_Worker_Laborer_Worker_Laborer_32_600_2499.jpg" alt="32_Worker_Laborer_Worker_Laborer_32_600_2499"></p>
<h2 id="数据集与处理方式"><a href="#数据集与处理方式" class="headerlink" title="数据集与处理方式"></a>数据集与处理方式</h2><p>300w的数据集除了private外，没有官方边界。</p>
<p>WFLW提供的边界框不是很准确。</p>
<p>因此在两个维度上都将边界框放大了10%，300W直接裁剪人脸。</p>
<h2 id="数据集WFLW-7500训练-2500测试。"><a href="#数据集WFLW-7500训练-2500测试。" class="headerlink" title="数据集WFLW#7500训练 2500测试。"></a>数据集WFLW#7500训练 2500测试。</h2><p>人脸多种属性、关键点标注数据集，包含了10000张脸，其中7500用于训练，2500张用于测试，共98个关键点。除了关键点之外，还有姿态，表情，光照，妆容，遮挡，模糊。</p>
<p><img src="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20220102110938051.png" alt="image-20220102110938051"></p>
<p>list_98pt_rect_attr_train_test.txt是98X2关键点+4左上右下+6属性+图片名称 = 196 +4+6+1 = 207 list_98pt_test.txt196 + 名字 =197。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/" data-id="ckxvubcnn0001esup7ini72c2" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag">文献阅读</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-”风格造成的关键点检测误差“" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/" class="article-date">
  <time datetime="2021-12-31T12:03:53.000Z" itemprop="datePublished">2021-12-31</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/">风格造成的人脸关键点检测误差</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>由于图片拍摄时候外界环境,比如光线变化等会造成图片”风格”的不同,不同风格的同一张人脸经过landmark detector 可能会获得不同的地标,这并不是由于人脸结构本身造成。对于此问题的研究目前有两篇具有代表性的文章,主要思路都是通过数据增强,对图片进行某种风格变换,再去训练landmark detector,使得检测器对于风格变化具有鲁棒性。从而提高总体landmark检测准确度。</p>
<h2 id="2018CVPR-Style-Aggregated-Network-for-Facial-Landmark-Detection"><a href="#2018CVPR-Style-Aggregated-Network-for-Facial-Landmark-Detection" class="headerlink" title="[2018CVPR]Style Aggregated Network for Facial Landmark Detection"></a>[2018CVPR]Style Aggregated Network for Facial Landmark Detection</h2><blockquote>
<p>Xuanyi Dong1, Yan Yan1, Wanli Ouyang2, Yi Yang1∗<br>1University of Technology Sydney,2The University of Sydney<br>{xuanyi.dong,yan.yan-3}@student.uts.edu.au;<br>wanli.ouyang@sydney.edu.au; yi.yang@uts.edu.au</p>
</blockquote>
<ol>
<li><p>评价：提出了一种对图像风格差异不敏感的风格聚合网络(style - aggregation Network, SAN)人脸地标检测方法。在face landmark detection领域第一个明确了图像风格变化问题造成可能会带来检测失误。</p>
</li>
<li><p>针对问题：不同图像风格差异较大的问题。除了人脸本身的方差外，图像风格的内在方差，如灰度与彩色图像、亮与暗、强烈与暗淡等。landmark检测器对于不同风格的同一张人脸图片有不同的输出。例：三张图片内容完全相同。唯一不同的是形象风格。使用训练好的面部标志检测器来定位面部标志。zoom部分显示不同风格图像上相同面部标志的预测位置之间的偏差。</p>
<p><img src="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211229210439162.png" alt="image-20211229210439162"></p>
</li>
<li><p>本文的目的：提高对图像风格的大方差的鲁棒性。</p>
</li>
<li><p>实现的方法：</p>
<p><img src="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211229212141043.png" alt="image-20211229212141043"></p>
</li>
</ol>
<p>SAN架构概述。网络由两部分组成。第一个是样式聚合的脸生成模块，该模块将输入图像转换成不同的样式，然后将它们组合成样式聚合的脸。</p>
<p>二是人脸地标预测模块。该模块将原始图像和样式聚合图像作为输入，得到两个互补的特征，然后将两个特征进行融合，级联生成热图预测。“fc”意味着fully-convolution。</p>
<p>5.方法简介</p>
<p>数据增强（风格变化）：通过将300-W和AFLW转换成不同的风格，发布了两个新的人脸标志物检测数据集:300W-Styles(≈12000 images)和AFLW- styles(≈80000 images)。</p>
<p>通过生成式对抗模块将原始人脸图像转换为风格聚合图像，使用风格聚合图像使得人脸图像对环境变化更具鲁棒性。</p>
<p>风格互补训练：将原始人脸图像与风格聚合的人脸图像作为双流输入landmark 检测器，二流输入可以互补。</p>
<blockquote>
<p>假如测试阶段移除gan呢，相当于多模态训练，单模态测试？</p>
</blockquote>
<p>在基准数据集AFLW方法表现良好。代码可在GitHub上公开获取:<a target="_blank" rel="noopener" href="https://github.com/D-X-Y/SAN">https://github.com/D-X-Y/SAN</a></p>
<p>6.个人的思考</p>
<p>相当于做了数据增强，增加了数据量。</p>
<h2 id="2019ICCV-Aggregation-via-Separation-Boosting-Facial-Landmark-Detector-with-Semi-Supervised-Style-Translation"><a href="#2019ICCV-Aggregation-via-Separation-Boosting-Facial-Landmark-Detector-with-Semi-Supervised-Style-Translation" class="headerlink" title="[2019ICCV]Aggregation via Separation: Boosting Facial Landmark Detector with Semi-Supervised Style Translation"></a>[2019ICCV]Aggregation via Separation: Boosting Facial Landmark Detector with Semi-Supervised Style Translation</h2><blockquote>
<p>Shengju Qian1, Keqiang Sun2, Wayne Wu2,3, Chen Qian3, Jiaya Jia1,4<br>1The Chinese University of Hong Kong2Tsinghua University<br>3SenseTime Research4Y ouTu Lab, Tencent<br>{sjqian, leojia}@cse.cuhk.edu.hk, skq17@mails.tsinghua.edu.cn,{wuwenyan, qianchen}@sensetime.com</p>
</blockquote>
<ol>
<li><p>评价：</p>
</li>
<li><p>针对问题：鉴于任何人脸图像都可以被分解成光线、纹理和图像环境的风格空间，以及一个风格不变的结构空间，本文的关键想法是利用每个个体的风格和形状空间。[2018CVPR]Style Aggregated Network for Facial Landmark Detection中显式地研究了图像风格带来的畸变现象。</p>
</li>
<li><p>思路：</p>
<p>在实践中，图像内容是指对象、语义和边缘特征，而风格可以是颜色和纹理。</p>
<p>基于人脸地标检测的目的，即通过过滤不受约束的“风格”，回归“人脸内容”，即人脸几何的主成分。定义“style”是指的是图像背景、光线、质量、是否存在眼镜等阻碍探测器识别人脸几何形状的因素。</p>
</li>
<li><p>实现的方法：</p>
<p>利用风格迁移和解纠缠表征学习disentangled representation learning来处理人脸对齐问题，因为风格迁移的目的是在保留内容的同时改变风格。在不使用额外知识的情况下，增加人脸地标检测的训练。</p>
</li>
<li><p>方法简介：</p>
<p>a. 不是直接生成图像来做数据增强，而是首先将人脸图像映射到结构和风格的空间中。</p>
<p>b. 为了保证这两个空间的解纠缠，设计了一个条件变分自编码器模型，该模型将Kullback-Leiber (KL)散度损失和skip连接分别用于风格和结构的紧凑表示。通过分解这些特征，在现有的面部几何图形之间执行视觉风格转换。根据现有的人脸结构，将戴眼镜、质量较差、在模糊或强光下的人脸进行相应的风格，用于进一步训练人脸地标探测器，形成一个较为通用和健壮的人脸几何识别系统。</p>
</li>
</ol>
<p><img src="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211231173635145.png" alt="image-20211231173635145"></p>
<p>框架由两部分组成。一个是学习面部外观和结构的解离化表示，而另一个可以是任何面部标志检测器。</p>
<p>在第一阶段，提出了条件变化自动编码器，用于学习风格和结构之间的分离表示。</p>
<p>在第二阶段，在从其他人脸转换到风格后，带有结构的“风格化”图像可用于提高训练性能和风格不变检测器。</p>
<ol>
<li><p>个人的思考</p>
<p>与第一篇文章中类似，相当于做了数据增强，增加了数据量。不同的是不是显示地改变光线或是rgb图转换成灰度，而是隐式地对于风格特征和结构特征进行了分离，在把一张图片风格迁移到其他风格。</p>
</li>
<li><p>实验效果</p>
<p><img src="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/coding\stylealign结果\test_00075000.png" alt></p>
</li>
</ol>
<p>   <img src="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/coding\stylealign结果\test_01000000.png" alt></p>
<p>   上面两张图 给出了在batch_size=8的情况下，第75个epoch和1000000个 epoch数据增强 的效果。</p>
<p>   最上面第一行是第一列各个图像地关键点，也就是人脸地结构特征。在第二行到第九行中，第一列为原始图像，之后地第二列到第九列为各个原始图像保持原始landmark结构的情况下，生成的具有风格迁移地人脸。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/" data-id="ckxv6sclq00006oup4vsf2wiw" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag">文献阅读</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-人脸关键点检测论文梳理" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/12/28/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/" class="article-date">
  <time datetime="2021-12-28T12:26:26.000Z" itemprop="datePublished">2021-12-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/12/28/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/">人脸关键点检测论文梳理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="常用数据集"><a href="#常用数据集" class="headerlink" title="常用数据集"></a>常用数据集</h1><p><img src="/2021/12/28/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211228205403494.png" alt="image-20211228205403494"></p>
<h1 id="3FabRec-Fast-Few-shot-Face-alignment-by-Reconstruction"><a href="#3FabRec-Fast-Few-shot-Face-alignment-by-Reconstruction" class="headerlink" title="3FabRec: Fast Few-shot Face alignment by Reconstruction"></a>3FabRec: Fast Few-shot Face alignment by Reconstruction</h1><blockquote>
<p>2020CVPR</p>
<p>Bjorn Browatzki and Christian Wallraven* ¨ Dept. of Artificial Intelligence, Korea University, Seoul browatbn@korea.ac.kr, wallraven@korea.ac.kr</p>
</blockquote>
<p>•<strong>人脸关键点定位的**</strong>监督学习<strong><strong>方法需要</strong></strong>大量训练数据<strong><strong>，在特定数据集上由于参数量多会</strong></strong>过拟合**</p>
<p>•<strong>此文**</strong>使用半监督学习方法<strong><strong>在于首先从大量的</strong></strong>无标签图像<strong><strong>要生成</strong></strong>隐藏的人脸知识** </p>
<p>••two-stage architecture 3FabRec达到sota的表现</p>
<p>•在极小训练集（10张图片）可以保持同样的精度</p>
<p>•插入迁移层在解码器上增加少量参数，推断过程在GPU上可以达到数百FPS</p>
<p>•精确关键的人脸定位对于之后的人脸处理很重要。挑战在于克服各种模糊、遮挡、视觉不可见，实现低定位错误，对于人脸变化由高鲁棒性，保证高定位精度。</p>
<p>•多数方法都使用了高度协调的有监督的学习方案，并且几乎总是在要测试的特定数据集上进行特殊优化，从而增加了对该数据集进行过度拟合的可能性。</p>
<p>•不同数据集中的注释可能是不精确且不一致的。</p>
<p>用已经存在的标定好的人脸数据集和其他任务的人脸数据集，学习潜在人脸形状知识可以使得跨数据集间更好的泛化能力.</p>
<h1 id="LUVLi-Face-Alignment-Estimating-Landmarks’-Location-Uncertainty-and-Visibility-Likelihood"><a href="#LUVLi-Face-Alignment-Estimating-Landmarks’-Location-Uncertainty-and-Visibility-Likelihood" class="headerlink" title="LUVLi Face Alignment: Estimating Landmarks’ Location, Uncertainty, and Visibility Likelihood"></a>LUVLi Face Alignment: Estimating Landmarks’ Location, Uncertainty, and Visibility Likelihood</h1><blockquote>
<p>2020 CVPR</p>
<p>Abhinav Kumar∗,1 , Tim K. Marks∗,2 , Wenxuan Mou∗,3 , Ye Wang2 , Michael Jones2 , Anoop Cherian2 , Toshiaki Koike-Akino2 , Xiaoming Liu4 , Chen Feng5 abhinav3663@gmail.com, tmarks@merl.com, wenxuanmou@gmail.com, [ywang, mjones, cherian, koike]@merl.com, liuxm@cse.msu.edu, cfeng@nyu.edu 1University of Utah, 2Mitsubishi Electric Research Labs (MERL), 3University of Manchester, 4Michigan State University, 5New York University</p>
</blockquote>
<p>•现代人脸对齐方法在预测面部标志的位置方面已经非常准确，但通常不会估算其预测位置的不确定性，也无法预测标志是否可见。</p>
<p>•本文提出了一种用深度网络联合预测标志位置、不确定度、标值可见性的新颖框架。</p>
<p>•把以上三点用混合随机变量建模，用提出的Location,Uncertainty, and Visibility Likelihood (LUVLi) loss来训练</p>
<p>•用了全新带标签的68点人脸数据集，数据中还包含了每个标记是否没有被遮挡、是否出现了自遮挡、外部遮挡。</p>
<p>•联合评估的方法产生了对于预测标定点不确定度的精确估计，并且在多个标准人脸配准数据集上达到了SOTA</p>
<p>•对于标定点不确定的估计可以自动合格判定人脸配准是否失败</p>
<p><strong>贡献</strong></p>
<p>•这是引入用于面对齐的参数不确定性估计概念的第一项工作。</p>
<p>•提出了一个端到端可训练模型，用于联合估计地标位置，不确定性和可见性可能性（LUVLi），建模为混合随机变量。</p>
<p>•使用多元高斯和多元拉普拉斯概率分布比较我们的模型。</p>
<p>•算法在多个面部对齐数据集上产生准确的不确定性估计和最新的地标定位结果。</p>
<p>•发布一个新的数据集，其中将以手动方式标记各种姿势中超过19; 000张面部图像上68个地标的位置，其中每个地标也被标记为三种可见性类别之一。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/12/28/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/" data-id="ckxv6sclr00016oup79cmcuo3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag">文献阅读</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-EyediapRGBDgaze" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/12/10/EyediapRGBDgaze/" class="article-date">
  <time datetime="2021-12-10T07:07:56.000Z" itemprop="datePublished">2021-12-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/12/10/EyediapRGBDgaze/">EyediapRGBDgaze</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>Funes Mora K A, Monay F, Odobez J M. Eyediap: A database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras[C]//Proceedings of the Symposium on Eye Tracking Research and Applications. 2014: 255-258.</p>
</blockquote>
<ol>
<li><p>评价：用于开发和评估来自RGB和RGB- d相机的注视估计算法的数据库</p>
</li>
<li><p>针对的问题：缺乏一个共同的基准来评估从RGB和RGB- d数据的凝视估计任务。</p>
</li>
<li><p>本文的目的：引入一个新的数据库和一个共同的框架来克服缺乏一个共同的benchmark这一局限性，用于注视估计方法的训练和评价。可以评估算法鲁棒性 。</p>
<p>i)头部姿势变化;</p>
<p>ii)人变化;</p>
<p>iii)环境和感知条件的变化和</p>
<p>iv)目标类型:屏幕或3D对象</p>
</li>
</ol>
<h1 id="数据集采集设置"><a href="#数据集采集设置" class="headerlink" title="数据集采集设置"></a>数据集采集设置</h1><h2 id="Set-up"><a href="#Set-up" class="headerlink" title="Set-up"></a>Set-up</h2><p><img src="/2021/12/10/EyediapRGBDgaze/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211210155014440.png" alt="image-20211210155014440"></p>
<p>Kinect:这种消费设备提供标准(RGB)和VGA分辨率(640×480)和30fps的深度视频流。</p>
<p>高清摄像头:Kinect设计的视场更大，对用户移动性的限制更少，但这对于基于VGA分辨率的眼球追踪来说是个问题。因此，我们也使用了全高清相机(1920x1080)， 25fps记录场景。</p>
<p>led:两个摄像头都可见的5个led用于同步RGB-D和HD流。</p>
<p>平面屏幕:使用一个24英寸的屏幕来显示一个可视目标。</p>
<p>小球:使用直径为4cm的小球作为视觉目标，有两个目的:在3D环境中作为视觉目标，在RGB和深度数据中都具有识别力，从而可以精确跟踪其3D位置(见第3节)。</p>
<p>如图1所示，摄像头位于电脑屏幕正下方，这样可以从下方观察参与者的眼睛，尽量减少眼皮遮挡。参与者被要求在一定距离内坐在装置前面，这取决于视觉目标的类型(见下一段)，并凝视指定的视觉目标。没有在说话活动、面部表情等方面给出指示。</p>
<h2 id="Recording-sessions"><a href="#Recording-sessions" class="headerlink" title="Recording sessions"></a>Recording sessions</h2><p>为了评估注视估计算法的不同方面，我们设计了一组记录会话，每个会话的特征是四个主要变量的组合，影响注视估计的准确性:视觉目标，头部姿势，参与者和记录条件。这些措施说明如下:</p>
<h3 id="Discrete-screen-target"><a href="#Discrete-screen-target" class="headerlink" title="Discrete screen target"></a>Discrete screen target</h3><p>离散目标(DS)屏幕上,一个小圆匀画每1.1秒随机位置在计算机屏幕上,</p>
<h3 id="Continuous-screen-target"><a href="#Continuous-screen-target" class="headerlink" title="Continuous screen target"></a>Continuous screen target</h3><p>屏幕连续目标(CS),圆圈沿着一个随机2 s轨道移动,以获得注视运动更平稳的例子。</p>
<h3 id="3D-floating-target"><a href="#3D-floating-target" class="headerlink" title="3D floating target"></a>3D floating target</h3><p>3D浮动目标(FT):一个直径4厘米的球挂在一根细线上，这根细线附着在一根棍子上，在摄像机和参与者之间的3D区域内移动。与屏幕上的目标相比，参与者离相机的距离更大(1.2米而不是80-90厘米)，以便有足够的空间让目标移动。</p>
<h2 id="头部姿势。"><a href="#头部姿势。" class="headerlink" title="头部姿势。"></a>头部姿势。</h2><p>为了评估方法对头部姿势的稳健性，要求参与者保持注视视觉目标，同时</p>
<p>(i)面对屏幕保持近似静止的头部姿势(静态情况，S);</p>
<p>(ii)进行头部运动(平移和旋转translation and rotation)，以引入头部姿势变化(Mobile case, M)。</p>
<h2 id="记录的条件"><a href="#记录的条件" class="headerlink" title="记录的条件"></a>记录的条件</h2><p>对于参与者12、13和14，在两种不同的条件下(记为A或B)，一些进程被记录了两次:不同的日子、光照和与相机间的距离</p>
<blockquote>
<p>总结。记录了94个2 - 3分钟的会话，总共获得了超过4小时的数据。每个会话由字符串“P-C-T-H”表示，它指的是参与者 participan id P=(1-16)，记录条件conditions C=(A或B)，使用的目标target T=(DS, CS或FT)和头部姿势 head pose H=(S或M)。录音示例如图2所示。</p>
</blockquote>
<p><img src="/2021/12/10/EyediapRGBDgaze/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211215204745435.png" alt="image-20211215204745435"></p>
<p>记录数据样本使用:a-c) RGB-D相机;d-e)高清相机，原始图像的640×480pixels patch显示用于与VGA分辨率数据进行比较。在这些例子中，参与者注视的是:a,d)头部保持静止的屏幕目标;b)静态头姿的浮动目标;c,e)移动头部时浮动目标。</p>
<h1 id="Data-processing"><a href="#Data-processing" class="headerlink" title="Data processing"></a>Data processing</h1><p>除了原始数据本身，还提供了额外的信息，这些信息对推导地面真相测量至关重要，或者只是对开发数据集和运行实验很有用。如何估计它们的更多细节可以在[Funes Mora et al. 2014]中找到。</p>
<h2 id="RGB-D-sensor-calibration"><a href="#RGB-D-sensor-calibration" class="headerlink" title="RGB-D sensor calibration"></a>RGB-D sensor calibration</h2><p>RGB-D传感器校准。提供了RGB-D立体集成的校准参数，这些参数是使用开源校准工具箱获得的[Herrera C. et al. 2012]。这允许结合RGB-D数据到一个有纹理的3D表面。</p>
<h2 id="RGB-D-to-screen-calibration"><a href="#RGB-D-to-screen-calibration" class="headerlink" title="RGB-D to screen calibration"></a>RGB-D to screen calibration</h2><p>RGB-D屏幕校准。提供摄像机坐标系统(3D)和2D屏幕坐标之间的校准。</p>
<h2 id="RGB-D-and-HD-camera-synchrony-and-calibration"><a href="#RGB-D-and-HD-camera-synchrony-and-calibration" class="headerlink" title="RGB-D and HD camera synchrony and calibration"></a>RGB-D and HD camera synchrony and calibration</h2><p>RGB-D和高清相机同步和校准。由于使用了5个led，高清数据与RGB-D视频流同步[Funes Mora等人，2014]。实现了两摄像机间的标准立体标定。</p>
<h2 id="Head-pose-and-eyes-tracking"><a href="#Head-pose-and-eyes-tracking" class="headerlink" title="Head pose and eyes tracking."></a>Head pose and eyes tracking.</h2><p>对于每个参与者，我们使用[Funes Mora和Odobez 2012]中描述的方法，将一个3D Morphable Model [Paysan et al. 2009]与深度数据拟合，从而创建一个与他/她特定面部形状相对应的3D网格。此外，给定这个模板，我们使用迭代最近点(ICP)算法跟踪3D头部姿势，然后从该算法得到眼球在相机3D空间中的近似位置。</p>
<h2 id="Floating-target-tracking"><a href="#Floating-target-tracking" class="headerlink" title="Floating target tracking"></a>Floating target tracking</h2><p>漂浮目标跟踪。对于使用一个球作为可视目标的记录会话，我们提供了球在每个时间步t的3D中心，使用彩色滤波和ICP拟合计算。</p>
<h2 id="Manual-annotations"><a href="#Manual-annotations" class="headerlink" title="Manual annotations"></a>Manual annotations</h2><p>手工注释。进一步的手工注释，当数据被认为是不可靠的注视估计(和评价)提供。这对应于眨眼的时刻，或者当这个人分心的时候(没有看目标)。手动注释是针对涉及屏幕目标的正面会话进行的。鉴于这些情况的发生率很低，我们目前没有在其他会话中对其进行标记(它将被视为噪音)。不过，如果需要，还可以做进一步的注释。</p>
<h1 id="Considered-tasks"><a href="#Considered-tasks" class="headerlink" title="Considered tasks"></a>Considered tasks</h1><p>文章描述了用于评估注视估计算法的准确性及其对不同变量(如头姿、光照条件等)的鲁棒性的不同评估基准。首先总结评估协议框架的主要元素，包括性能度量，然后列出一组基准。</p>
<h2 id="Evaluation-protocol-and-measures"><a href="#Evaluation-protocol-and-measures" class="headerlink" title="Evaluation protocol and measures"></a>Evaluation protocol and measures</h2><p>本节介绍实验描述中涉及的概念:什么被理解为注视估计算法;训练、测试和评估集的定义 train, test and evaluation sets;性能测量。</p>
<h3 id="Gaze-estimation-algorithm注释估计算法"><a href="#Gaze-estimation-algorithm注释估计算法" class="headerlink" title="Gaze estimation algorithm注释估计算法"></a>Gaze estimation algorithm注释估计算法</h3><p>注视估计的输出取决于应用程序。这里考虑两个非常常见的情况:一个3D gaze ray单位向量;或屏幕坐标系Screen coordinates上的二维像素，这通常用于基于屏幕的HCI应用程序。提供额外信息(摄像机-屏幕校准)，可以从3D凝视射线推断屏幕坐标[Funes Mora等人，2014]。</p>
<p>训练数据集的真值包含3d视线估计p和2d视线估计s，给出一定的参数，p可以从s中计算处理。考虑不同的方法来收集这些训练样本:时间，即数据对应于一个较大的视频的部分;或结构化:训练数据以结构化的方式收集，以满足注视估计算法的特定要求(例如，获取最接近屏幕上预定义数量的特定真值的样本)。</p>
<p>测试集假设它是帧索引范围内较大视频的时间序列。</p>
<p>验证集将用于计算算法性能。这是测试数据的一个子集，通过移除数据或地面真相被破坏的样本，由于眨眼和分心，极端的头部姿势，影响眼睛的可见性(例如，鼻子遮挡)。</p>
<h2 id="Predefined-experimental-protocols"><a href="#Predefined-experimental-protocols" class="headerlink" title="Predefined experimental protocols"></a>Predefined experimental protocols</h2><p>为了在不同的实验条件下比较算法和它们的优点，定义了一组协议，它们的区别主要在于用于训练和测试算法的数据库。注意，该数据集有两种主要类型的视觉目标:3D浮动目标(FT)和屏幕目标(CS或DS)。因此，定义的评价方案可以根据首选的视觉目标而变化。总结如下。更多细节见[Funes Mora et al. 2014]。</p>
<p>Protocol 1:注视估计的准确性。在本方案中，我们评估了一个算法$H$的准确性，在所有参数的最小变化下，不注视变化。更准确地说，对于一个session s，其中唯一的变化是注视本身，我们将训练集定义为s的前半部分。测试集定义为s的后半部分。这种实验的结果是平均角误差$\epsilon$◦。可以根据视觉目标的类型导出相关会话。</p>
<p>Protocol 2:头姿变化的鲁棒性。这里的目的是测量由于头部姿势的变化，凝视的准确性下降了多少。首先进行静态头姿(S)实验，然后进行头姿变化(M)实验，然后报告两种情况下的平均误差以及算法对头姿变化的敏感性。</p>
<p>Protocol 3:个人依赖。目的是评估一种方法推广到不可见用户的效果。这可以通过 leave-one-person-out留一法实验装置来实现。</p>
<p>Protocol 4:环境变化。最后，本案例的目标是研究一种方法在不同条件下的泛化特性。为此，可以对参与者12、13和14进行实验，在不同的设置和光照条件下记录会话。</p>
<h1 id="Evaluation-protocol-example"><a href="#Evaluation-protocol-example" class="headerlink" title="Evaluation protocol example"></a>Evaluation protocol example</h1><p>为了说明数据集的用法，这里我们详细描述了与其中一个基准相关的数据:3D浮动目标的注视估计精度协议(Protocol 1)，并使用RGBD流作为注视估计算法的输入数据。注意，这种配置是数据集中最具挑战性的情况之一，对于这种情况，典型的眼睛图像大小为≈14×10pixels。</p>
<h2 id="头部姿势和3D目标跟踪"><a href="#头部姿势和3D目标跟踪" class="headerlink" title="头部姿势和3D目标跟踪"></a>头部姿势和3D目标跟踪</h2><p>在表1中，我们显示了每个会话对应的帧总数，以及我们能够成功估计头部姿势或视觉目标位置的帧数。注意，头部姿势的召回率很高(99.9%)，因为在本方案中，涉及近额和静态头部姿势的人。由于目标在相机的视野之外，或者太靠近传感器导致缺失深度数据，可视目标位置的召回率较低(69%)。尽管如此，这些数字表明有大量的凝视标记数据可供实验使用(每次记录大约100秒)。</p>
<p><img src="/2021/12/10/EyediapRGBDgaze/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211217152023265.png" alt="image-20211217152023265"></p>
<p>表1:第1 - 3行:记录的视频帧数(Total)，头部姿态或浮动目标位置估计成功。第四到第六行:Protocol 1的训练、测试和评估集的大小。</p>
<p>表2:协议1的注视角度误差比较。“头”的情况下，只使用估计的头部方向作为凝视预测。</p>
<p>Protocol sets每个会话被平均分为两个时间区域:前半部分是训练集，后半部分是测试集。测试集被筛选以定义评估集(下一节将描述标准)。表1显示了每组样本的数量(仅考虑已知头姿和目标位置的样本)</p>
<p>目光估计方法。我们实现了一种基于RGB-D的方法[Funes Mora和Odobez 2012]，该方法依靠RGB-D数据将眼睛图像的视点矫正为典型的头部姿势。我们将这种方法称为位校正自适应线性回归(PRALR)。对每个参与者，从训练集中提取42个样本的注视外观模型。这些样本有规律地分布在注视偏航值之间的±40◦和注视仰角值之间的±30◦。由于PR-ALR是一种基于插值的方法，我们只考虑凝视外观模型中使用的数据的凸壳内的测试样本，即评价集仅由地面真值测量遵循相同的偏航和仰角凝视标准的测试样本组成。</p>
<p>估计精度。为了证明数据的可变性，我们计算了在假设参与者注视前方时获得的注视角度误差，即假设注视方向是由头部姿势方向给出的。实验结果如表2所示。注意，这两种方法都输出3D凝视光线，我们使用相同的评估集，因此这些结果可以直接进行比较。“头部”案例显示的角度误差提供了数据中凝视的大变异性的证据。采用PR-ALR注视估计算法后，注视估计的精度得到了显著提高。</p>
<p>然而，与文献报道的结果相比，误差仍然很高，这主要是由于低分辨率(~ 14×10每只眼睛像素)和低对比度(例如参与者7-A是黑色皮肤)。此外，异常值(眨眼、分心等)还没有从评估集3中剔除。注意，这是我们的数据中最具挑战性的场景之一，这个实验足以演示如何使用数据、定义的协议以及如何描述一个实验。</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们描述了一个新的数据集，用于开发和评估来自RGB或RGB- d数据的注视估计算法，解决了社区对标准化基准的需求。</p>
<p>数据库丰富多样，因为它代表了这项任务的主要挑战。大多数变量(头部姿势，人，条件和目标类型)已经被系统地隔离，目标是恰当地描述注视估计在准确性和稳健性方面的不利条件。</p>
<p>描述了记录方法和所记录数据的摘要。我们还列出了提供给用户的额外信息，如设置校准，目标位置，头部和眼睛跟踪信息。我们更详细地描述了一个作为使用实例的实验。我们相信该数据库对研究人员具有很高的价值，因为它将有助于促进注视估计技术在较少约束条件下的发展。</p>
<h1 id="数据集组织结构"><a href="#数据集组织结构" class="headerlink" title="数据集组织结构"></a>数据集组织结构</h1><p><img src="/2021/12/10/EyediapRGBDgaze/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211217165829214.png" alt="image-20211217165829214"></p>
<h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>Data下文件夹命名格式为“P-C-T-H”，指的是参与者 participan id P=(1-16)，记录条件conditions C=(A或B)，使用的目标target T=(DS, CS或FT)和头部姿势 head pose H=(S或M)。</p>
<p><img src="/2021/12/10/EyediapRGBDgaze/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211230120945151.png" alt="image-20211230120945151"></p>
<p><img src="/2021/12/10/EyediapRGBDgaze/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211217180544505.png" alt="image-20211217180544505"></p>
<p>每个文件夹下包含三个视频，3个视频采集设备的参数文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">rgb_vga_calibration.txt</span><br><span class="line"></span><br><span class="line">[resolution]</span><br><span class="line">640;480</span><br><span class="line">[intrinsics]</span><br><span class="line">522.335571;0.000000;323.681061</span><br><span class="line">0.000000;522.346497;269.110931</span><br><span class="line">0.000000;0.000000;1.000000</span><br><span class="line">[R]</span><br><span class="line">1.000000;-0.000000;-0.000000</span><br><span class="line">-0.000000;-1.000000;-0.000000</span><br><span class="line">-0.000000;-0.000000;-1.000000</span><br><span class="line">[T]</span><br><span class="line">0.000000</span><br><span class="line">0.000000</span><br><span class="line">1.000000</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">rgb_hd_calibration.txt</span><br><span class="line"></span><br><span class="line">[resolution]</span><br><span class="line">1920;1080</span><br><span class="line">[intrinsics]</span><br><span class="line">1485.921875;0.000000;940.767761</span><br><span class="line">0.000000;1497.773804;530.065613</span><br><span class="line">0.000000;0.000000;1.000000</span><br><span class="line">[R]</span><br><span class="line">0.946361;-0.081335;-0.312705</span><br><span class="line">-0.077775;-0.996685;0.023863</span><br><span class="line">-0.313610;0.001738;-0.949550</span><br><span class="line">[T]</span><br><span class="line">0.218694</span><br><span class="line">-0.016453</span><br><span class="line">1.015923</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">depth_calibration.txt</span><br><span class="line"></span><br><span class="line">[resolution]</span><br><span class="line">640;480</span><br><span class="line">[intrinsics]</span><br><span class="line">588.194946;0.000000;310.267731</span><br><span class="line">0.000000;585.303101;248.230743</span><br><span class="line">0.000000;0.000000;1.000000</span><br><span class="line">[R]</span><br><span class="line">0.999919;-0.003616;-0.012239</span><br><span class="line">-0.003528;-0.999968;0.007239</span><br><span class="line">-0.012265;-0.007195;-0.999899</span><br><span class="line">[T]</span><br><span class="line">-0.028705</span><br><span class="line">-0.001298</span><br><span class="line">1.002781</span><br><span class="line">[k_coefficients]</span><br><span class="line">3.1199302673;-0.0028519365</span><br><span class="line">[alpha]</span><br><span class="line">1.1090897322;0.0013763016</span><br><span class="line">[beta] #该项目后面的参数跟的很多很多</span><br><span class="line">0.0000000000;0.0000000000;0.0000000000;0.0000000000;0.0000000000;0.0000000000;0.0000000000;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">screen_coordinates.txt</span><br><span class="line"></span><br><span class="line">Screen target coordinates: 2D coordinates (x,y); 3D coordinates (x,y,z)</span><br><span class="line">0;413.0000;477.0000;0.1165;-0.1441;-0.0632</span><br><span class="line">1;416.0000;474.0000;0.1154;-0.1451;-0.0630</span><br><span class="line">2;419.0000;472.0000;0.1143;-0.1457;-0.0629</span><br><span class="line">3;422.0000;469.0000;0.1132;-0.1467;-0.0627</span><br><span class="line">4;425.0000;467.0000;0.1121;-0.1473;-0.0626</span><br><span class="line">5;429.0000;465.0000;0.1107;-0.1480;-0.0625</span><br><span class="line">6;433.0000;463.0000;0.1092;-0.1486;-0.0625</span><br><span class="line">...</span><br><span class="line">#用于记录目标的坐标</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">head_pose.txt</span><br><span class="line"></span><br><span class="line">Head pose: (Rotation matrix (9 values, row by row);Translation(3 values, x,y,z))</span><br><span class="line">0;0.9997;0.0236;-0.0082;-0.0231;0.9980;0.0588;0.0096;-0.0586;0.9982;-0.0046;0.1084;0.3938</span><br><span class="line">1;0.9997;0.0252;-0.0050;-0.0249;0.9980;0.0578;0.0064;-0.0577;0.9983;-0.0047;0.1090;0.3943</span><br><span class="line">2;0.9995;0.0316;-0.0085;-0.0312;0.9988;0.0387;0.0097;-0.0384;0.9992;-0.0049;0.1102;0.3943</span><br><span class="line">3;0.9996;0.0250;-0.0150;-0.0244;0.9987;0.0441;0.0161;-0.0438;0.9989;-0.0049;0.1091;0.3948</span><br><span class="line">4;0.9997;0.0249;-0.0019;-0.0248;0.9985;0.0485;0.0032;-0.0484;0.9988;-0.0049;0.1090;0.3939</span><br><span class="line">5;0.9998;0.0176;0.0042;-0.0178;0.9991;0.0388;-0.0035;-0.0388;0.9992;-0.0046;0.1103;0.3919</span><br><span class="line">6;0.9997;0.0251;0.0051;-0.0253;0.9990;0.0362;-0.0042;-0.0363;0.9993;-0.0046;0.1107;0.3887</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">eye_tracking.txt</span><br><span class="line"></span><br><span class="line">Eyes tracking: Kinect RGB eyeball center left and right (x_l,y_l,x_r,y_r); Kinect depth eyeball center left and right (x_l,y_l,x_r,y_r); HD camera eyeball center left and right (x_l,y_l,x_r,y_r); 3D position of the eyeballs, left and right (x_l, y_l, z_l, x_r, y_r, z_r)</span><br><span class="line">0;286.1482;120.0683;351.1937;121.9867;306.9435;85.2708;379.7931;86.8933;725.6400;164.9603;876.9300;143.2760;-0.0376;0.1493;0.4766;0.0275;0.1472;0.4773</span><br><span class="line">1;286.3311;119.3493;351.4520;121.4472;307.1886;84.4702;380.1107;86.2908;725.2724;163.1687;876.9486;141.8322;-0.0374;0.1499;0.4773;0.0278;0.1476;0.4778</span><br><span class="line">2;285.9384;119.3542;351.1345;121.8086;306.7900;84.4777;379.8091;86.6937;723.7910;163.4062;875.4026;142.9983;-0.0377;0.1497;0.4780;0.0274;0.1470;0.4786</span><br><span class="line">3;285.1989;120.1658;350.4309;122.0558;305.9690;85.3842;379.0494;86.9733;722.2474;165.7260;873.3084;143.9852;-0.0385;0.1488;0.4780;0.0267;0.1467;0.4791</span><br><span class="line">4;286.2832;120.0966;351.3980;122.2475;307.1400;85.2999;380.0429;87.1811;725.2728;165.0547;877.1600;143.8994;-0.0374;0.1491;0.4774;0.0277;0.1469;0.4776</span><br><span class="line">5;287.0309;120.2445;351.9596;122.0580;307.8890;85.4603;380.5585;86.9690;728.2791;164.9782;880.3108;142.9601;-0.0368;0.1494;0.4759;0.0284;0.1476;0.4757</span><br><span class="line">6;287.5150;120.6915;352.0477;122.9920;308.2452;85.9573;380.4743;88.0112;732.2335;165.5833;883.7582;144.9835;-0.0365;0.1498;0.4729;0.0286;0.1475;0.4726</span><br></pre></td></tr></table></figure>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2><h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><p>1.depth数据的处理，depth相机参数的使用。</p>
<p>2.depth相机参数中的k_coefficients、alpha、beta是啥？</p>
<h1 id="数据预处理程序坐标转化部分"><a href="#数据预处理程序坐标转化部分" class="headerlink" title="数据预处理程序坐标转化部分"></a>数据预处理程序坐标转化部分</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">head_rot = head[<span class="number">1</span>:<span class="number">10</span>]</span><br><span class="line"><span class="comment"># Head pose: (Rotation matrix (9 values, row by row)</span></span><br><span class="line">head_rot = np.array(head_rot).reshape([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line"><span class="comment"># reshape</span></span><br><span class="line">head1 = cv2.Rodrigues(head_rot)[<span class="number">0</span>].T[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#此处的T[0]不知道是啥</span></span><br><span class="line">head2d = dpc.HeadTo2d(head1)</span><br><span class="line"><span class="comment">#HeadTo2d 计算Rodrigues，返回航偏角和俯仰角</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(head2d, end=<span class="string">&quot;------&quot;</span>)</span><br><span class="line"><span class="comment">#输出航偏角和俯仰角</span></span><br><span class="line"></span><br><span class="line">head_rot = np.dot(cam_rot, head_rot) </span><br><span class="line"><span class="comment">#相机旋转X头部旋转,基于任意坐标系为参考的成像几何https://www.zhihu.com/people/zhang-xiao-he-8-15,相当于选择世界坐标系</span></span><br><span class="line">head1 = cv2.Rodrigues(head_rot)[<span class="number">0</span>].T[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#此处计算将前面的计算值覆盖掉了</span></span><br><span class="line">head2d = dpc.HeadTo2d(head1)</span><br><span class="line"><span class="comment">#HeadTo2d 计算Rodrigues，返回航偏角和俯仰角</span></span><br><span class="line"><span class="built_in">print</span>(head2d, end=<span class="string">&quot;------&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># rotate the head into camera coordinate system</span></span><br><span class="line">head_trans = np.array(head[<span class="number">10</span>:<span class="number">13</span>])*<span class="number">1000</span></span><br><span class="line"></span><br><span class="line">head_trans = np.dot(cam_rot, head_trans)</span><br><span class="line">head_trans = head_trans + cam_trans</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the 3d coordinates of origin.</span></span><br><span class="line">anno = anno_info[index]</span><br><span class="line">anno = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">eval</span>, anno.strip().split(<span class="string">&quot;;&quot;</span>)))</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(anno) != <span class="number">19</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[Error anno]&quot;</span>)</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line">anno = np.array(anno)</span><br><span class="line"></span><br><span class="line">left3d = anno[<span class="number">13</span>:<span class="number">16</span>]*<span class="number">1000</span></span><br><span class="line">left3d = np.dot(cam_rot, left3d) + cam_trans</span><br><span class="line">right3d = anno[<span class="number">16</span>:<span class="number">19</span>]*<span class="number">1000</span></span><br><span class="line">right3d = np.dot(cam_rot, right3d) + cam_trans</span><br><span class="line"></span><br><span class="line">face3d = (left3d + right3d)/<span class="number">2</span></span><br><span class="line">face3d = (face3d + head_trans)/<span class="number">2</span></span><br><span class="line"></span><br><span class="line">left2d = anno[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">right2d = anno[<span class="number">3</span>:<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the 3d coordinates of target</span></span><br><span class="line">target = target_info[index]</span><br><span class="line">target = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">eval</span>,target.strip().split(<span class="string">&quot;;&quot;</span>)))</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(target) != <span class="number">6</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[Error target]&quot;</span>)</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line">target3d = np.array(target)[<span class="number">3</span>:<span class="number">6</span>]*<span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># target3d = target3d.T - cam_trans</span></span><br><span class="line"><span class="comment"># target3d = np.dot(np.linalg.inv(cam_rot), target3d)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize the left eye image</span></span><br><span class="line">norm = dpc.norm(center = face3d,</span><br><span class="line">                gazetarget = target3d,</span><br><span class="line">                headrotvec = head_rot,</span><br><span class="line">                imsize = (<span class="number">224</span>, <span class="number">224</span>),</span><br><span class="line">                camparams = camera)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Acquire essential info</span></span><br><span class="line">im_face = norm.GetImage(frame)</span><br><span class="line">gaze = norm.GetGaze(scale=scale)</span><br><span class="line">head = norm.GetHeadRot(vector=<span class="literal">False</span>)</span><br><span class="line">head = cv2.Rodrigues(head)[<span class="number">0</span>].T[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">origin = norm.GetCoordinate(face3d)</span><br><span class="line">rvec, svec = norm.GetParams()</span><br><span class="line"></span><br><span class="line">gaze2d = dpc.GazeTo2d(gaze)</span><br><span class="line">head2d = dpc.HeadTo2d(head)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Crop Eye Image</span></span><br><span class="line">left2d = norm.GetNewPos(left2d)</span><br><span class="line">right2d = norm.GetNewPos(right2d)</span><br><span class="line"><span class="comment">#   im_face = ipt.circle(im_face, left2d, 2)</span></span><br><span class="line"><span class="comment">#im_face = ipt.circle(im_face, [left2d[0]+10, left2d[1]], 2)</span></span><br><span class="line"><span class="comment"># cv2.imwrite(&quot;eye.jpg&quot;, im_face)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">im_left = norm.CropEyeWithCenter(left2d)</span><br><span class="line">im_left = dpc.EqualizeHist(im_left)</span><br><span class="line">im_right = norm.CropEyeWithCenter(right2d)</span><br><span class="line">im_right = dpc.EqualizeHist(im_right)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/12/10/EyediapRGBDgaze/" data-id="ckx01sfd500003wupdq7o5gxf" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/" rel="tag">实验记录</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-web3-0" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/11/19/web3-0/" class="article-date">
  <time datetime="2021-11-19T12:06:24.000Z" itemprop="datePublished">2021-11-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/11/19/web3-0/">web3.0</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>1.什么是web3.0？</p>
<p>web1.0类似于博客，有专门的人边界、pgc发布信息，用户消费信息。</p>
<p>web2.0类似于博客、微博，用户可以read和write，user generate content。</p>
<p>web3.0不仅仅能write，还可以own。现阶段比如微博网红相比于平台只能拥有价值网络的一小部分。web3.0用户类似于股东。</p>
<p>公司以前创始人所有-&gt;创始人+股东-&gt;创始人+股东+员工持股，相当于一个范式的转变。</p>
<p>web3.0user就是investor。</p>
<blockquote>
<p>追星需要投入时间、感情 和金钱，用爱发电。但最后收益的是经济公司。web3.0更多的是付出的人获得收益。</p>
</blockquote>
<p>用户、创始人、员工。</p>
<blockquote>
<p>初始的用户可能对于一个公司的作用很大，但享受到的收益大。</p>
</blockquote>
<p>基于protocol web3.0公司核心团队拿15%-30%已经很多了。</p>
<p>2.web3.0公司治理结构如何保持稳定？</p>
<p>在相当长一个时间段需要一个领导人强势引导。民主会影响效率。这一点需要摸索迭代。</p>
<p>deo decentralized autonomous organization。</p>
<p>3.什么时候对于web3.0感兴趣？</p>
<p>16、17九四时间，比特币冲入2万刀。</p>
<p>看人才净流入、净流出。</p>
<p>杰克·多西、Fred Wilson。</p>
<p>但巴菲特、芒格的反对意见。</p>
<p>defi、普惠式金融。</p>
<p>4.怎么学习web3.0</p>
<p>如果写到教科书里，那就不用学了？</p>
<p>a.钱这东西到底是啥？我们为什么要投资？为什么要攒钱？</p>
<p>b.虚拟货币的发展?为啥中本聪要提出比特币？eth解决了啥？defi解决了啥？有什么新变化？解决了华尔街解决的哪些问题？defi1？defi2。</p>
<blockquote>
<p>了解每个人创新的脉络</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://digitalnative.substack.com/p/chain-reactions-how-creators-web3">https://digitalnative.substack.com/p/chain-reactions-how-creators-web3</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/11/19/web3-0/" data-id="ckx01sfdb00023wup3k499kpv" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Few-Shot-Adaptive-Gaze-Estimation" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/11/18/Few-Shot-Adaptive-Gaze-Estimation/" class="article-date">
  <time datetime="2021-11-18T03:30:41.000Z" itemprop="datePublished">2021-11-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/11/18/Few-Shot-Adaptive-Gaze-Estimation/">Few-Shot Adaptive Gaze Estimation</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>Seonwook Park12*, Shalini De Mello1*, Pavlo Molchanov1, Umar Iqbal1, Otmar Hilliges2, Jan Kautz1<br>1NVIDIA,2ETH Zürich</p>
<p>2019ICCV</p>
</blockquote>
<p>(1) 评价：少样本自适应注视估计。</p>
<p>(2)针对问题：</p>
<p>a.由于个体解剖学之间的差异，基于大量数据训练出的注视估计网络限制了对于个体注视估计的准确性。</p>
<blockquote>
<p>每个人眼睛的生理结构不同，即使处于同一位置的人盯着同一物体，所获得的视线也可能有差异。眼睛类似于相机，每一双眼睛的内参都不一眼。</p>
</blockquote>
<p>b.过度参数化的神经网络并不适合从少数例子中学习，因为它们会很快过度拟合。</p>
<p>(3) 本文的目的：使用小样本自适应凝视估计网络，再少量校准样本情况下处理个性化的凝视估计。</p>
<p>(4)实现的方法：通过一个解耦的编码-解码器架构，以及使用元学习训练的高度适应性的凝视估计器，获得<strong>旋转（rotation aware）感知的凝视潜在表征</strong>。</p>
<p><img src="/2021/11/18/Few-Shot-Adaptive-Gaze-Estimation/faze.png" alt="image-20211118144150719"></p>
<blockquote>
<p>FAZE framework在给定一组具有地面真实注视方向信息的训练图像的基础上，首先学习一种为注视估计任务量身定制的潜在特征表示。考虑到这些特征，然后学习一个适应性强的注视估计网络adaptable gaze estimation network AdaGEN。使用元学习可以很容易地适应一个强大的个人特定的注视估计网络Person-specific gaze estimation network(PS-GEN)，只需很少的校准数据。</p>
</blockquote>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p><a target="_blank" rel="noopener" href="https://github.com/NVlabs/few_shot_gaze">https://github.com/NVlabs/few_shot_gaze</a></p>
<p>The bash script should be self-explanatory and can be edited to replicate the final FAZE model evaluation procedure, given that hardware requirements are satisfied (8x GPUs, where each are Tesla V100 GPUs with 32GB of memory)</p>
<p>需要显卡资源多Orz</p>
<p>We also provide a realtime demo that runs with live input from a webcam in the <code>demo/</code> folder. Please check the separate <a target="_blank" rel="noopener" href="https://github.com/NVlabs/few_shot_gaze/blob/master/demo/README.md">demo instructions</a> for details of how to setup and run it.、</p>
<p>给了实时demo。</p>
<hr>
<p>\1)    评价：贡献创新点。</p>
<p>\2)    针对问题：啥情况啥场景。</p>
<p>\3)    本文的目的：可以做到啥。</p>
<p>\4)    实现的方法：</p>
<p>\5)    方法简介</p>
<p>\6)    方法优化</p>
<p>\7)    方法总结‘</p>
<p>\8)    文章存在的问题</p>
<p>\9)    个人的思考</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/11/18/Few-Shot-Adaptive-Gaze-Estimation/" data-id="ckw4ebw5t0000ogup5l87ay1e" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag">文献阅读</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Appearance-Based-Gaze-Estimation-Using-Dilated-Convolutions" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/11/08/Appearance-Based-Gaze-Estimation-Using-Dilated-Convolutions/" class="article-date">
  <time datetime="2021-11-08T06:19:15.000Z" itemprop="datePublished">2021-11-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/11/08/Appearance-Based-Gaze-Estimation-Using-Dilated-Convolutions/">Appearance-Based Gaze Estimation Using Dilated-Convolutions</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>2018 AACV</p>
<p>Zhaokang Chen[0000−0003−0237−0358]and Bertram E. Shi[0000−0001−9167−7495]<br>The Hong Kong University of Science and Technology, Hong Kong SAR<br>{zchenbc,eebert}@ust.hk</p>
</blockquote>
<hr>
<p> \1)  评价：贡献创新点。</p>
<p>\2)    针对问题：啥情况啥场景。</p>
<p>\3)    本文的目的：可以做到啥。</p>
<p>\4)    实现的方法：</p>
<p>\5)    方法简介</p>
<p>\6)    方法优化</p>
<p>\7)    方法总结‘</p>
<p>\8)    文章存在的问题</p>
<p>\9)    个人的思考</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/11/08/Appearance-Based-Gaze-Estimation-Using-Dilated-Convolutions/" data-id="ckvtao51q00017cupccd6by18" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag">文献阅读</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/" class="article-date">
  <time datetime="2021-11-07T01:52:13.000Z" itemprop="datePublished">2021-11-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/">Multiview Multitask Gaze Estimation With Deep Convolutional Neural Networks</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>2018 TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</p>
<p>Dongze Lian, Lina Hu, Weixin Luo, Y anyu Xu, Lixin Duan, Jingyi Y u,Member , IEEE, and Shenghua Gao.</p>
</blockquote>
<p>(1) 评价：基于深度卷积神经网络的<strong>多视图多任务</strong>注视估计</p>
<p>(2)  针对问题：现有的许多方法都是基于单个摄像机的，大多数方法只关注注视点估计或注视方向估计。</p>
<p>(3) 本文的方法：</p>
<p>a.分析了注视点估计和注视方向估计之间的密切关系，并采用<strong>部分共享卷积神经网络结构</strong>来同时估计注视方向和注视点。</p>
<p>b.引入了一种新的<strong>多视角注视跟踪数据集</strong>，该数据集由<strong>不同被试的多视角注视图像组成</strong>。</p>
<p>c.对于注视方向的预测，提出在左右眼注视方向上引入共面约束。</p>
<p>对于注视点的估计，提出引入一个跨视图池模块。</p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211107213710618.png" alt="image-20211107213710618"></p>
<blockquote>
<p>四流输入、四流输出？怎么共享参数?</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://datasets.d2.mpi-inf.mpg.de/MPIIGaze/MPIIGaze.tar.gz">https://datasets.d2.mpi-inf.mpg.de/MPIIGaze/MPIIGaze.tar.gz</a></p>
<h1 id="数据集ShanghaiTechGaze"><a href="#数据集ShanghaiTechGaze" class="headerlink" title="数据集ShanghaiTechGaze"></a>数据集ShanghaiTechGaze</h1><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211112160847091.png" alt="image-20211112160847091"></p>
<p>设备：使用27英寸的苹果iMac机器作为显示设备。屏幕的宽/高分别为59.77厘米和33.62厘米。然后，屏幕底部部署了三台GoPro Hero 4相机，以捕捉参与者的图像。两台相邻摄像机之间的距离为18.22厘米。</p>
<p>环境：然后，在正常照明条件下，将系统固定在房间的书桌上。为了避免其他移动的物体/人或噪音造成的干扰，房间内保持安静和空无一人，只有一名待命助手除外。</p>
<p>使用大型iMac的第一个原因是，希望在水平和垂直方向上预测更大范围的注视点。相比之下，GazeCapture只预测了手机或平板电脑屏幕内的点。因此，数据集比GazeCapture更具挑战性。</p>
<p>使用iMac的第二个原因是，它的视网膜屏幕有更高的分辨率，以保证像素精度的地面真实;同时，减少了数据采集过程中的眼睛疲劳。</p>
<p>采集过程：要求参与者自由地坐在屏幕前。然后，在灰色背景的屏幕上随机显示一个半径为8像素的白点(作为ground truth)，让参与者用鼠标点击。这样的点击动作可以帮助参与者将注意力吸引到这个点上。然后，记录光标的坐标和动作的时间戳，之后显示光标位置上的蓝点。同时，我们计算白点和蓝点之间的距离。如果距离超过一定的阈值(在我们的设置中是8像素)，则有可能参与者没有盯着白点，因此该数据样本将被丢弃。在采集过程中，GoPro相机被设置为视频模式。根据点击动作的时间戳，我们可以从视频中提取参与者的图像帧。对于每个参与者，在记录数据之前，要求他/她先点击9个点，熟悉数据采集系统。接下来，50个点将在每个环节依次显示给参与者，以进行数据采集。当参与者成功点击上一个点后，屏幕会闪烁，下一个点会显示出来。每个参与者被要求点击12期(总共点击600个点)。在两个周期之间，我们设置了1分钟的休息时间，以避免眼睛疲劳。</p>
<p>我们总共招募了137名学生参与者进行数据收集(年龄在20 — 24岁之间，男性98名，女性39名)。所有参与者视力正常或矫正至正常。在去除白点与蓝点之间距离大于阈值的数据样本后，为每个参与者保留约450-600个点及其对应的眼睛和面孔图像。最后ShanghaiTechGaze数据集由233 796张图像组成。我们进一步使用100个参与者对应的图像作为训练集，其余37个参与者对应的图像作为测试集。</p>
<h2 id="数据组织格式"><a href="#数据组织格式" class="headerlink" title="数据组织格式"></a>数据组织格式</h2><p>—|dataset</p>
<p>——|annotations</p>
<p>———|txtfile</p>
<p>————|test_txt</p>
<p>—————|leftcamera</p>
<p>——————|eyelocation.txt    (images/face_landmarks/leftcamera/00109/00000.mat-images/face_landmarks/leftcamera/00147/00599.mat)21301rows</p>
<p>——————|lefteye.txt    (images/Single_eyes/leftcamera/00109/00000_left.jpg-images/Single_eyes/leftcamera/00147/00599_left.jpg)</p>
<p>——————|righteye.txt    (images/Single_eyes/leftcamera/00109/00000_right.jpg)</p>
<p>—————|middlecamera</p>
<p>——————|eyelocation.txt</p>
<p>——————|lefteye.txt</p>
<p>——————|righteye.txt</p>
<p>—————|rightcamera</p>
<p>——————|eyelocation.txt</p>
<p>——————|lefteye.txt</p>
<p>——————|righteye.txt</p>
<p>—————|gt.txt    (images/coordinate/00109/00000.mat-images/coordinate/00147/00599.mat)</p>
<p>————|train_txt</p>
<p>(images/coordinate/00004/00000.mat-images/coordinate/00108/00599.mat)56631rows</p>
<p>——|images</p>
<p>———|coordinate </p>
<p>————|candidate_index</p>
<p>—————|00000.mat-00599.mat 存放2维数据，真值。</p>
<p>———|face_landmarks（eyelocation，candidate_index，00000.mat-00599.mat）</p>
<p>————|leftcamera存放24维数据</p>
<p>————|middlecamera</p>
<p>————|rightcamera</p>
<p>———|Single_eyes（eye_patch，candidate_index，00000_left.jpg-00599_left.jpg，00000_right.jpg-00599_right.jpg）</p>
<p>————|leftcamera</p>
<p>————|middlecamera</p>
<p>————|rightcamera</p>
<blockquote>
<p>眼睛的landmart位置信息和gt使用.mat格式存储。</p>
</blockquote>
<h1 id="实验code"><a href="#实验code" class="headerlink" title="实验code"></a>实验code</h1><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/mmcodeStruct.png" alt="image-20211111170143842"></p>
<blockquote>
<p>查看mat格式。能不能清晰的读出来，是否需要下载matlab?</p>
<p>dataloader的__getitem__函数在enumerate部分</p>
</blockquote>
<blockquote>
<p>123行 eyelocation = sio.loadmat(eyelocation_name)[‘eyelocation’]是个24个数字</p>
<p>eyelocation_name = ’/data/ShanghaiTechGaze/images/face_landmarks/leftcamera/00061/00157.mat’</p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211116170548765.png" alt="image-20211116170548765"></p>
</blockquote>
<blockquote>
<p>124行gt = sio.loadmat(gt_name)[‘xy_gt’]是两个数字</p>
<p>gt_name = ‘/data/ShanghaiTechGaze/images/coordinate/00061/00157.mat’</p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211116170844442.png" alt="image-20211116170844442"></p>
<p>数据Normalization 是因为是左眼吗所以这么处理。   </p>
<p>gt[0] -= W_screen / 2 宽</p>
<p>​    gt[1] -= H_screen / 2 高</p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211116171147547.png" alt="image-20211116171147547"></p>
<p>第151行 data, target = (input[‘le’], input[‘re’], input[‘eyelocation’]), input[‘gt’]</p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211116172803224.png" alt="image-20211116172803224"></p>
</blockquote>
<h2 id="multi-view-gaze-master-code-Train-Single-View-ST-py"><a href="#multi-view-gaze-master-code-Train-Single-View-ST-py" class="headerlink" title="multi-view-gaze-master/code/Train_Single_View_ST.py"></a>multi-view-gaze-master/code/Train_Single_View_ST.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GazeImageDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, txt_file, txt_dir, transform=<span class="literal">None</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        </span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">train_loader, model, criterion, optimizer, epoch</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">test_loader, model, criterion, epoch, minimal_error</span>):</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AverageMeter</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, val, n=<span class="number">1</span></span>):</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_error</span>(<span class="params">output, target</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_checkpoint</span>(<span class="params">state, filename=<span class="string">&#x27;checkpoint.pth.tar&#x27;</span></span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adjust_learning_rate</span>(<span class="params">optimizer, epoch</span>):</span></span><br></pre></td></tr></table></figure>

<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/%E5%AE%9E%E9%AA%8C%E5%8F%82%E6%95%B0.png" alt="image-20211115131555168"></p>
<h2 id="multi-view-gaze-master-code-network-gazenet-py"><a href="#multi-view-gaze-master-code-network-gazenet-py" class="headerlink" title="multi-view-gaze-master/code/network/gazenet.py"></a>multi-view-gaze-master/code/network/gazenet.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#选对应的resnet</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">single_view</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line"><span class="comment">#在resnet.py中设置好resnet</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#训练过程，向前传递参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, *<span class="built_in">input</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> self.view == <span class="string">&#x27;single&#x27;</span>:</span><br><span class="line">        out = self.single_view(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<p>128,512.  128,512. 128,128. 128,1152.</p>
</blockquote>
<h2 id="single-eye-训练思路"><a href="#single-eye-训练思路" class="headerlink" title="single eye 训练思路"></a>single eye 训练思路</h2><p>在每一个epoch里分别训练和测试，每完成一个epoch训练后，保存权重。</p>
<p>每一次训练，每一个batch（即每一个iteration）中，左右眼分别过Resnet34，得到长度为512输出，24维landmark过线性层得到长度为128的输出，之后这3个输出concatenate成1152的输出。再过fc（两次linear），输出。</p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211116212707500.png" alt="image-20211116212707500"></p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211117164429587.png" alt="image-20211117164429587"></p>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h2 id="路径出错"><a href="#路径出错" class="headerlink" title="路径出错"></a>路径出错</h2><p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks%5CSTpy_101th_row.png" alt="image-20211112150649867"></p>
<p><img src="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211119155759306.png" alt="image-20211119155759306"></p>
<blockquote>
<p>眼睛的位置是瞳仁的中心？还是眼周做平均</p>
</blockquote>
<hr>
<p><a target="_blank" rel="noopener" href="https://github.com/dongzelian/multi-view-gaze">dongzelian/multi-view-gaze: Multi-view gaze estimation (github.com)</a></p>
<p>\1)    评价：贡献创新点。</p>
<p>\2)    针对问题：啥情况啥场景。</p>
<p>\3)    本文的目的：可以做到啥。</p>
<p>\4)    实现的方法：</p>
<p>\5)    方法简介</p>
<p>\6)    方法优化</p>
<p>\7)    方法总结‘</p>
<p>\8)    文章存在的问题</p>
<p>\9)    个人的思考</p>
<p>简要的评价，任务，方法的简要描述。关注文章的动机。 </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/" data-id="ckvtao51l00007cupd8enfwjb" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Blog/">Blog</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/">数学建模</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1/">课程设计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86/">面试经历</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Verilog/" rel="tag">Verilog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/" rel="tag">blog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dsp/" rel="tag">dsp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%9E%E9%AA%8C/" rel="tag">实验</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/" rel="tag">实验记录</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/" rel="tag">数学建模</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" rel="tag">数据处理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag">文献阅读</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%94%9F%E6%B4%BB%E6%80%BB%E7%BB%93/" rel="tag">生活总结</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="tag">论文阅读</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Verilog/" style="font-size: 10px;">Verilog</a> <a href="/tags/blog/" style="font-size: 10px;">blog</a> <a href="/tags/dsp/" style="font-size: 10px;">dsp</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/%E5%AE%9E%E9%AA%8C/" style="font-size: 10px;">实验</a> <a href="/tags/%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/" style="font-size: 10px;">实验记录</a> <a href="/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/" style="font-size: 10px;">数学建模</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" style="font-size: 10px;">数据处理</a> <a href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" style="font-size: 20px;">文献阅读</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 10px;">爬虫</a> <a href="/tags/%E7%94%9F%E6%B4%BB%E6%80%BB%E7%BB%93/" style="font-size: 10px;">生活总结</a> <a href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" style="font-size: 10px;">论文阅读</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">October 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/01/02/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C/">人脸关键点检测的轻量化网络</a>
          </li>
        
          <li>
            <a href="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E7%AE%80%E4%BB%8B/">人脸关键点检测简介</a>
          </li>
        
          <li>
            <a href="/2022/01/01/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8Bloss%E8%AE%BE%E8%AE%A1/">人脸关键点检测对heatmap的loss设计</a>
          </li>
        
          <li>
            <a href="/2021/12/31/%E2%80%9D%E9%A3%8E%E6%A0%BC%E9%80%A0%E6%88%90%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AF%AF%E5%B7%AE%E2%80%9C/">风格造成的人脸关键点检测误差</a>
          </li>
        
          <li>
            <a href="/2021/12/28/%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/">人脸关键点检测论文梳理</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 iszff<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>