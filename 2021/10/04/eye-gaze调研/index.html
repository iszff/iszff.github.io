<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="文献阅读," />





  <link rel="alternate" href="/atom.xml" title="iszff' Blog" type="application/atom+xml" />






<meta name="description" content="Gaze  Estimation简介广义的Gaze Estimation 泛指与眼球、眼动、视线等相关的研究。  不少做saliency和egocentric的论文也以gaze为关键词。 近些年随着数据和技术的发展，对gaze的需求渐渐浮出水面，这方面的研究也开始进入主流的视野。   根据不同的场景与应用大致可分为三类注视目标估计、注视点估计以及三维视线估计。  应用Tobii CEO：AR&#x2F;VR">
<meta property="og:type" content="article">
<meta property="og:title" content="eye gaze调研">
<meta property="og:url" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/index.html">
<meta property="og:site_name" content="iszff&#39; Blog">
<meta property="og:description" content="Gaze  Estimation简介广义的Gaze Estimation 泛指与眼球、眼动、视线等相关的研究。  不少做saliency和egocentric的论文也以gaze为关键词。 近些年随着数据和技术的发展，对gaze的需求渐渐浮出水面，这方面的研究也开始进入主流的视野。   根据不同的场景与应用大致可分为三类注视目标估计、注视点估计以及三维视线估计。  应用Tobii CEO：AR&#x2F;VR">
<meta property="og:locale">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-5d28d5b34067a60edb6b0b56b7ea45bf_1440w.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-502dd1b1678d3bb84d2d51c7955038b5_1440w.jpg">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/tobii.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/objgaze.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/mitgazefollow.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/whataretheylooking">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/tomhanksvedio1">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/tomhanksvedio2">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/gazefollow_vedio.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/vedioprject.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211005204913343.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/vedio_heatmap.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/mitEyeTrackingforEveryone.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/ONdecvice2019CVPRW.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/tabletgaze.png">
<meta property="og:image" content="https://pic3.zhimg.com/v2-1748477bc2558a9442ff18d8280bb79a_r.jpg">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/2017TAPMI_MPIIGaze_pipeline.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211007154253501.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211007160924206.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211007161335847.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211007162046332.png">
<meta property="og:image" content="https://pic4.zhimg.com/v2-baec315548674738560b1acb384e77cf_r.jpg">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211007164953107.png">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-3026afc055f18e5e3e0a0a65f94c4a16_1440w.jpg">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211007170804421.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/MPIIGaze.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211007202852686.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211014172151988.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211014190045184.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211014201017047.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211014195732273.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211014201322789.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211015104645809.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211015104707764.png">
<meta property="og:image" content="https://pic1.zhimg.com/v2-1f1ed9ae231cd771a1da6a584a947964_r.jpg">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211015142501754.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211015143935895.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211014172151988.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211007202852686.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211015152031823.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211007142949977.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211006164028365.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211006164223297.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211008155155643.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211008155246008.png">
<meta property="og:image" content="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211008163556619.png">
<meta property="article:published_time" content="2021-10-04T08:13:29.000Z">
<meta property="article:modified_time" content="2021-12-16T08:26:40.234Z">
<meta property="article:author" content="iszff">
<meta property="article:tag" content="文献阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic4.zhimg.com/80/v2-5d28d5b34067a60edb6b0b56b7ea45bf_1440w.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2021/10/04/eye-gaze调研/"/>





  <title>eye gaze调研 | iszff' Blog</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?5b9fc7fd579b171d84d00067691ad8ab";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




<meta name="generator" content="Hexo 5.4.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">iszff' Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">people move on.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="iszff' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">eye gaze调研</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-10-04T16:13:29+08:00">
                2021-10-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Gaze-Estimation"><a href="#Gaze-Estimation" class="headerlink" title="Gaze  Estimation"></a>Gaze  Estimation</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>广义的Gaze Estimation 泛指与<strong>眼球</strong>、<strong>眼动、视线</strong>等相关的研究。</p>
<blockquote>
<p>不少做saliency和egocentric的论文也以gaze为关键词。</p>
<p>近些年随着数据和技术的发展，对gaze的需求渐渐浮出水面，这方面的研究也开始进入主流的视野。</p>
</blockquote>
<p><img src="https://pic4.zhimg.com/80/v2-5d28d5b34067a60edb6b0b56b7ea45bf_1440w.jpg" alt="img"></p>
<h2 id="根据不同的场景与应用大致可分为三类"><a href="#根据不同的场景与应用大致可分为三类" class="headerlink" title="根据不同的场景与应用大致可分为三类"></a>根据不同的场景与应用大致可分为三类</h2><p>注视目标估计、注视点估计以及三维视线估计。</p>
<p><img src="https://pic2.zhimg.com/80/v2-502dd1b1678d3bb84d2d51c7955038b5_1440w.jpg" alt="img"></p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>Tobii CEO：AR/VR的未来形态将广泛结合眼球追踪<a target="_blank" rel="noopener" href="https://www.bilibili.com/read/cv13178442?from=articleDetail&amp;spm_id_from=333.976.b_726561645265636f6d6d656e64496e666f.2">Tobii CEO：AR/VR的未来形态将广泛结合眼球追踪 - 哔哩哔哩 (bilibili.com)</a></p>
<h3 id="用Tobii眼动仪玩游戏的Demo"><a href="#用Tobii眼动仪玩游戏的Demo" class="headerlink" title="用Tobii眼动仪玩游戏的Demo"></a>用Tobii眼动仪玩游戏的Demo</h3><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/tobii.png" alt></p>
<h3 id="VR"><a href="#VR" class="headerlink" title="VR"></a>VR</h3><p>VR头盔。现阶段VR的问题是全场景精细渲染对硬件要求较高导致硬件成本居高不下。如果能够通过头盔内置摄像头准确估计人的视线方向，则可以对场景做局部精细渲染，即仅对人注视范围内的场景精细渲染，从而大大降低硬件成本。</p>
<h3 id="医疗"><a href="#医疗" class="headerlink" title="医疗"></a>医疗</h3><p>gaze在医疗方面的应用主要是两类。一类是用于检测和诊断精神类或心理类的疾病。一个典型例子是自闭症儿童往往表现出与正常儿童不同的gaze行为与模式。另一类是通过基于gaze的交互系统来为一些病人提供便利。如渐冻症患者可以使用眼动仪来完成一些日常活动。</p>
<h3 id="辅助驾驶（智能座舱）"><a href="#辅助驾驶（智能座舱）" class="headerlink" title="辅助驾驶（智能座舱）"></a>辅助驾驶（智能座舱）</h3><p>gaze在辅助驾驶上有两方面应用。一是检测驾驶员是否疲劳驾驶以及注意力是否集中。二是提供一些交互从而解放双手。</p>
<h3 id="线下零售"><a href="#线下零售" class="headerlink" title="线下零售"></a>线下零售</h3><p>人的注意力某种程度上反映了其兴趣，可以提供大量的信息。但是目前并没有看到相关的应用，包括Amazon Go。或许现阶段精度难以达到要求。可以通过gaze行为做市场调研。</p>
<h3 id="其他交互类应用"><a href="#其他交互类应用" class="headerlink" title="其他交互类应用"></a>其他交互类应用</h3><p>如手机解锁、短视频特效等。</p>
<h2 id="相关团队与公司"><a href="#相关团队与公司" class="headerlink" title="相关团队与公司"></a>相关团队与公司</h2><p>ETH的Otmar Hilliges教授和东京大学的Yusuke Sugano教授。</p>
<p>- EPFL与Idiap的感知组：<a href="https://link.zhihu.com/?target=https%3A//www.idiap.ch/~odobez/">https://www.idiap.ch/~odobez/</a></p>
<ul>
<li>Improving Few-Shot User-Specific Gaze Adaptation via Gaze Redirection Synthesis, CVPR 2019</li>
<li>Unsupervised Representation Learning for Gaze Estimation, CVPR 2020</li>
<li>A Differential Approach for Gaze Estimation, PAMI accepted 2019</li>
</ul>
<p>- ETH交互组：<a href="https://link.zhihu.com/?target=https%3A//ait.ethz.ch/people/hilliges/">https://ait.ethz.ch/people/hill</a></p>
<p>- 德国马普所交互组：<a href="https://link.zhihu.com/?target=https%3A//perceptualui.org/people/bulling/">https://perceptualui.org/people</a></p>
<p>- MIT Antonio Torralba组：<a href="https://link.zhihu.com/?target=http%3A//web.mit.edu/torralba/www/">http://web.mit.edu/torralba/www/</a></p>
<p>- 伦斯勒理工Qiang Ji组：<a href="https://link.zhihu.com/?target=https%3A//www.ecse.rpi.edu/~qji/">https://www.ecse.rpi.edu/~qji/</a></p>
<p>- 东京大学Sugano组：<a href="https://link.zhihu.com/?target=https%3A//www.yusuke-sugano.info/">https://www.yusuke-sugano.info/</a></p>
<p>- 北航Feng Lu组：<a href="https://link.zhihu.com/?target=http%3A//phi-ai.org/default.htm">http://phi-ai.org/default.htm</a></p>
<p>工业界方面，目前主力依旧在欧美。大公司，如Facebook Reality Lab（去年组织举办了第一届gaze相关的challenge）， 微软Hololens，谷歌广告，NVIDIA自动驾驶等团队都在致力于gaze方面的研究。而专注于gaze的中小型公司，龙头老大当属瑞典公司Tobii，其眼动仪已臻物美价廉之境。另外也可以关注下瑞士创业公司eyeware。</p>
<h1 id="注视目标估计"><a href="#注视目标估计" class="headerlink" title="注视目标估计"></a>注视目标估计</h1><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/objgaze.png" alt></p>
<p>注视目标估计英文关键词为gaze following，即检测给定人物所注视的目标。MIT Antonio Torralba组最先提出了这一问题并公开了相关数据集[1]。</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/mitgazefollow.png" alt></p>
<blockquote>
<p>我们引入了一个新的数据集，用于在自然图像中跟踪视线。在左边，我们展示了几个示例注释和图像。在右边的图中，我们总结了一些关于数据集测试分区的统计信息。前三张热图显示了头部位置、固定位置和固定位置相对于头部位置归一化的概率密度。下图显示了不同头部位置的平均凝视方向。</p>
</blockquote>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/whataretheylooking" alt="image-20211005193952263"></p>
<center><p>Where are they looking?</p></center>


<p>网络主要由两个支路组成，一个支路（Saliency Pathway）以原始图片为输入，用于显著性检测，输出为反映显著性的heat map。另一个支路（Gaze Pathway）以某一个人的头部图片和头部位置为输入，用于检测这个人可能的注视区域，输出同样是一个heat map。这两个heat map的乘积反映了目标显著性与可能的注视区域的交集，即可能的注视目标。整个网络以端对端的方式训练。以AUC为指标，文章最后得到了0.878的精度。</p>
<p>这种结构设计也适用于多人注视目标的检测。只需要将Gaze Pathway中的头部图片与位置更换为另一个人的即可。然而这种方案的一大局限是，人与其注视的目标必须同时出现在同一张图片中。这大大限制了其应用范围。</p>
<p>作者们在ICCV 2017提出了针对视频的跨帧注视目标检测[2]，即人与注视目标可出现在不同视频帧中，如下图所示。</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/tomhanksvedio1" alt="image-20211005203207646"></p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/tomhanksvedio2" alt="image-20211005203432979"></p>
<center><p>Following Gaze in Video</p></center>


<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/gazefollow_vedio.png" alt="image-20211005204012006"></p>
<p>整个框架由三条支路构成。与之前框架相比，现在的方案增加了一个Transformation Pathway，用于估计source frame（人所在帧）与target frame（目标所在帧）的几何变换。而现在的Gaze Pathway则用于估计一个视锥的参数。这两路网络的输出表示source frame中的人可能注视的target frame区域。下面的图更为直观。</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/vedioprject.png" alt="image-20211005204541675"><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211005204913343.png" alt="image-20211005204913343"><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/vedio_heatmap.png" alt="image-20211005205031681"></p>
<blockquote>
<p>注视点估计的相关工作（这个方向的论文同样不多），然后再介绍gaze领域的重点研究对象，三维视线估计。</p>
</blockquote>
<h1 id="注视点估计"><a href="#注视点估计" class="headerlink" title="注视点估计"></a>注视点估计</h1><p>注视点估计即估算人双目视线聚焦的落点。</p>
<p>其一般场景是估计人在一个二维平面上的注视点。</p>
<p>这个二维平面可以是手机屏幕，pad屏幕和电视屏幕等，而模型输入的图像则是这些设备的前置摄像头。</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/mitEyeTrackingforEveryone.png" alt="这里放图片显示不出的时候出现的文字" style="zoom:这里写缩放的百分比，比如:30%"></p>
<center><p>Eye Tracking for Everyone MIT 2016CVPR</p></center>


<p>这个工作同样来自MIT Antonio Torralba组。</p>
<p>他有四个输入，左眼图像、右眼图像、人脸图像（由iPhone拍照软件检测）以及人脸位置。</p>
<p>四种输入由四条支路（眼睛图像的支路参数共享）分别处理，融合后输出得到一个二维坐标位置。</p>
<p>实验表明，模型在iPhone上的误差是1.71cm，而在平板上的误差是2.53cm（误差为标注点与估计点之间的欧式距离）。该工作收集并公布了一个<strong>涵盖1400多人、240多万样本</strong>的数据集， <strong>GazeCapture</strong>。</p>
<p>人脸主要提供头部姿态信息（head pose），而人脸位置主要提供眼睛位置信息。这里存在一定的信息冗余。基于这一观察，Google对上述模型做了进一步压缩，即将人脸和人脸位置这两个输入替换为四个眼角的位置坐标，如下图所示。</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/ONdecvice2019CVPRW.png" alt="image-20211006143655188"></p>
<center><p>On-device few-shot personalization 
for real-time gaze estimation Google 2019ICCVW</p></center>



<p>眼角位置坐标不仅直接提供了眼睛位置信息，同时又暗含head pose信息（眼角间距越小，head pose越大，反之头部越正）。实验结果表明，这个精简后的模型在iPhone上的误差为1.78cm，与原始模型的精度相差无几。同时，该模型在Google Pixel 2 Phone的处理速度达到10ms/帧。</p>
<p>三星在2019年也公开了相关研究A Generalized and Robust Method<br>Towards Practical Gaze Estimation on Smart Phone.。他们采取的网络架构与[1]类似，不同点是在网络训练过程中加入了distillation与pruning等技巧，来防止过拟合并获得更鲁棒的结果。</p>
<blockquote>
<p>蒸馏了什么？</p>
</blockquote>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/tabletgaze.png" alt="image-20211006160228094"></p>
<center><p>TabletGaze2015</p></center>

<p>2015年莱斯大学已公开了一篇针对平板的注视点估计论文TabletGaze[4]。但当时的深度学习还不像今天这样盛行，作者使用了传统特征（LBP、HOG等）+ 统计模型的方式来解决这一问题.</p>
<h1 id="三维视线估计（通用方法）"><a href="#三维视线估计（通用方法）" class="headerlink" title="三维视线估计（通用方法）"></a>三维视线估计（通用方法）</h1><p>三维视线估计的目标是从眼睛图片或人脸图片中推导出人的视线方向。通常，这个视线方向是由两个角度，<strong>pitch（垂直方向）和 yaw（水平方向）</strong>来表示的，见下图a。需要注意的是，在相机坐标系下，视线的方向<strong>不仅取决于眼睛的状态（眼珠位置，眼睛开合程度等）</strong>，还取决于<strong>头部姿态</strong>（见图b：虽然眼睛相对头部是斜视，但在相机坐标系下，他看的是正前方)。</p>
<p><img src="https://pic3.zhimg.com/v2-1748477bc2558a9442ff18d8280bb79a_r.jpg" alt="preview"></p>
<h2 id="传统方法："><a href="#传统方法：" class="headerlink" title="传统方法："></a>传统方法：</h2><p>视线估计方法分为：</p>
<p>基于几何的方法（Geometry Based Methods）。</p>
<p>基于外观的方法（Appearance Based Methods）两大类。</p>
<p>基于几何的方法的基本思想是检测眼睛的一些特征（例如眼角、瞳孔位置等关键点），然后根据这些特征来计算gaze。而基于外观的方法则是<strong>直接学习一个将外观映射到gaze的模型</strong>。</p>
<p>几何方法相对更准确，且对不同的domain表现稳定，然而这类方法<strong>对图片的质量和分辨率</strong>有很高的要求。</p>
<blockquote>
<p>基于几何特征。对于不同的domain稳定，此处的domain指的是？</p>
</blockquote>
<p>基于外观的方法对低分辨和高噪声的图像表现更好，但<strong>模型的训练需要大量数据，并且容易对domain overfitting</strong>。</p>
<blockquote>
<p>appearance</p>
</blockquote>
<p>随着深度学习的崛起以及大量数据集的公开，基于外观的方法越来越受到关注。</p>
<p><strong>通用（person independent）的视线估计方法</strong>，即模型的训练数据与测试数据采集自不同的人（与之相对的是<strong>个性化视线估计</strong>，即训练数据与测试数据采集自相同的人）。按照方法所依赖的信息，将他们分类为<strong>单眼/双眼视线估计</strong>，<strong>基于语义信息的视线估计</strong>和<strong>全脸视线估计</strong> 三类。</p>
<h1 id="单眼-双眼视线估计："><a href="#单眼-双眼视线估计：" class="headerlink" title="单眼/双眼视线估计："></a>单眼/双眼视线估计：</h1><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/2017TAPMI_MPIIGaze_pipeline.png" alt="image-20211007135523674"></p>
<blockquote>
<p>多个模态，有哪些模态？  </p>
</blockquote>
<p>他们当时使用的是一个类似于LeNet的浅层架构，还称不上“深度”学习。而其中一个有启发性的贡献是，他们将<strong>头部姿态（head pose）信息与提取出的眼睛特征拼接</strong>，用以学习相机坐标系下的gaze。该工作的另一个重要贡献是提出并公开了gaze领域目前最常用的数据集之一：MPIIGaze。在MPIIGaze数据集上，该工作的误差为6.3度。</p>
<p>Xucong Zhang在他2017年的工作中[2]，用VGG16 代替了这个浅层网络，大幅提升了模型精度，将误差缩小到了5.4度。</p>
<p>上面两个工作都以单眼图像为输入，没有充分利用<strong>双眼的互补信息</strong>。北航博士Yihua Cheng在ECCV 2018上提出了一个基于双眼的非对称回归方法[3]。其方法框图如下：</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211007154253501.png" alt="image-20211007154253501"></p>
<center><p>Appearance-based gaze estimation via evaluation- guided asymmetric regression ECCV2018</p></center>

<p>AR-Net（非对称回归网络），以双眼为输入，经四个支路处理后得<strong>到两只眼睛的不同视线方向</strong>；E-Net（评价网络），同样以双眼为输入，输出是两个权重，用于加权AR-Net训练过程中两只眼睛视线的loss。其基本思想是，双眼中某一只眼睛可能因为一些原因（如光照原因等），更容易得到精准的视线估计，因此在AR-Net训练中应该赋予这只眼睛对应的loss更大的权重。该工作在MPIIGaze数据集上取得了5.0度的误差。</p>
<h1 id="基于语义信息的视线估计"><a href="#基于语义信息的视线估计" class="headerlink" title="基于语义信息的视线估计"></a>基于语义信息的视线估计</h1><p>基于几何的方法是通过检测眼睛特征，如关键点位置，来估计视线的。</p>
<blockquote>
<p>几何检测眼睛的几何信息。</p>
<h2 id="Deep-Pictorial-Gaze-Estimation-2018ECCV"><a href="#Deep-Pictorial-Gaze-Estimation-2018ECCV" class="headerlink" title="Deep Pictorial Gaze Estimation 2018ECCV"></a>Deep Pictorial Gaze Estimation 2018ECCV</h2><p>这启发了一部分工作使用<strong>额外的语义信息</strong>来帮助提升视线估计的精度。ETH博士Park等在ECCV 2018上提出了一种基于眼睛图形表示的视线估计方法。</p>
</blockquote>
<p>通过深度网络将眼睛抽象为一个眼球图形表示来提升视线估计（这一表示相对gaze来说更具象也更易学习）。其中，眼球图形表示这一监督信号是由视线的ground truth经几何方法反推生成的。</p>
<p>不是直接回归两个角度的俯仰和偏航的眼球，而是回归到一个中间的图像表示，这反过来简化了三维注视方向估计的任务。</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211007160924206.png" alt="image-20211007160924206">  </p>
<center><p>Deep Pictorial Gaze Estimation 2018ECCV </p></center> 

<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211007161335847.png" alt="image-20211007161335847"></p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211007162046332.png" alt="image-20211007162046332"></p>
<p>ETH在ETRA 2018上的工作[5]，利用眼睛关键点的heat map估计视线。方法框架如下图所示，其中眼睛关键点这一监督信息由合成数据集UnityEyes提供，这里不展开了。</p>
<p><img src="https://pic4.zhimg.com/v2-baec315548674738560b1acb384e77cf_r.jpg" alt="preview"></p>
<h2 id="Deep-multitask-gaze-estimation-with-a-constrained-landmark-gaze-model俞雨"><a href="#Deep-multitask-gaze-estimation-with-a-constrained-landmark-gaze-model俞雨" class="headerlink" title="Deep multitask gaze estimation with a constrained landmark-gaze model俞雨"></a>Deep multitask gaze estimation with a constrained landmark-gaze model俞雨</h2><p>下一篇论文<img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211007164953107.png" alt="image-20211007164953107"></p>
<center><p> Deep multitask gaze estimation with a constrained landmark-gaze model 2018 ECCVW</p></center>

<p>(a)从UnityEyes地标集(顶部)选择地标(底部)。(b)地标水平或垂直位置与凝视偏航或俯仰角度之间的相关系数。(c)<strong>虹膜中心水平</strong>或垂直位置和偏航或俯仰凝视角度的联合分布图。</p>
<p>俞雨组2018年提出了一种基于约束模型的视线估计方法[6]，其基本出发点是<strong>多任务学习</strong>的思想，即在<strong>估计gaze的同时检测眼睛关键点位置</strong>，两个任务<strong>同时学习，信息互补</strong>，可以在一定程度上得到共同提升。</p>
<ul>
<li><p><strong>首先对眼睛关键点与视线的关系进行了统计建模</strong>。具体地，对于合成数据集UnityEyes中的一个样本，我们抽取其17个眼睛关键点与两个gaze角度，将他们展开并拼接为一个36维（17<em>2 + 2）的向量。然后将n个样本对应的n个向量堆叠生成一个n</em>36大小的矩阵，对该矩阵做PCA分解后可以得到一个mean shape和一系列deformation basis。</p>
<p>这两个部分共同构成了一个约束模型。其中mean shape代表眼睛的平均形状以及对应的gaze平均值，而deformation basis则包含了眼睛形状与gaze的协同变化信息。约束模型的建立过程如下图所示。</p>
</li>
<li><p><img src="https://pic3.zhimg.com/80/v2-3026afc055f18e5e3e0a0a65f94c4a16_1440w.jpg" alt="img"></p>
</li>
</ul>
<p>对于一个输入样本，如果能学习出这个约束模型中的deformation basis系数，并与mean shape组合，就可以重建出这个样本的眼睛形状和gaze。</p>
<p>因此我们使用的网络架构如下，网络的主要<strong>输出即约束模型的系数，用以重建眼睛的形状和gaze</strong>。另外，由于约束模型表示的眼睛形状是经过归一化操作的，网络同时学习一个缩放系数，和一个平移向量，通过几何变换（decoder）得到正确的眼睛关键点位置。网络通过优化关键点位置loss与视线loss实现end to end training。实验结果表明，取得了比直接回归更精准的结果。</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211007170804421.png" alt="image-20211007170804421"></p>
<center><p>Deep multitask gaze estimation with a constrained landmark-gaze model 2018ECCVW</p></center>

<blockquote>
<p>线性重建的方法类似于人脸重建？</p>
</blockquote>
<h1 id="全脸视线估计"><a href="#全脸视线估计" class="headerlink" title="全脸视线估计"></a>全脸视线估计</h1><h2 id="Real-World-Dataset-and-Deep-Appearance-Based-Gaze-Estimation"><a href="#Real-World-Dataset-and-Deep-Appearance-Based-Gaze-Estimation" class="headerlink" title="Real-World Dataset and Deep Appearance-Based Gaze Estimation."></a>Real-World Dataset and Deep Appearance-Based Gaze Estimation.</h2><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/MPIIGaze.png" alt="image-20211007135414078"></p>
<center><p>MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation  2017 TPAMI
    单目相机进行凝视估计
    </p></center>


<h3 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h3><ul>
<li><p>面部landmark检测，(眼睛和嘴角）（Baltruˇsaitis et al.</p>
</li>
<li><p>通过使用EPnP算法[21]估计初始解来拟合模型，并通过非线性优化进一步<strong>精炼姿态</strong>。<strong>三维头部旋转</strong>定义为从<strong>头部坐标系到摄像机坐标系的旋转camera calibration</strong>，眼睛位置定义为每只眼睛的眼角中点。</p>
</li>
<li><p>虽然之前的作品假设了准确的头部姿态，但我们使用一个通用。平均面部形状模型进行三维姿态估计<strong>generic face model</strong>，以评估实际环境中的整个凝视估计管道。</p>
</li>
<li><p>在收集数据之前，使用外部立体摄像机记录所有参与者的6个地标的3D位置，并建立通用形状作为所有参与者的平均形状。</p>
</li>
</ul>
<h2 id="It’s-Written-All-Over-Y-our-Face-Full-Face-Appearance-Based-Gaze-Estimation-CVPR2017"><a href="#It’s-Written-All-Over-Y-our-Face-Full-Face-Appearance-Based-Gaze-Estimation-CVPR2017" class="headerlink" title="It’s Written All Over Y our Face:Full-Face Appearance-Based Gaze Estimation CVPR2017"></a>It’s Written All Over Y our Face:Full-Face Appearance-Based Gaze Estimation CVPR2017</h2><p>以上视线估计方法都要求单眼/双眼图像为输入，有两个缺陷：</p>
<p>1）需要额外的模块检测眼睛；</p>
<p>2）需要额外的模块估计头部姿态。</p>
<p>基于此，Xucong Zhang等于2017年提出了基于注意力机制的全脸视线估计方法[7]。<img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211007202852686.png" alt="image-20211007202852686"></p>
<p>这里注意力机制的主要思想是<strong>通过一个支路学习人脸区域各位置的权重</strong>，其目标是增大眼睛区域的权重，抑制其他与gaze无关的区域的权重。网络的输入为人脸图像并采用end to end的学习策略，直接学习出最终相机坐标系下的gaze。这一工作同时公开了全脸视线数据集MPIIFaceGaze。</p>
<p>然MPIIGaze与MPIIFaceGaze使用的是同一批数据，但并不是同一个数据集（许多论文把这两个数据集混淆）。首先MPIIGaze数据集并不包含全脸图片，其次MPIIFaceGaze的ground truth定义方式与MPIIGaze不同。该工作最终在MPIIFaceGaze数据集上取得了4.8度的精度。</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211014172151988.png" alt="image-20211014172151988"></p>
<p>与上面工作不同的是，除人脸输入外，该工作同时要求输入眼睛图片，如图所示。该工作主要认为工作[1]中gaze特征与head pose拼接的方式并不能准确地反映两者的的几何关系。因此，该工作提出了一个gaze的几何变换层，用于将head pose（人脸支路学习得到）与人脸坐标系下的gaze（眼睛支路学习得到）进行几何解析，得到最终相机坐标系下的gaze。该工作在自己收集的数据集上取得了4.3度的误差。</p>
<p>不知各位读者发现没有，在person independent（训练数据与测试数据采集自不同的人）这一设定下，上述方法的精度大都在4-5度之间徘徊，似乎很难得到进一步的提升。这个<strong>瓶颈主要是由人的眼球内部构造造成的</strong>。如果希望继续提升精度，一般要使用个性化策略。这一部分内容准备在下下一个篇章中讲解。在下一篇章中，我会简要介绍三维视线数据如何收集标注的问题，以及如何在数据集短缺的情况下，训练一个gaze模型。</p>
<h1 id="视线估计-Gaze-Estimation-简介-五-三维视线估计（数据集问题）"><a href="#视线估计-Gaze-Estimation-简介-五-三维视线估计（数据集问题）" class="headerlink" title="视线估计(Gaze Estimation)简介(五)-三维视线估计（数据集问题）"></a>视线估计(Gaze Estimation)简介(五)-三维视线估计（数据集问题）</h1><p>1.介绍三维视线数据如何收集和标注的问题。</p>
<p>2.以及如何在数据集短缺的情况下，训练一个gaze模型。</p>
<h2 id="数据收集："><a href="#数据收集：" class="headerlink" title="数据收集："></a>数据收集：</h2><p>与分类、检测等任务不同，<strong>三维视线难以人工标注</strong>。</p>
<p>一位参与者坐在深度摄像头Kinect前，而一名实验人员则手握一根吊着乒乓球的棍子，操纵乒乓球在参与者面前随机运动。参与者被要求始终盯着乒乓球，而<strong>深度摄像头</strong>则会记录下整个过程。数据收集完毕后，我们可以通过算法或人工的方式<strong>标注RGB视频中的眼睛中心点位置</strong>和<strong>乒乓球位置</strong>。</p>
<p>我们把这两个位置<strong>映射到深度摄像头记录的三维点云中</strong>，从而得到<strong>对应的三维位置坐标</strong>。这两个<strong>三维位置坐标相减后即得到视线方向</strong>。在我看来，这种数据收集和标注方式不仅精准而且相对简单。额外的要求就是需要一台深度摄像头。</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211014190045184.png" alt="image-20211014190045184"></p>
<p>另一种数据收集方式以MPIIGaze[2]为代表，仅仅需要普通的RGB摄像头即可。其基本做法是利用<strong>相机的公开参数</strong>，将<strong>gaze目标以及眼睛位置坐标</strong>（通过一个三维的6关键点模型得到）通过算法变换到相机坐标下，然后再计算gaze作为ground truth。但是这种标注方法不仅操作复杂，而且并不准确。</p>
<p>由以上两个例子可以看到，gaze数据的收集和标注比较耗时耗力。因此在实际应用中，如何在数据短缺的情况下训练一个可靠的gaze模型，就成了一个亟待解决的问题。本篇剩余部分主要介绍三类针对数据短缺的解决方案：<strong>基于合成数据的方法</strong>、<strong>基于Domain Adaptation的方法</strong>以及<strong>基于无监督学习的方法</strong>。</p>
<h2 id="基于合成数据的方法："><a href="#基于合成数据的方法：" class="headerlink" title="基于合成数据的方法："></a>基于合成数据的方法：</h2><h3 id="Rendering-of-Eyes-for-Eye-Shape-Registration-and-Gaze-Estimation"><a href="#Rendering-of-Eyes-for-Eye-Shape-Registration-and-Gaze-Estimation" class="headerlink" title="Rendering of Eyes for Eye-Shape Registration and Gaze Estimation"></a>Rendering of Eyes for Eye-Shape Registration and Gaze Estimation</h3><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211014201017047.png" alt="image-20211014201017047" style="zoom:50%;"></p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211014195732273.png" alt="image-20211014195732273"></p>
<p>模型准备过程概述:密集3D头部扫描(140万个多边形)(a)首先重新拓扑成动画的最佳形式(9005个多边形)(b)。高分辨率皮肤表面细节通过位移图恢复(c)，人工标注虹膜和眼睑的3D地标(d)。如图(e)所示。</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211014201322789.png" alt="image-20211014201322789"></p>
<h2 id="基于Domain-Adaptation的方法"><a href="#基于Domain-Adaptation的方法" class="headerlink" title="基于Domain Adaptation的方法"></a>基于Domain Adaptation的方法</h2><p>Shrivastava, A., Pfister, T., Tuzel, O., Susskind, J., Wang, W., and Webb, R. (2017). Learning from simulated and unsupervised images through adversarial training. In <em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, volume 3, page 6.</p>
<h2 id="基于无监督学习的方法-标注数据少"><a href="#基于无监督学习的方法-标注数据少" class="headerlink" title="基于无监督学习的方法,标注数据少"></a>基于无监督学习的方法,标注数据少</h2><h3 id="CVPR2020-Unsupervised-Representation-Learning-for-Gaze-Estimation余"><a href="#CVPR2020-Unsupervised-Representation-Learning-for-Gaze-Estimation余" class="headerlink" title="CVPR2020 Unsupervised Representation Learning for Gaze Estimation余"></a>CVPR2020 Unsupervised Representation Learning for Gaze Estimation余</h3><p>从无标签的数据中学习gaze表征。实验表明，通过我们的方法学习到的gaze表征与真实值呈强线性关系。在实际使用时，仅需要极少量的标注样本（&lt;=100），就可以得到有效可靠的视线估计模型。据我所知，这应该是第一篇通过无监督的方式学习gaze的论文。</p>
<h3 id="CVPR2019-Improving-few-shot-user-specific-gaze-adaptation-via-gaze-redirection-synthesis"><a href="#CVPR2019-Improving-few-shot-user-specific-gaze-adaptation-via-gaze-redirection-synthesis" class="headerlink" title="CVPR2019 Improving few-shot user-specific gaze adaptation via gaze redirection synthesis"></a>CVPR2019 Improving few-shot user-specific gaze adaptation via gaze redirection synthesis</h3><p>注视作为人类注意的指示物，是一种微妙的行为线索，具有广泛的应用价值。然而，由于缺乏大量的数据(真实的凝视是昂贵的，现有的数据集使用不同的设置)，以及由于<strong>个体差异而固有的凝视偏差</strong>，即使对深度神经网络来说，推断3D凝视方向也是具有挑战性的。在这项工作中，我们只<strong>从少数参考训练样本</strong>中解决了特定于人的凝视模型的适应问题。</p>
<p>主要和新颖的想法是，通过从<strong>已有的参考样本合成凝视重定向的眼睛图像来生成额外的训练样本，以提高凝视适应能力</strong>。</p>
<p>在此过程中，我们的贡献有三个方面:</p>
<p>(i)我们从合成数据中设计了我们的注视重定向框架，使我们能够从对齐的训练样本对中受益，以预测精确的<strong>逆映射域</strong>;</p>
<p>(ii)提出<strong>领域适应的self-supervised自我监督</strong>方法;</p>
<blockquote>
<p>domain adaption</p>
</blockquote>
<p>(iii)我们<strong>利用凝视重定向来提高特定于人的凝视估计的性能</strong>。在两个公共数据集上的大量实验证明了我们的视线重定向和视线估计框架的有效性。</p>
<blockquote>
<p>用少样本domain adaption生成很多样本。元学习的部分如何让理解？对原本的gaze estimator重定向。</p>
</blockquote>
<p>我们当时采取了如下网络结构。训练该网络需要输入样本，目标样本，以及输入样本与目标样本间gaze角度（包括垂直方向角度pitch与水平方向角度yaw）的差值。网络通过解析输入样本与gaze差值输出两个光流场（垂直和水平两个方向），来对输入图像的像素重定向，从而得到接近于目标图像的输出。</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211015104645809.png" alt="image-20211015104645809"></p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211015104707764.png" alt="image-20211015104707764"></p>
<p>我们期望通过缩小最终的输出图像与目标图像之间的loss，来迫使右上角的网络学习到gaze相关的表征（end to end training）。</p>
<p>这样一来我们就可以<strong>不依赖任何标注数据而学习出gaze表征</strong>。这对于视线估计，这个数据标注十分复杂和困难的领域十分有意义。需要注意的是，由于gaze由两个角度表示，我们的gaze表征也设定为二维。</p>
<p><strong>赋予物理意义：</strong>至此，网络或许能够学习出gaze相关的表征，但我们并不清楚这个表征所表示的物理意义。<strong>我们观察到，当人的视线上下变化时，眼皮和眼珠等部位主要在垂直方向运动，水平方向的运动几乎为0；当人的视线左右变化时，主要是眼珠在水平方向运动，几乎所有部位在垂直方向的运动为0。</strong></p>
<p>这也就是说，当垂直方向的gaze角度pitch变化（差值）为0时，视线重定向网络生成的垂直方向光流场应该接近于一个identity mapping；</p>
<p>而当水平方向gaze角度yaw变化（差值）为0时，水平方向光流场则接近于一个identity mapping。</p>
<p>由此我们提出一个针对光流场的正则：一方面，我们人为将gaze表征的第一维修改为0，然后输入重定向网络，并优化垂直方向光流场与identity mapping之间的差异；</p>
<p>另一方面，我们人为将gaze表征的第二维修改为0，输入重定向网络，优化水平方向光流场与identity mapping间的差异。</p>
<p>如下图所示。经此操作，gaze表征的第一维即对应pitch，第二维即对应yaw。</p>
<p><strong>扩展1-头部姿态估计：</strong>我们将提出的框架应用到了头部姿态估计（head pose estimation）这一任务中。我们使用BIWI数据集。</p>
<p><strong>扩展2-视线迁移：</strong>我们可以把person A的眼睛图片输入到表征学习网络中（抽取A的视线变化），而把person B的眼睛图片输入到视线重定向网络中，从而实现无监督视线迁移，即把A的gaze行为转移给B。</p>
<h1 id="视线估计-Gaze-Estimation-简介-六-三维视线估计（个性化问题）"><a href="#视线估计-Gaze-Estimation-简介-六-三维视线估计（个性化问题）" class="headerlink" title="视线估计(Gaze Estimation)简介(六)-三维视线估计（个性化问题）"></a>视线估计(Gaze Estimation)简介(六)-三维视线估计（个性化问题）</h1><p>在上上一篇中我们提到，在person independent（训练数据与测试数据采集自不同的人）设定下，大部分主流视线估计方法的<strong>精度大都在4-5度之间徘徊</strong>，很难得到进一步的提升。其主要原因是<strong>人与人之间存在一定的视线偏差</strong>。对于不同的两个人，即便眼球的旋转角度完全相同，其<strong>视线也会存在2到3度的不同</strong>。</p>
<h2 id="视线偏差产生原因："><a href="#视线偏差产生原因：" class="headerlink" title="视线偏差产生原因："></a>视线偏差产生原因：</h2><p>视线偏差产生的原因当然跟每个人的<strong>眼睛形状，眼珠大小</strong>等因素相关。但这些因素导致的视线偏差事实上很小，并且这些都是视觉元素，他们与gaze的相关性是可以从图像中学习得到的。真正<strong>产生视线偏差的原因来自于眼球内部构造</strong>。</p>
<p><img src="https://pic1.zhimg.com/v2-1f1ed9ae231cd771a1da6a584a947964_r.jpg" alt="preview"></p>
<p>我师兄Kenneth在他CVPR 2014年的论文中[1]较详细的介绍了原因。如图是一个眼球的构造图，</p>
<p>其中v表示视线，</p>
<p>o表示optical axis（瞳孔中心与眼球中心的连线，不知道中文该怎么翻），</p>
<p>pc是眼球中心，</p>
<p>而fovea是视网膜上对光敏感度最高的一个点，</p>
<p>N则代表一个与眼球中心距离为d的节点。</p>
<p>直觉上来说，视线v应该就是瞳孔中心与眼球中心的连线。然而事实上，视线p是连接fovea与N的直线，它与我们通常认为的“视线”，即optical axis不同。我们用k来表示视线p与optical axis的夹角，<strong>k的大小因人而异，由人眼球内部参数决定，无法从图像中学习获得。</strong>了解了这个原因之后，我们就理解了为什么在person independent设定下，模型精度难以进一步提高的原因：训练数据的后验概率分布与测试数据的后验概率分布不同。</p>
<p><strong>偏差消除方法，偏差估计方法与模型微调方法</strong>。</p>
<h2 id="A-Differential-Approach-for-Gaze-Estimation"><a href="#A-Differential-Approach-for-Gaze-Estimation" class="headerlink" title="A Differential Approach for Gaze Estimation"></a>A Differential Approach for Gaze Estimation</h2><p>大多数非侵入性凝视估计方法直接从一张脸或眼睛的图像中回归凝视方向。然而，由于个体之间眼睛形状和内部眼睛结构的重要变量，通用模型获得的精度有限，它们的输出通常表现出高方差和受试者依赖偏差。因此，提高准确度通常是通过校准来完成的，允许对一个对象的凝视预测被映射到她的实际凝视。在本文中，我们介绍了一种新的方法，通过直接训练微分卷积神经网络来预测同一被试的两个眼睛输入图像之间的注视差异。然后，给定一组受试者特定的校准图像，我们可以利用推断的差异来预测新眼睛样本的注视方向。假设通过比较同一用户的眼睛图像，通常困扰单一图像预测方法的烦恼因素(对齐、眼睑闭合、光照扰动)可以大大减少，从而更好地进行预测。此外，差分网络本身可以通过微调进行调整，使预测与可用的用户参考对一致。在3个公共数据集上的实验验证了我们的方法，即使只使用一个校准样本或那些依赖于受试者特定的注视适应的方法，我们的方法也不断优于最先进的方法。</p>
<h1 id="视线估计-Gaze-Estimation-简介-七-三维视线估计（头部姿态问题）"><a href="#视线估计-Gaze-Estimation-简介-七-三维视线估计（头部姿态问题）" class="headerlink" title="视线估计(Gaze Estimation)简介(七)-三维视线估计（头部姿态问题）"></a>视线估计(Gaze Estimation)简介(七)-三维视线估计（头部姿态问题）</h1><p>视线的方向不仅取决于眼球的旋转，还取决于头部的姿态（head pose)。如图，虽然眼睛相对头部是斜视，但<strong>在相机坐标系下，他看的是正前方</strong>。</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211015142501754.png" alt="image-20211015142501754"></p>
<p>在大多数情况下，我们希望得到的是相对于相机坐标系的视线，那么如何在视线估计中有效使用头部姿态信息就成了一个非常值得研究的问题。本篇会介绍五类方法。</p>
<p>其中前三类方法需要一个独立的前处理步骤事先估计出head pose，</p>
<p>而后两类方法则是在同一个框架内估计head pose和gaze。</p>
<h2 id="基于逐姿态视线估计的方法"><a href="#基于逐姿态视线估计的方法" class="headerlink" title="基于逐姿态视线估计的方法"></a>基于逐姿态视线估计的方法</h2><p>先对head pose聚类，然后对每一类的样本分别训练一个gaze模型。其代表是东京大学的Sugano教授发表在CVPR 2014上的工作[1]，如图所示。需要注意的是，为提高鲁棒性，该方法在训练一个随机森林时，也会用到相邻的head pose类的样本。这类方法的一大缺点是需要训练多个gaze模型，对实际应用参考意义不大。</p>
<h2 id="基于视角变换的方法"><a href="#基于视角变换的方法" class="headerlink" title="基于视角变换的方法"></a>基于视角变换的方法</h2><p>这类方法的一个前提条件是需要一个深度摄像头获取点云数据。在计算得到head pose之后，该方法利用head pose信息对点云数据作几何逆变换，从而得到前视视角（frontal view）的人脸或眼睛图像，如下图所示。该方法使用前视视角的样本训练gaze模型，再把估计得到的gaze利用head pose信息变换到相机坐标系。由于眼睛图像被统一变换到前视视角，可以认为这些训练样本处于同一个pose空间，因此该方法训练的模型相对比较准确。但这类方法也有两大缺点，一是需要深度摄像头（普通摄像头无法操作），二是在head pose较大时，视角变换后的样本信息会有所缺失（如图右侧眼睛）。</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211015143935895.png" alt="image-20211015143935895"></p>
<h2 id="基于特征拼接的方法"><a href="#基于特征拼接的方法" class="headerlink" title="基于特征拼接的方法"></a>基于特征拼接的方法</h2><p>concatenate</p>
<h2 id="基于几何变换的方法"><a href="#基于几何变换的方法" class="headerlink" title="基于几何变换的方法"></a>基于几何变换的方法</h2><p>这类方法的代表作是商汤在ICCV 2017上发表的一个全脸视线估计工作[4]。该工作主要认为特征拼接的方式并不能准确地反映gaze与head pose的的几何关系。因此，该工作提出了一个gaze的几何变换层，用于将head pose（人脸支路学习得到）与人脸坐标系下的gaze（眼睛支路学习得到）进行几何解析，得到最终相机坐标系下的gaze。</p>
<p>该方法将head pose的估计和gaze的估计放在一个框架内（所需要的前处理步骤主要是眼睛的定位），因此容错性更大。</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211014172151988.png" alt="image-20211014172151988"></p>
<h2 id="基于头部姿态隐式估计的方法"><a href="#基于头部姿态隐式估计的方法" class="headerlink" title="基于头部姿态隐式估计的方法"></a>基于头部姿态隐式估计的方法</h2><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211007202852686.png" alt="image-20211007202852686" style="zoom:150%;"></p>
<p>ucong Zhang等于2017年提出的基于注意力机制的全脸视线估计方法[5]。网络的输入是人脸图像，并采用end to end的学习策略，直接学习出最终相机坐标系下的gaze。因此我们可以认为这类方法隐式估计了head pose。这类方法的优点是简单直接，但内部机制不明，可解释性较差。</p>
<p>本篇主要介绍了五类处理头部姿态问题的方法。从我比较熟悉的单眼/双眼视线估计来说，我更推荐第三种方式：基于特征拼接的方法。商汤的论文[4]认为特征拼接并不是一个好的方式。他们通过在MPIIGaze上做实验发现处理head pose的weight几乎等于0，进而得出特征拼接效果欠佳这一结论。我认同这一现象，但不认同其结论。事实上，在我看来，造成这一现象的真正原因是<strong>MPIIGaze提供的head pose非常非常不准确</strong>，能提供的有效信息十分有限，用不用<strong>head pose差别不大（结果差0.1度左右）</strong>。相反，如果换做其他数据集，如<strong>ColumbiaGaze和UTMultiview</strong>，特征拼接则会显著提高精度（至少提高2度）。</p>
<p>在我们CVPR 2020的论文中[6]，我们也比较了特征拼接和几何变换（网络的输入是单眼，输出是头部坐标系下的gaze，然后通过事先获得的head pose对gaze进行几何变换，得到相机坐标系下的gaze）两种方式，结果是特征拼接的效果明显好于几何变换（1-2度）。这个结果可能与我们的直觉相悖。我理解的原因是，一般来说，<strong>如果注视目标位于前方，则相机坐标系下的gaze范围更小，而头部坐标系下的gaze范围更大</strong>。有兴趣的话可以尝试一下，<strong>变换头部姿态，但眼睛始终盯着正前方一个物体。如果这时正前方有一个摄像机，那么相机坐标系下你的gaze范围很小，接近于0度</strong>。<strong>但头部坐标系下的gaze，根据你头部运动幅度的大小，范围可以很大。拿ColumbiaGaze举例，相机坐标系下的gaze yaw的范围是-15~15，而头部坐标系下的gaze yaw是-45~45</strong>。因此，网络如果通过特征拼接的方式直接预测相机坐标系下的gaze的话，反而更容易学习。</p>
<h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><h2 id="ETH-XGaze-A-Large-Scale-Dataset-for-Gaze-Estimation-under-Extreme-Head-Pose-and-Gaze-Variation2020"><a href="#ETH-XGaze-A-Large-Scale-Dataset-for-Gaze-Estimation-under-Extreme-Head-Pose-and-Gaze-Variation2020" class="headerlink" title="ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation2020"></a>ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation2020</h2><p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211015152031823.png" alt="image-20211015152031823"></p>
<h2 id="商汤ICCV-2017Monocular-free-head-3d-gaze-tracking-with-deep-learning-and-geometry-constraints数据集"><a href="#商汤ICCV-2017Monocular-free-head-3d-gaze-tracking-with-deep-learning-and-geometry-constraints数据集" class="headerlink" title="商汤ICCV 2017Monocular free-head 3d gaze tracking with deep learning and geometry constraints数据集"></a>商汤ICCV 2017Monocular free-head 3d gaze tracking with deep learning and geometry constraints数据集</h2><p>看一下有没有开源</p>
<h2 id="MPIIFaceGaze"><a href="#MPIIFaceGaze" class="headerlink" title="MPIIFaceGaze"></a>MPIIFaceGaze</h2><h2 id="UnityEyes"><a href="#UnityEyes" class="headerlink" title="UnityEyes"></a>UnityEyes</h2><p><a target="_blank" rel="noopener" href="https://www.cl.cam.ac.uk/research/rainbow/projects/unityeyes/">UnityEyes (cam.ac.uk)</a></p>
<h2 id="Columbia"><a href="#Columbia" class="headerlink" title="Columbia"></a>Columbia</h2><p>eth eccv2018 眼睛黑白图片</p>
<h2 id="GazeCapture涵盖1400多人、240多万样本的数据集。"><a href="#GazeCapture涵盖1400多人、240多万样本的数据集。" class="headerlink" title="GazeCapture涵盖1400多人、240多万样本的数据集。"></a>GazeCapture涵盖1400多人、240多万样本的数据集。</h2><p>Krafka, K., Khosla, A., Kellnhofer, P ., Kannan, H., Bhandarkar,<br>S., Matusik, W., Torralba, A.: Eye tracking for everyone. In: Pro-<br>ceedings of the IEEE Conference on Computer Vision and Pattern<br>Recognition, pp. 2176–2184 (2016)，</p>
<h3 id="RGBD注视跟踪数据集Eyediap，该数据集由16个参与者的视频组成。"><a href="#RGBD注视跟踪数据集Eyediap，该数据集由16个参与者的视频组成。" class="headerlink" title="RGBD注视跟踪数据集Eyediap，该数据集由16个参与者的视频组成。"></a><strong>RGBD注视跟踪数据集</strong>Eyediap，该数据集由16个参与者的视频组成。</h3><p> UnityEyes 如图1a的第一排所示。考虑到依赖大量的路标无助于提高鲁棒性和准确性，只会增加方法的复杂性，我们只选择可用路标的一个子集作为替代。它包含来自眼睑的16个标志，虹膜中心是由虹膜轮廓标志估计出来的。如图1a第二行所示。</p>
<h2 id="MPIIGaze数据集"><a href="#MPIIGaze数据集" class="headerlink" title="MPIIGaze数据集"></a>MPIIGaze数据集</h2><p>MPIIGaze数据集，2015年，这是一个野外RGB凝视数据集，收集了15名参与者在几个月的日常使用笔记本电脑期间的数据。包含了在超过三个月的日常使用笔记本电脑期间收集的15名参与者的213659张图像。数据集在外观和照明方面比现有的数据集有更大的变化。Appearance-Based Gaze Estimation in the Wild提的。</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211007142949977.png" alt="image-20211007142949977"></p>
<p>虽然MPIIGaze与MPIIFaceGaze使用的是同一批数据，但并不是同一个数据集（许多论文把这两个数据集混淆）。首先MPIIGaze数据集并不包含全脸图片，其次MPIIFaceGaze的ground truth定义方式与MPIIGaze不同。该工作最终在MPIIFaceGaze数据集上取得了4.8度的精度。</p>
<h2 id="Top-8-Eye-Tracking-Applications-in-Research-imotions-com"><a href="#Top-8-Eye-Tracking-Applications-in-Research-imotions-com" class="headerlink" title="Top 8 Eye Tracking Applications in Research (imotions.com)"></a><a target="_blank" rel="noopener" href="https://imotions.com/blog/top-8-applications-eye-tracking-research/">Top 8 Eye Tracking Applications in Research (imotions.com)</a></h2><h2 id="UT-Multiview。"><a href="#UT-Multiview。" class="headerlink" title="UT Multiview。"></a>UT Multiview。</h2><p>建立了迄今为止最大的RGB-D凝视跟踪数据集，收集了218名参与者，包含超过165000张图像。该数据集将向所有研究人员公开，以促进对数据驱动的凝视跟踪方法的研究</p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211006164028365.png" alt="image-20211006164028365"><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211006164223297.png" alt="image-20211006164223297"></p>
<blockquote>
<p>用iTracker的方法怎么处理深度分支？</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9153754/akiny.t4-3013540-large.gif">https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/6287639/8948470/9153754/akiny.t4-3013540-large.gif</a></p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211008155155643.png" alt="image-20211008155155643"></p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211008155246008.png" alt="image-20211008155246008"></p>
<p><img src="/2021/10/04/eye-gaze%E8%B0%83%E7%A0%94/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211008163556619.png" alt="image-20211008163556619"></p>
<h1 id="LNSMM-Eye-Gaze-Estimation-With-Local-Network-Share-Multiview-Multitask"><a href="#LNSMM-Eye-Gaze-Estimation-With-Local-Network-Share-Multiview-Multitask" class="headerlink" title="LNSMM: Eye Gaze Estimation With Local Network Share Multiview Multitask"></a>LNSMM: Eye Gaze Estimation With Local Network Share Multiview Multitask</h1><blockquote>
<p>2021arxiv </p>
<p>Yong Huang, Ben Chen, Daiming Qu<br>Department of Electronics and Information Engineering,<br>HuaZhong University of Science and Technolog</p>
</blockquote>
<p>评价：基于<strong>局部网络共享多视图多任务</strong>的眼睛注视估计</p>
<hr>
<p>评价：</p>
<p>针对问题：</p>
<p>本文的目的：</p>
<p>实现的方法：<br>方法简介<br>方法优化<br>方法总结<br>文章存在的问题<br>个人的思考</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag"># 文献阅读</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2021/10/04/leetcode%E9%A2%98%E7%9B%AE%E8%AE%B0%E5%BD%95/" rel="next" title="leetcode题目记录">
                <i class="fa fa-chevron-left"></i> leetcode题目记录
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2021/10/04/%E2%80%9Clab%E5%A4%9A%E6%A8%A1%E6%80%81%E7%BA%A2%E5%A4%96%E2%80%9D/" rel="prev" title="lab多模态红外">
                lab多模态红外 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">
		  
		    
              <div class="site-state-item site-state-posts">
                <a href="/archives">
                  <span class="site-state-item-count">27</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
		    


       

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/iszff" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Gaze-Estimation"><span class="nav-number">1.</span> <span class="nav-text">Gaze  Estimation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">1.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E4%B8%8D%E5%90%8C%E7%9A%84%E5%9C%BA%E6%99%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E5%A4%A7%E8%87%B4%E5%8F%AF%E5%88%86%E4%B8%BA%E4%B8%89%E7%B1%BB"><span class="nav-number">1.2.</span> <span class="nav-text">根据不同的场景与应用大致可分为三类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%94%E7%94%A8"><span class="nav-number">1.3.</span> <span class="nav-text">应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8Tobii%E7%9C%BC%E5%8A%A8%E4%BB%AA%E7%8E%A9%E6%B8%B8%E6%88%8F%E7%9A%84Demo"><span class="nav-number">1.3.1.</span> <span class="nav-text">用Tobii眼动仪玩游戏的Demo</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VR"><span class="nav-number">1.3.2.</span> <span class="nav-text">VR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8C%BB%E7%96%97"><span class="nav-number">1.3.3.</span> <span class="nav-text">医疗</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%85%E5%8A%A9%E9%A9%BE%E9%A9%B6%EF%BC%88%E6%99%BA%E8%83%BD%E5%BA%A7%E8%88%B1%EF%BC%89"><span class="nav-number">1.3.4.</span> <span class="nav-text">辅助驾驶（智能座舱）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E4%B8%8B%E9%9B%B6%E5%94%AE"><span class="nav-number">1.3.5.</span> <span class="nav-text">线下零售</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E4%BA%A4%E4%BA%92%E7%B1%BB%E5%BA%94%E7%94%A8"><span class="nav-number">1.3.6.</span> <span class="nav-text">其他交互类应用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E5%9B%A2%E9%98%9F%E4%B8%8E%E5%85%AC%E5%8F%B8"><span class="nav-number">1.4.</span> <span class="nav-text">相关团队与公司</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B3%A8%E8%A7%86%E7%9B%AE%E6%A0%87%E4%BC%B0%E8%AE%A1"><span class="nav-number">2.</span> <span class="nav-text">注视目标估计</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B3%A8%E8%A7%86%E7%82%B9%E4%BC%B0%E8%AE%A1"><span class="nav-number">3.</span> <span class="nav-text">注视点估计</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%89%E7%BB%B4%E8%A7%86%E7%BA%BF%E4%BC%B0%E8%AE%A1%EF%BC%88%E9%80%9A%E7%94%A8%E6%96%B9%E6%B3%95%EF%BC%89"><span class="nav-number">4.</span> <span class="nav-text">三维视线估计（通用方法）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="nav-number">4.1.</span> <span class="nav-text">传统方法：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%95%E7%9C%BC-%E5%8F%8C%E7%9C%BC%E8%A7%86%E7%BA%BF%E4%BC%B0%E8%AE%A1%EF%BC%9A"><span class="nav-number">5.</span> <span class="nav-text">单眼&#x2F;双眼视线估计：</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E7%9A%84%E8%A7%86%E7%BA%BF%E4%BC%B0%E8%AE%A1"><span class="nav-number">6.</span> <span class="nav-text">基于语义信息的视线估计</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Deep-Pictorial-Gaze-Estimation-2018ECCV"><span class="nav-number">6.1.</span> <span class="nav-text">Deep Pictorial Gaze Estimation 2018ECCV</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deep-multitask-gaze-estimation-with-a-constrained-landmark-gaze-model%E4%BF%9E%E9%9B%A8"><span class="nav-number">6.2.</span> <span class="nav-text">Deep multitask gaze estimation with a constrained landmark-gaze model俞雨</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%A8%E8%84%B8%E8%A7%86%E7%BA%BF%E4%BC%B0%E8%AE%A1"><span class="nav-number">7.</span> <span class="nav-text">全脸视线估计</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Real-World-Dataset-and-Deep-Appearance-Based-Gaze-Estimation"><span class="nav-number">7.1.</span> <span class="nav-text">Real-World Dataset and Deep Appearance-Based Gaze Estimation.</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#pipeline"><span class="nav-number">7.1.1.</span> <span class="nav-text">pipeline</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#It%E2%80%99s-Written-All-Over-Y-our-Face-Full-Face-Appearance-Based-Gaze-Estimation-CVPR2017"><span class="nav-number">7.2.</span> <span class="nav-text">It’s Written All Over Y our Face:Full-Face Appearance-Based Gaze Estimation CVPR2017</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A7%86%E7%BA%BF%E4%BC%B0%E8%AE%A1-Gaze-Estimation-%E7%AE%80%E4%BB%8B-%E4%BA%94-%E4%B8%89%E7%BB%B4%E8%A7%86%E7%BA%BF%E4%BC%B0%E8%AE%A1%EF%BC%88%E6%95%B0%E6%8D%AE%E9%9B%86%E9%97%AE%E9%A2%98%EF%BC%89"><span class="nav-number">8.</span> <span class="nav-text">视线估计(Gaze Estimation)简介(五)-三维视线估计（数据集问题）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%9A"><span class="nav-number">8.1.</span> <span class="nav-text">数据收集：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%90%88%E6%88%90%E6%95%B0%E6%8D%AE%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="nav-number">8.2.</span> <span class="nav-text">基于合成数据的方法：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Rendering-of-Eyes-for-Eye-Shape-Registration-and-Gaze-Estimation"><span class="nav-number">8.2.1.</span> <span class="nav-text">Rendering of Eyes for Eye-Shape Registration and Gaze Estimation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8EDomain-Adaptation%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">8.3.</span> <span class="nav-text">基于Domain Adaptation的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%B9%E6%B3%95-%E6%A0%87%E6%B3%A8%E6%95%B0%E6%8D%AE%E5%B0%91"><span class="nav-number">8.4.</span> <span class="nav-text">基于无监督学习的方法,标注数据少</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CVPR2020-Unsupervised-Representation-Learning-for-Gaze-Estimation%E4%BD%99"><span class="nav-number">8.4.1.</span> <span class="nav-text">CVPR2020 Unsupervised Representation Learning for Gaze Estimation余</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CVPR2019-Improving-few-shot-user-specific-gaze-adaptation-via-gaze-redirection-synthesis"><span class="nav-number">8.4.2.</span> <span class="nav-text">CVPR2019 Improving few-shot user-specific gaze adaptation via gaze redirection synthesis</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A7%86%E7%BA%BF%E4%BC%B0%E8%AE%A1-Gaze-Estimation-%E7%AE%80%E4%BB%8B-%E5%85%AD-%E4%B8%89%E7%BB%B4%E8%A7%86%E7%BA%BF%E4%BC%B0%E8%AE%A1%EF%BC%88%E4%B8%AA%E6%80%A7%E5%8C%96%E9%97%AE%E9%A2%98%EF%BC%89"><span class="nav-number">9.</span> <span class="nav-text">视线估计(Gaze Estimation)简介(六)-三维视线估计（个性化问题）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%86%E7%BA%BF%E5%81%8F%E5%B7%AE%E4%BA%A7%E7%94%9F%E5%8E%9F%E5%9B%A0%EF%BC%9A"><span class="nav-number">9.1.</span> <span class="nav-text">视线偏差产生原因：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Differential-Approach-for-Gaze-Estimation"><span class="nav-number">9.2.</span> <span class="nav-text">A Differential Approach for Gaze Estimation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A7%86%E7%BA%BF%E4%BC%B0%E8%AE%A1-Gaze-Estimation-%E7%AE%80%E4%BB%8B-%E4%B8%83-%E4%B8%89%E7%BB%B4%E8%A7%86%E7%BA%BF%E4%BC%B0%E8%AE%A1%EF%BC%88%E5%A4%B4%E9%83%A8%E5%A7%BF%E6%80%81%E9%97%AE%E9%A2%98%EF%BC%89"><span class="nav-number">10.</span> <span class="nav-text">视线估计(Gaze Estimation)简介(七)-三维视线估计（头部姿态问题）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E9%80%90%E5%A7%BF%E6%80%81%E8%A7%86%E7%BA%BF%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">10.1.</span> <span class="nav-text">基于逐姿态视线估计的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E8%A7%86%E8%A7%92%E5%8F%98%E6%8D%A2%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">10.2.</span> <span class="nav-text">基于视角变换的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%89%B9%E5%BE%81%E6%8B%BC%E6%8E%A5%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">10.3.</span> <span class="nav-text">基于特征拼接的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%87%A0%E4%BD%95%E5%8F%98%E6%8D%A2%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">10.4.</span> <span class="nav-text">基于几何变换的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%A4%B4%E9%83%A8%E5%A7%BF%E6%80%81%E9%9A%90%E5%BC%8F%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">10.5.</span> <span class="nav-text">基于头部姿态隐式估计的方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">11.</span> <span class="nav-text">数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ETH-XGaze-A-Large-Scale-Dataset-for-Gaze-Estimation-under-Extreme-Head-Pose-and-Gaze-Variation2020"><span class="nav-number">11.1.</span> <span class="nav-text">ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation2020</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%95%86%E6%B1%A4ICCV-2017Monocular-free-head-3d-gaze-tracking-with-deep-learning-and-geometry-constraints%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">11.2.</span> <span class="nav-text">商汤ICCV 2017Monocular free-head 3d gaze tracking with deep learning and geometry constraints数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MPIIFaceGaze"><span class="nav-number">11.3.</span> <span class="nav-text">MPIIFaceGaze</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#UnityEyes"><span class="nav-number">11.4.</span> <span class="nav-text">UnityEyes</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Columbia"><span class="nav-number">11.5.</span> <span class="nav-text">Columbia</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GazeCapture%E6%B6%B5%E7%9B%961400%E5%A4%9A%E4%BA%BA%E3%80%81240%E5%A4%9A%E4%B8%87%E6%A0%B7%E6%9C%AC%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82"><span class="nav-number">11.6.</span> <span class="nav-text">GazeCapture涵盖1400多人、240多万样本的数据集。</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RGBD%E6%B3%A8%E8%A7%86%E8%B7%9F%E8%B8%AA%E6%95%B0%E6%8D%AE%E9%9B%86Eyediap%EF%BC%8C%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E7%94%B116%E4%B8%AA%E5%8F%82%E4%B8%8E%E8%80%85%E7%9A%84%E8%A7%86%E9%A2%91%E7%BB%84%E6%88%90%E3%80%82"><span class="nav-number">11.6.1.</span> <span class="nav-text">RGBD注视跟踪数据集Eyediap，该数据集由16个参与者的视频组成。</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MPIIGaze%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">11.7.</span> <span class="nav-text">MPIIGaze数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Top-8-Eye-Tracking-Applications-in-Research-imotions-com"><span class="nav-number">11.8.</span> <span class="nav-text">Top 8 Eye Tracking Applications in Research (imotions.com)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#UT-Multiview%E3%80%82"><span class="nav-number">11.9.</span> <span class="nav-text">UT Multiview。</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LNSMM-Eye-Gaze-Estimation-With-Local-Network-Share-Multiview-Multitask"><span class="nav-number">12.</span> <span class="nav-text">LNSMM: Eye Gaze Estimation With Local Network Share Multiview Multitask</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">iszff</span>

  
 </div>



  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>





        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.4"></script>



  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
