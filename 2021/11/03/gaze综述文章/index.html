<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="文献阅读," />





  <link rel="alternate" href="/atom.xml" title="iszff' Blog" type="application/atom+xml" />






<meta name="description" content="人的眼睛由视网膜、瞳孔、角膜、巩膜、虹膜等组成，眼睛检测是对眼睛进行定位并测量眼睛注视的过程。有几种用于注视跟踪的方法，如基于形状的、基于特征的、基于外观的和混合的方法。 基于特征的凝视估计技术是通过检测眼睛的一组不同的特征，对光线和视点的差异不敏感，虹膜和瞳孔需要清晰才能正确检测，包括瞳孔的轮廓，没有角膜反射。 此外，必须应用三维和基于几何的方法。相比之下，基于外观的凝视估计技术是通过提取图像内">
<meta property="og:type" content="article">
<meta property="og:title" content="gaze综述文章">
<meta property="og:url" content="http://yoursite.com/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/index.html">
<meta property="og:site_name" content="iszff&#39; Blog">
<meta property="og:description" content="人的眼睛由视网膜、瞳孔、角膜、巩膜、虹膜等组成，眼睛检测是对眼睛进行定位并测量眼睛注视的过程。有几种用于注视跟踪的方法，如基于形状的、基于特征的、基于外观的和混合的方法。 基于特征的凝视估计技术是通过检测眼睛的一组不同的特征，对光线和视点的差异不敏感，虹膜和瞳孔需要清晰才能正确检测，包括瞳孔的轮廓，没有角膜反射。 此外，必须应用三维和基于几何的方法。相比之下，基于外观的凝视估计技术是通过提取图像内">
<meta property="og:locale">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-fc125c419a01a7baa7e9ab0c93ef4a86_720w.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-c047bc5b78bca574f785d4c2dc55aea7_r.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-141ec7bc86c26ee912167377e25dc6fc_r.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-d1e2edb5067000110a05c7ff670440de_720w.jpg">
<meta property="og:image" content="http://yoursite.com/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211105101710286.png">
<meta property="og:image" content="http://yoursite.com/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211104112538152.png">
<meta property="og:image" content="http://yoursite.com/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211104112608127.png">
<meta property="og:image" content="http://yoursite.com/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211104112700336.png">
<meta property="og:image" content="http://yoursite.com/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/Users/iszha/AppData/Roaming/Typora/typora-user-images/image-20211104112735277.png">
<meta property="article:published_time" content="2021-11-03T03:08:18.000Z">
<meta property="article:modified_time" content="2022-01-03T13:53:40.754Z">
<meta property="article:author" content="iszff">
<meta property="article:tag" content="文献阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic3.zhimg.com/80/v2-fc125c419a01a7baa7e9ab0c93ef4a86_720w.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2021/11/03/gaze综述文章/"/>





  <title>gaze综述文章 | iszff' Blog</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?5b9fc7fd579b171d84d00067691ad8ab";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




<meta name="generator" content="Hexo 5.4.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">iszff' Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">people move on.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="iszff' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">gaze综述文章</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-11-03T11:08:18+08:00">
                2021-11-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%A7%86%E7%BA%BF%E4%BC%B0%E8%AE%A1/" itemprop="url" rel="index">
                    <span itemprop="name">视线估计</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>人的眼睛由视网膜、瞳孔、角膜、巩膜、虹膜等组成，眼睛检测是对眼睛进行定位并测量眼睛注视的过程。有几种用于注视跟踪的方法，如基于形状的、基于特征的、基于外观的和混合的方法。</p>
<p>基于特征的凝视估计技术是通过检测眼睛的一组不同的特征，对光线和视点的差异不敏感，虹膜和瞳孔需要清晰才能正确检测，包括瞳孔的轮廓，没有角膜反射。</p>
<p>此外，必须应用三维和基于几何的方法。相比之下，基于外观的凝视估计技术是通过提取图像内容并将其映射到屏幕上的位置来实现的。因此，寻找关注点、相关特征和个人变化是这项技术的关键步骤。这种技术的例子是流形的，包括灰度单元、交叉比、高斯插值和变形模型。</p>
<h1 id="眼动跟踪技术"><a href="#眼动跟踪技术" class="headerlink" title="眼动跟踪技术"></a>眼动跟踪技术</h1><p>四种眼动跟踪技术已经成为该领域和开发新的眼动跟踪应用的研究重点。他们是巩膜搜索线圈技术、红外眼图(IOG)、眼电图(EOG)、眼视频图(VOG)scleral search coil technique, infrared oculography (IOG), electrooculography (EOG), and video oculography (VOG).。</p>
<h2 id="红外眼图-IOG"><a href="#红外眼图-IOG" class="headerlink" title="红外眼图(IOG)"></a>红外眼图(IOG)</h2><p>红外眼膜照相术(IOG)技术测量从巩膜反射的红外光的强度，这提供了关于眼睛位置的各种信息。一副眼镜就能发出光。该方法主要依赖于光和瞳孔检测算法。为了解决头部运动灵敏度的问题，在应用该技术时，使用红外光源包括一个参考点，<strong>即角膜反射或闪烁</strong>。</p>
<p>红外技术产生的干扰比EOG少。红外眼图(IOG)、眼电图(EOG)。</p>
<p>IOG技术的一个优势是它能够平滑地处理眨眼。然而，它无法量化扭转运动。</p>
<h2 id="Electrooculography"><a href="#Electrooculography" class="headerlink" title="Electrooculography"></a>Electrooculography</h2><p>眼动电波图测量技术，眼电描记法。</p>
<p>眼电描记术(Electrooculography, EOG)是一种实用廉价的人机交互技术。在这种方法中，传感器被连接到眼睛周围的区域，通过<strong>测量眼睛旋转时的波动来检测电场</strong>。</p>
<p><strong>水平和垂直的眼球运动是用电极分离记录的</strong>。然而，这种信号可以在没有眼球运动的情况下被改变。图5显示了带有传感器的可穿戴EOG设备。</p>
<p>EOG不是一种日常使用的方法;它的应用将有利于医学领域和实验室。该方法与眼球运动呈线性相关，可以使用头部运动进行跟踪(Chennamma &amp; Yuan, 2013;Sorate et al.， 2017)。EOG眼凝视界面结构简单，易于使用。然而，由于眼球漂移和眼部疾病，EOG的使用受到限制(Tsui, Jia, Gan， &amp; Hu, 2007;Constable等人，2017)。针对这个问题已经开发了一个解决方案。用户可以使用指针和切换器轻松地打开/关闭呼叫设备，或将1位信号转发到具有高度牢固和精细度的PC (Tsui, Jia, Gan, Hu， &amp; Yuan, 2007)。</p>
<h2 id="Video-Oculography"><a href="#Video-Oculography" class="headerlink" title="Video Oculography"></a>Video Oculography</h2><p>一个典型的装置包括一个记录眼睛运动的摄像机和一个保存和分析目光数据的电脑。VOG方法可以使用可见光或红外光。VOG是一种非侵入式系统，可以远程执行眼球追踪。</p>
<p>基于使用的摄像机数量，有两种实现视频眼动跟踪的方法:第一种方法使用单个摄像机，而另一种方法使用多个摄像机(Majaranta &amp; Bulling, 2014)。</p>
<p>两种类型的眼动跟踪器，远程或头部安装，如果在HCI系统中使用，由于头部位置的变化有一个主要缺点。</p>
<p>对于远程跟踪器，这可以通过使用<strong>两个立体摄像头或一个广角摄像头来解决</strong>，以搜索前面的人，另一个指向人的脸，并放大。图6显示了一个使用两个摄像机的VOG示例。</p>
<blockquote>
<p>克服头部位置变化？怎么克服？</p>
</blockquote>
<p>单相机系统:单相机系统捕捉固定在单点上的有限视场和高分辨率图像。这种方法<strong>使用红外光源产生角膜反射</strong>，将作为注视估计的参考点。</p>
<p>当头部移动时，闪烁明显改变其位置，瞳孔-闪烁矢量对于眼睛和头部运动保持恒定。一些商业系统使用一个摄像头和一个红外光。</p>
<p>这种方法的主要困难是要充分捕捉高分辨率图像所需的视野有限。通过在设置中添加多个光源可以获得更好的结果(Chennamma &amp; Yuan, 2013;Sorate et al.， 2017)。</p>
<p><strong>多相机眼动仪</strong>:需要大视场的自由头部运动。通过广角镜头相机或可移动的窄角镜头相机来实现这一目标。一个摄像头用于眼睛，另一个摄像头用于捕捉头部位置。所有从这些相机收集到的数据都被结合起来，用来估计凝视点。眼动仪系统使用两个摄像机组成一个<strong>立体视觉系统来校准瞳孔中心的计算</strong>，这是使用<strong>三维坐标</strong>实现的。</p>
<p>从这些摄像机生成的视频，如VOG，被引入来寻找解决基于图像的眼球追踪技术面临的许多问题的方法。其中一些问题包括由于头部运动而难以发现眼睛，由于眨眼而眼睛模糊，或睫毛弯曲。VOG有助于提高眼动跟踪系统的准确性和观察眼动障碍。视频系统可以操作简单，允许头部运动和完全远程记录。然而，视频录制系统价格昂贵，需要更多的存储和具有高计算能力的设备。此外，在记录闭眼时，无法测量眼睛扭转。</p>
<h1 id="使用机器学习的眼球追踪"><a href="#使用机器学习的眼球追踪" class="headerlink" title="使用机器学习的眼球追踪"></a>使用机器学习的眼球追踪</h1><h1 id="眼动追踪技术的方法-知乎-zhihu-com"><a href="#眼动追踪技术的方法-知乎-zhihu-com" class="headerlink" title="眼动追踪技术的方法 - 知乎 (zhihu.com)"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/159057692">眼动追踪技术的方法 - 知乎 (zhihu.com)</a></h1><p>在早期眼动追踪中，通常是直接观察的，这种方式非常的粗略。在最初的自动化系统追踪中，使用的是一种与眼睛直接接触的搜索方法，通过<strong>将一个橡胶吸盘吸在眼球表面</strong>，发明了第一个可以记录数据的眼动仪，这种眼动仪非常的不方便，且不舒适。</p>
<p>1963最早的眼动追踪电子系统之一是<strong>电磁巩膜搜索线圈</strong>。它们被嵌入到专用的隐形眼镜中，并且将导线连接到记录设备。根据眼睛运动产生的感应电压来记录眼动轨迹，该系统具有很高的精度和速度，但是在进行实验时是侵入式的，且需要先麻醉人的眼睛，将实验用的设备吸附早眼球上，通常的实验环境是在法拉第笼中，该方法对参与者的眼睛影响较大，有一定的生理伤害。</p>
<p>双眼浦肯（Purkinje）成像系统（DPI）等眼动追踪设备虽然没有物理接触到眼睛</p>
<h2 id="非侵入式眼动技术主要采用的追踪方法主要有1-巩膜一虹膜边缘法、2-瞳孔追踪方法、3-瞳孔一角膜反射法。"><a href="#非侵入式眼动技术主要采用的追踪方法主要有1-巩膜一虹膜边缘法、2-瞳孔追踪方法、3-瞳孔一角膜反射法。" class="headerlink" title="非侵入式眼动技术主要采用的追踪方法主要有1.巩膜一虹膜边缘法、2.瞳孔追踪方法、3.瞳孔一角膜反射法。"></a>非侵入式眼动技术主要采用的追踪方法主要有1.巩膜一虹膜边缘法、2.瞳孔追踪方法、3.瞳孔一角膜反射法。</h2><h3 id="1-巩膜一虹膜边缘法"><a href="#1-巩膜一虹膜边缘法" class="headerlink" title="1.巩膜一虹膜边缘法"></a>1.巩膜一虹膜边缘法</h3><p>利用红外光照射人眼，在眼睛附近安装的两只红外光敏管用来接收巩膜和虹膜边缘处两部分反射的红外光。当眼球向一侧运动时，虹膜就转向这边，这一侧的光敏管所接受的红外线就会减少;而另一侧的巩膜反射部分增加，导致这边的光敏管所接受的红外线增加。利用这个差分信号就能无接触的测出眼动。这种方法的水平精度较高，垂直精度较低、干扰大、头部误差大。</p>
<p>利用红外线差分信号。</p>
<blockquote>
<p>在现在大多数基于视频的眼动跟踪系统都包括一个红外摄像机，红外光（IR）照明器以及用于瞳孔中心检测和伪影排除的眼动追踪算法，图像处理和数据收集由专用硬件处理或者由计算机（Host PC）或软件处理。基于红外的照明具有多个优点：参与者在很大程度上看不到照明，并且可以通过波长过滤来自人造光源的伪影。</p>
</blockquote>
<h3 id="2-瞳孔追踪方法"><a href="#2-瞳孔追踪方法" class="headerlink" title="2.瞳孔追踪方法"></a>2.瞳孔追踪方法</h3><p>现在眼动仪中常用的两种近红外眼动追踪技术：明瞳和暗瞳。它们的差异基于照明源相对于光学系统的位置。</p>
<p>如果照明与光路同轴，则当光从视网膜反射时，眼睛将充当反射器，从而产生类似于红眼的明亮瞳孔效果。</p>
<p>如果照明源偏离光路，则瞳孔会变暗，因为来自视网膜的回射被定向为远离相机。</p>
<p><img src="https://pic3.zhimg.com/80/v2-fc125c419a01a7baa7e9ab0c93ef4a86_720w.jpg" alt="img"></p>
<h3 id="3-瞳孔-角膜反射追踪方法"><a href="#3-瞳孔-角膜反射追踪方法" class="headerlink" title="3.瞳孔-角膜反射追踪方法"></a>3.瞳孔-角膜反射追踪方法</h3><p>首先利用眼摄像机拍摄眼睛图像，接着通过图像处理得到瞳孔中心位置。然后把角膜反射点（黄色斑点）作为眼摄像机和眼球的相对位置的基点，根据图像处理得到的瞳孔中心即可以得到视线向量坐标，从而确定人眼注视点。</p>
<p><img src="https://pic4.zhimg.com/v2-c047bc5b78bca574f785d4c2dc55aea7_r.jpg" alt="preview"></p>
<blockquote>
<p>怎么得到角膜反射点？</p>
<p>使用rgb的数据集是怎么label的，做一次实验就知道了。</p>
</blockquote>
<p>在找到较好的瞳孔-角膜反射点后，通过一些校准程序，找出瞳孔与角膜反射点间组成的向量与屏幕注视点之间的映射函数，然后通过检测瞳孔-角膜向量的变化量，实时跟踪出人在屏幕中所凝视的兴趣点。</p>
<p>由于人眼形状，大小，结构，存在个体差异，眼睛球面上的点在摄像机参照系中的投影点位置和眼睛转动角度之间存在非线性关系，并且视线估计方向与真实视线方向有模型误差，所以视线跟踪系统需要校准环节。</p>
<p><img src="https://pic1.zhimg.com/v2-141ec7bc86c26ee912167377e25dc6fc_r.jpg" alt="preview"></p>
<p>校准点可以是一点，也可以是3点、5点、9点以及13点，校准点数依据实验任务的不同而不同，该算法在每个目标的眼睛位置（减去CR）和注视位置之间创建数学转换，然后创建一个矩阵来覆盖整个校准区域，并在每个点之间进行插值。使用的校准点越多，在整个视场中的精度就越高，越均匀。</p>
<p>在EyeLink系统中，除了校准过程外，还有一个过程是验证过程。因为在大多数时候校准是需要受试者一定的配合和能力的，因此在大多数情况下，需要进行验证校准过程产生的误差。比如：以下9点校准。</p>
<p><img src="https://pic3.zhimg.com/80/v2-d1e2edb5067000110a05c7ff670440de_720w.jpg" alt="img"></p>
<p>当平均值小于1度，最大值小于1.5度时，会显示绿色，说明是GOOD；当平均值小于1.5度，最大值小于2度时，会显示灰色，说明是FAIR；当平均值大于1.5度，最大值大于2度时，会显示红色，说明是POOR，在大多数情况下，需要调整到GOOD情况下，才能被允许进行实验数据的收集，对于大多数实验来说，超过1度被认为是校准失败，需要再次校准。</p>
<h2 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h2><p>1、瞳孔遮挡</p>
<p>在当前的眼动仪中，很明确的需要瞳孔无遮挡的视线，浓厚的眼睫毛有可能会被误认为瞳孔，从而使眼动追踪无法继续进行，当然，在绝大多数时候，即使瞳孔被部分遮挡，当前的眼动仪也可以根据算法确定中心，但是在某个点上，如果被遮挡足够多，眼动追踪也会无法继续进行。我们目前依然保持着较高的信心，这种情况发生的事件很小。</p>
<p>2、眼部化妆</p>
<p>简单来说，做眼动实验前，要求受试者不要化眼部妆，而且不要带美瞳，它会造成红外的照射不能完全捕捉瞳孔区域。</p>
<p>3、眼睛度数</p>
<p>一般建议是超过800度以上的或者是有严重的散光的被试，将会被排除在外。任何眼镜都会使拍摄的眼睛变形，有可能会减少一些反射的红外照明光源，在现代的眼动仪中，能追踪到低度数的眼动，这是可以的，但是也需要注意，在进行实验时，尽可能的不戴有黑色镜框的眼镜，它可能会遮挡或被误认为瞳孔区域进而眼动追踪无法继续。</p>
<h1 id="Vision-based-Gaze-Estimation-A-Review"><a href="#Vision-based-Gaze-Estimation-A-Review" class="headerlink" title="Vision-based Gaze Estimation: A Review"></a>Vision-based Gaze Estimation: A Review</h1><p>从元学习、因果推理、解纠缠表征和无约束注视估计的社会注视行为等方面指出了注视估计未来的研究方向和挑战。</p>
<h2 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h2><p>头部姿势可以粗略地决定注视的方向。然而，头部姿势不足以描述人的注视，因为眼睛可以旋转。为了更好地描述人眼注视，除了考虑头部姿态外，还需要从眼睛区域中提取与注视相关的特征。</p>
<p>因此，鲁棒的凝视估计需要从头部姿态和凝视中提取具有代表性的信息，以多粒度描述人眼的注视。</p>
<p>头部姿势变化的一个影响是眼睛位置的变化。为了提取高度相关的注视特征，需要从原始图像中裁剪出眼睛区域或人脸区域，以去除噪声背景。人脸地标通常作为参考点来裁剪所需区域。然后，可以提取凝视特征，但仍然会出现外观变化。头部运动的另一个影响是，当考虑头部运动时，眼睛的外观发生了显著变化。</p>
<blockquote>
<p>头的运动影响的是,眼睛的位置和外观。</p>
</blockquote>
<p><img src="/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211105101710286.png" alt="image-20211105101710286"></p>
<blockquote>
<p>使用UnityEye生成的不同头部姿势(偏斜:30°，0°，-30°)下，眼睛外观随相同的注视角度变化。例如，从左到右，虹膜中心与眼角之间的相对位置变化明显</p>
</blockquote>
<p>凝视和头姿在现有数据集中的分布不均匀，这也导致了凝视和头姿之间的过拟合。假设没有提取姿态不敏感特征，当训练样本和测试样本不服从独立同分布假设时，训练模型将无法估计注视量。</p>
<p><img src="/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211104112538152.png" alt></p>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><h3 id="5-Y-Sugano-Y-Matsushita-and-Y-Sato-“Learning-bysynthesis-for-appearance-based-3d-gaze-estimation-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-CVPR-2014-pp-1821–-1828"><a href="#5-Y-Sugano-Y-Matsushita-and-Y-Sato-“Learning-bysynthesis-for-appearance-based-3d-gaze-estimation-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-CVPR-2014-pp-1821–-1828" class="headerlink" title="[5] Y. Sugano, Y. Matsushita, and Y. Sato, “Learning-bysynthesis for appearance-based 3d gaze estimation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 1821– 1828."></a>[5] Y. Sugano, Y. Matsushita, and Y. Sato, “Learning-bysynthesis for appearance-based 3d gaze estimation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 1821– 1828.</h3><p>通过训练一个随机回归森林模型来<strong>学习眼睛区域和眼睛注视点之间的映射函数</strong>。具体来说，回归森林模型中的每棵回归树都针对每一个冗余聚类进行训练。来重建眼睛区域。</p>
<h3 id="6-类似的聚类方案也用于处理头部姿态的方差。"><a href="#6-类似的聚类方案也用于处理头部姿态的方差。" class="headerlink" title="[6]类似的聚类方案也用于处理头部姿态的方差。"></a>[6]类似的聚类方案也用于处理头部姿态的方差。</h3><p>但是，为每个集群训练多个模型会增加计算复杂度。</p>
<h3 id="7-R-Ranjan-S-D-Mello-and-J-Kautz-“Light-weight-head-pose-invariant-gaze-tracking-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-Workshops-CVPRW-Los-Alamitos-CA-USA-IEEE-Computer-Society-jun-2018-pp-2237–-22-378"><a href="#7-R-Ranjan-S-D-Mello-and-J-Kautz-“Light-weight-head-pose-invariant-gaze-tracking-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-Workshops-CVPRW-Los-Alamitos-CA-USA-IEEE-Computer-Society-jun-2018-pp-2237–-22-378" class="headerlink" title="[7] R. Ranjan, S. D. Mello, and J. Kautz, “Light-weight head pose invariant gaze tracking,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Los Alamitos, CA, USA: IEEE Computer Society, jun 2018, pp. 2237– 22 378."></a>[7] R. Ranjan, S. D. Mello, and J. Kautz, “Light-weight head pose invariant gaze tracking,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Los Alamitos, CA, USA: IEEE Computer Society, jun 2018, pp. 2237– 22 378.</h3><p>受聚类方法的启发，提出了一种新的卷积神经网络模型即由5个卷积层(conv1-conv5)和1个全连接层(fc1)组成的卷积神经网络模型。与训练多元回归模型不同的是，该CNN模型<strong>根据梯度更新conv1-conv5和fc1的权值来学习注视表示</strong>，然后在fc1之后<strong>利用学习到的注视表示</strong>和来自相应<strong>头部姿态聚类的头部姿态向量</strong>进一步更新多个CNN流。每个流在fc1之后有两个完全连接的层。该方案充分利用有限的数据学习基本的注视表示，然后根据不同的头部姿态调整注视表示。</p>
<h2 id="人脸正面化"><a href="#人脸正面化" class="headerlink" title="人脸正面化"></a>人脸正面化</h2><blockquote>
<p>基于人脸正面化的方法的主要思想是重建一个三维的人脸模型。人脸可以被转换为正面，这样就可以消除头部的方差。</p>
<p>虽然人脸frontalization方法可以生成一个正面人脸，但如果没有捕获部分眼睛区域，眼睛外观仍然不完整，降低了凝视估计。</p>
</blockquote>
<h3 id="8-Q-Shi-S-Nobuhara-and-T-Matsuyama-“3d-face-reconstruction-and-gaze-estimation-from-multi-view-video-using-symmetry-prior-”-Ipsj-Transactions-on-Computer-Vision-and-Applications-vol-4-pp-149–160-2012"><a href="#8-Q-Shi-S-Nobuhara-and-T-Matsuyama-“3d-face-reconstruction-and-gaze-estimation-from-multi-view-video-using-symmetry-prior-”-Ipsj-Transactions-on-Computer-Vision-and-Applications-vol-4-pp-149–160-2012" class="headerlink" title="[8] Q. Shi, S. Nobuhara, and T. Matsuyama, “3d face reconstruction and gaze estimation from multi-view video using symmetry prior,” Ipsj Transactions on Computer Vision and Applications, vol. 4, pp. 149–160, 2012."></a>[8] Q. Shi, S. Nobuhara, and T. Matsuyama, “3d face reconstruction and gaze estimation from multi-view video using symmetry prior,” Ipsj Transactions on Computer Vision and Applications, vol. 4, pp. 149–160, 2012.</h3><p>Qun等人[8]提出了一种基于<strong>对称先验知识的三维重建方法用于注视估计</strong>。该方法首先从多个摄像机的三维网格数据和视频中<strong>检测出人脸区域。然后提取人脸的关键点，得到对称平面。</strong>其次，在对称平面的基础上，<strong>利用对称先验知识重建三维人脸模型</strong>，然后利用超解绘制方法绘制出虚拟正面人脸图像。最后，在虹膜直径等于眼球半径的假设下，<strong>将二维虹膜中心映射到三维眼球模型，得到三维虹膜位置和三维眼球中心</strong>。因此，凝视方向就是从三维眼球中心到三维虹膜中心的连线。</p>
<h3 id="9-K-A-Funes-Mora-and-J-Odobez-“Gaze-estimation-from-multimodal-kinect-data-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-Workshops-CVPRW-2012-pp-25–30-10-K-A-Funes-Mora-and-J-M-Odobez-“Gaze-estimation-in-the-3d-space-using-rgb-d-sensors-”-International-Journal-of-Computer-Vision-vol-118-no-2-pp-194–-216-2016"><a href="#9-K-A-Funes-Mora-and-J-Odobez-“Gaze-estimation-from-multimodal-kinect-data-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-Workshops-CVPRW-2012-pp-25–30-10-K-A-Funes-Mora-and-J-M-Odobez-“Gaze-estimation-in-the-3d-space-using-rgb-d-sensors-”-International-Journal-of-Computer-Vision-vol-118-no-2-pp-194–-216-2016" class="headerlink" title="[9] K. A. Funes Mora and J. Odobez, “Gaze estimation from multimodal kinect data,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2012, pp. 25–30. [10] K. A. Funes-Mora and J. M. Odobez, “Gaze estimation in the 3d space using rgb-d sensors,” International Journal of Computer Vision, vol. 118, no. 2, pp. 194– 216, 2016."></a>[9] K. A. Funes Mora and J. Odobez, “Gaze estimation from multimodal kinect data,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2012, pp. 25–30. [10] K. A. Funes-Mora and J. M. Odobez, “Gaze estimation in the 3d space using rgb-d sensors,” International Journal of Computer Vision, vol. 118, no. 2, pp. 194– 216, 2016.</h3><p>提出首先在离线阶段基于三维变形模型学习一个针对特定人群的三维网格模型，然后在在线阶段根据深度数据估计头部姿态。通过头部姿态和三维网格数据，可以进一步生成正面。最后，凝视可以从裁剪的眼睛图像学习，然后映射回WCS坐标系。</p>
<h3 id="11-R-S-Ghiass-and-O-Arandjelovic-“Highly-accurate-gaze-estimation-using-a-consumer-rgb-d-sensor-”-in-Proceedings-of-the-Twenty-Fifth-International-Joint-Conference-on-Artificial-Intelligence-ser-IJCAI16-AAAI-Press-2016-p-33683374"><a href="#11-R-S-Ghiass-and-O-Arandjelovic-“Highly-accurate-gaze-estimation-using-a-consumer-rgb-d-sensor-”-in-Proceedings-of-the-Twenty-Fifth-International-Joint-Conference-on-Artificial-Intelligence-ser-IJCAI16-AAAI-Press-2016-p-33683374" class="headerlink" title="[11] R. S. Ghiass and O. Arandjelovic, “Highly accurate gaze estimation using a consumer rgb-d sensor,” in Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, ser. IJCAI16. AAAI Press, 2016, p. 33683374."></a>[11] R. S. Ghiass and O. Arandjelovic, “Highly accurate gaze estimation using a consumer rgb-d sensor,” in Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, ser. IJCAI16. AAAI Press, 2016, p. 33683374.</h3><p>Reza Shoja和Ognjen[11]提出了一个准确的头部姿势估计。采用基于人的变形模型，采用迭代最近点算法在深度空间中估计头部姿态。然后，通过变换和重渲染得到合成的人脸。最后，从合成图像的外观特征中回归凝视方向。该方法在被试头部运动时的平均误差为8.9°</p>
<h3 id="12-L-A-Jeni-and-J-F-Cohn-“Person-independent-3d-gaze-estimation-using-face-frontalization-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-Workshops-CVPRW-Los-Alamitos-CA-USA-IEEE-Computer-Society-jul-2016-pp-792–800"><a href="#12-L-A-Jeni-and-J-F-Cohn-“Person-independent-3d-gaze-estimation-using-face-frontalization-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-Workshops-CVPRW-Los-Alamitos-CA-USA-IEEE-Computer-Society-jul-2016-pp-792–800" class="headerlink" title="[12] L. A. Jeni and J. F. Cohn, “Person-independent 3d gaze estimation using face frontalization,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Los Alamitos, CA, USA: IEEE Computer Society, jul 2016, pp. 792–800."></a>[12] L. A. Jeni and J. F. Cohn, “Person-independent 3d gaze estimation using face frontalization,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Los Alamitos, CA, USA: IEEE Computer Society, jul 2016, pp. 792–800.</h3><p>检测人脸上<strong>密集的标记，并使用级联回归进行二维到三维的桥接。</strong>然后，通过求解优化问题进行三维面重建。在人脸正面化后，从6个眼睛计数器标记和1个瞳孔标记中提取二值特征，训练线性支持向量回归器，将注视特征映射到注视方向。在UT-Multiview数据集上的平均误差为4.2867°。</p>
<h2 id="特征融合"><a href="#特征融合" class="headerlink" title="特征融合"></a>特征融合</h2><p>特征融合通过考虑多模型特征来提高特征表示能力，从而提高分类或回归的性能。因此，<strong>头部姿态向量与注视特征的融合</strong>也是对抗头部姿态方差的有效方法。</p>
<p>采用了简单的连续融合凝视特征和头部姿态的方案，简单明了。</p>
<h3 id="13-R-Jafari-and-D-Ziou-“Gaze-estimation-using-kinect-ptz-camera-”-in-2012-IEEE-International-Symposium-on-Robotic-and-Sensors-Environments-Proceedings-2012-pp-13–18-14-R-Jafari-and-D-Ziou-“Eye-gaze-estimation-under-various-head-positions-and-iris-states-”-Expert-Systems-with-Applications-vol-42-no-1-pp-510–518-2015"><a href="#13-R-Jafari-and-D-Ziou-“Gaze-estimation-using-kinect-ptz-camera-”-in-2012-IEEE-International-Symposium-on-Robotic-and-Sensors-Environments-Proceedings-2012-pp-13–18-14-R-Jafari-and-D-Ziou-“Eye-gaze-estimation-under-various-head-positions-and-iris-states-”-Expert-Systems-with-Applications-vol-42-no-1-pp-510–518-2015" class="headerlink" title="[13] R. Jafari and D. Ziou, “Gaze estimation using kinect/ptz camera,” in 2012 IEEE International Symposium on Robotic and Sensors Environments Proceedings, 2012, pp. 13–18. [14] R. Jafari and D. Ziou, “Eye-gaze estimation under various head positions and iris states,” Expert Systems with Applications, vol. 42, no. 1, pp. 510–518, 2015"></a>[13] R. Jafari and D. Ziou, “Gaze estimation using kinect/ptz camera,” in 2012 IEEE International Symposium on Robotic and Sensors Environments Proceedings, 2012, pp. 13–18. [14] R. Jafari and D. Ziou, “Eye-gaze estimation under various head positions and iris states,” Expert Systems with Applications, vol. 42, no. 1, pp. 510–518, 2015</h3><p>Jafari和Ziou使用<strong>kinect估计头部姿势</strong>，并使用主动高清摄像机捕捉眼睛区域[13,14]。然后，从眼睛区域提取<strong>虹膜位移矢量和参考点</strong>。最后，<strong>结合虹膜位移向量、头部位置和头部姿态</strong>，采用变分贝叶斯多项式logistic回归估计凝视方向。</p>
<h3 id="15-X-Zhang-Y-Sugano-M-Fritz-and-A-Bulling-“Appearance-based-gaze-estimation-in-the-wild-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-CVPR-2015-pp-4511–4520"><a href="#15-X-Zhang-Y-Sugano-M-Fritz-and-A-Bulling-“Appearance-based-gaze-estimation-in-the-wild-”-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-CVPR-2015-pp-4511–4520" class="headerlink" title="[15] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Appearance-based gaze estimation in the wild,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 4511–4520"></a>[15] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Appearance-based gaze estimation in the wild,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 4511–4520</h3><p>Zhang等人[15]提出利用CNN从眼睛图像中提取凝视表征。具体来说，它首先通过旋转和平移相机到眼睛中心的眼睛区域。然后，将归一化后的眼睛图像和头部姿态矢量输入Lenet模型，利用眼睛姿态矢量和头部姿态矢量进行回归分析。该方法在MPIIGaze数据集上的平均误差为6.3°。</p>
<h3 id="16-X-Zhang-Y-Sugano-M-Fritz-and-A-Bulling-“Mpiigaze-Real-world-dataset-and-deep-appearance-based-gaze-estimation-”-IEEE-Trans-Pattern-Anal-Mach-Intell-vol-PP-no-99-pp-1–1-2017"><a href="#16-X-Zhang-Y-Sugano-M-Fritz-and-A-Bulling-“Mpiigaze-Real-world-dataset-and-deep-appearance-based-gaze-estimation-”-IEEE-Trans-Pattern-Anal-Mach-Intell-vol-PP-no-99-pp-1–1-2017" class="headerlink" title="[16] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Mpiigaze: Real-world dataset and deep appearance-based gaze estimation,” IEEE Trans Pattern Anal Mach Intell, vol. PP, no. 99, pp. 1–1, 2017."></a>[16] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Mpiigaze: Real-world dataset and deep appearance-based gaze estimation,” IEEE Trans Pattern Anal Mach Intell, vol. PP, no. 99, pp. 1–1, 2017.</h3><p>采用VGG-16模型代替Lenet模型，[16]进一步提高了性能，平均误差为5.5°。在MPIIGaze数据集中，右眼水平翻转。</p>
<h3 id="18-J-Lemley-A-Kar-A-Drimbarean-and-P-Corcoran-“Convolutional-neural-network-implementation-for-eyegaze-estimation-on-low-quality-consumer-imaging-systems-”-IEEE-Transactions-on-Consumer-Electronics-vol-65-no-2-pp-179–187-2019"><a href="#18-J-Lemley-A-Kar-A-Drimbarean-and-P-Corcoran-“Convolutional-neural-network-implementation-for-eyegaze-estimation-on-low-quality-consumer-imaging-systems-”-IEEE-Transactions-on-Consumer-Electronics-vol-65-no-2-pp-179–187-2019" class="headerlink" title="[18] J. Lemley, A. Kar, A. Drimbarean, and P. Corcoran, “Convolutional neural network implementation for eyegaze estimation on low-quality consumer imaging systems,” IEEE Transactions on Consumer Electronics, vol. 65, no. 2, pp. 179–187, 2019."></a>[18] J. Lemley, A. Kar, A. Drimbarean, and P. Corcoran, “Convolutional neural network implementation for eyegaze estimation on low-quality consumer imaging systems,” IEEE Transactions on Consumer Electronics, vol. 65, no. 2, pp. 179–187, 2019.</h3><p>然而，Lemley等人[18]证明，将两只眼睛都视为地面真理而不是将一只眼睛翻转到另一只眼睛，可以提高性能。此外，通过将5x5卷积替换为3x3卷积，并使用relu激活以及融合眼睛图像和头部姿态向量作为CNN的输入，进一步将MPIIGaze数据集上的注视估计平均误差提高到3.65°。</p>
<h3 id="17-H-Sun-C-Yang-and-S-Lai-“A-deep-learning-approach-to-appearance-based-gaze-estimation-under-head-pose-variations-”-in-2017-4th-IAPR-Asian-Conference-on-Pattern-Recognition-ACPR-2017-pp-935–940"><a href="#17-H-Sun-C-Yang-and-S-Lai-“A-deep-learning-approach-to-appearance-based-gaze-estimation-under-head-pose-variations-”-in-2017-4th-IAPR-Asian-Conference-on-Pattern-Recognition-ACPR-2017-pp-935–940" class="headerlink" title="[17] H. Sun, C. Yang, and S. Lai, “A deep learning approach to appearance-based gaze estimation under head pose variations,” in 2017 4th IAPR Asian Conference on Pattern Recognition (ACPR), 2017, pp. 935–940."></a>[17] H. Sun, C. Yang, and S. Lai, “A deep learning approach to appearance-based gaze estimation under head pose variations,” in 2017 4th IAPR Asian Conference on Pattern Recognition (ACPR), 2017, pp. 935–940.</h3><p>Sun等人[17]提出了一种多流CNN模型，该模型由4个并行的VGG模型组成，分别从眼睛和面部区域提取凝视特征和头部姿态。最终的凝视输出是所有流的平均值。结果是UT-Multiview数据集上的平均误差为6°。</p>
<h3 id="19-D-Lian-Z-Zhang-W-Luo-L-Hu-M-Wu-Z-Li-J-Yu-and-S-Gao-“Rgbd-based-gaze-estimation-via-multitask-cnn-”-in-Proceedings-of-the-AAAI-Conference-on-Artificial-Intelligence-vol-33-no-01-2019-pp-2488–-2495"><a href="#19-D-Lian-Z-Zhang-W-Luo-L-Hu-M-Wu-Z-Li-J-Yu-and-S-Gao-“Rgbd-based-gaze-estimation-via-multitask-cnn-”-in-Proceedings-of-the-AAAI-Conference-on-Artificial-Intelligence-vol-33-no-01-2019-pp-2488–-2495" class="headerlink" title="[19] D. Lian, Z. Zhang, W. Luo, L. Hu, M. Wu, Z. Li, J. Yu, and S. Gao, “Rgbd based gaze estimation via multitask cnn,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 2488– 2495."></a>[19] D. Lian, Z. Zhang, W. Luo, L. Hu, M. Wu, Z. Li, J. Yu, and S. Gao, “Rgbd based gaze estimation via multitask cnn,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 2488– 2495.</h3><p>Lian等人[19]提出了一种基于多流多任务的CNN模型。该模型从RGB图像和经过GAN精炼的深度图中，从裁剪过的眼睛图像和全人脸的头部姿态信息中学习凝视特征。然后，学习到的头部姿势信息与凝视特征连接在一起。在EYEDIAP和ShanghaiTechGaze数据集上的平均误差分别为4.8°和38.7 mm。</p>
<h2 id="几何特性的方法"><a href="#几何特性的方法" class="headerlink" title="几何特性的方法"></a>几何特性的方法</h2><blockquote>
<p>使用眼球模型的几何知识和不同坐标系之间的转换来补偿头部运动引起的偏差的研究。</p>
</blockquote>
<p><img src="/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211104112608127.png" alt="image-20211104112608127"></p>
<p><img src="/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211104112700336.png" alt="image-20211104112700336"></p>
<p><img src="/2021/11/03/gaze%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0/Users\iszha\AppData\Roaming\Typora\typora-user-images\image-20211104112735277.png" alt="image-20211104112735277"></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" rel="tag"># 文献阅读</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2021/10/31/gaze%E5%A4%A7%E4%BD%AC%E4%B8%BB%E9%A1%B5/" rel="next" title="gaze大佬主页">
                <i class="fa fa-chevron-left"></i> gaze大佬主页
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2021/11/07/Multiview-Multitask-Gaze-Estimation-With-Deep-Convolutional-Neural-Networks/" rel="prev" title="Multiview Multitask Gaze Estimation With Deep Convolutional Neural Networks">
                Multiview Multitask Gaze Estimation With Deep Convolutional Neural Networks <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">
		  
		    
              <div class="site-state-item site-state-posts">
                <a href="/archives">
                  <span class="site-state-item-count">27</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
		    


       

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/iszff" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9C%BC%E5%8A%A8%E8%B7%9F%E8%B8%AA%E6%8A%80%E6%9C%AF"><span class="nav-number">1.</span> <span class="nav-text">眼动跟踪技术</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%A2%E5%A4%96%E7%9C%BC%E5%9B%BE-IOG"><span class="nav-number">1.1.</span> <span class="nav-text">红外眼图(IOG)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Electrooculography"><span class="nav-number">1.2.</span> <span class="nav-text">Electrooculography</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Video-Oculography"><span class="nav-number">1.3.</span> <span class="nav-text">Video Oculography</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9C%BC%E7%90%83%E8%BF%BD%E8%B8%AA"><span class="nav-number">2.</span> <span class="nav-text">使用机器学习的眼球追踪</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9C%BC%E5%8A%A8%E8%BF%BD%E8%B8%AA%E6%8A%80%E6%9C%AF%E7%9A%84%E6%96%B9%E6%B3%95-%E7%9F%A5%E4%B9%8E-zhihu-com"><span class="nav-number">3.</span> <span class="nav-text">眼动追踪技术的方法 - 知乎 (zhihu.com)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9D%9E%E4%BE%B5%E5%85%A5%E5%BC%8F%E7%9C%BC%E5%8A%A8%E6%8A%80%E6%9C%AF%E4%B8%BB%E8%A6%81%E9%87%87%E7%94%A8%E7%9A%84%E8%BF%BD%E8%B8%AA%E6%96%B9%E6%B3%95%E4%B8%BB%E8%A6%81%E6%9C%891-%E5%B7%A9%E8%86%9C%E4%B8%80%E8%99%B9%E8%86%9C%E8%BE%B9%E7%BC%98%E6%B3%95%E3%80%812-%E7%9E%B3%E5%AD%94%E8%BF%BD%E8%B8%AA%E6%96%B9%E6%B3%95%E3%80%813-%E7%9E%B3%E5%AD%94%E4%B8%80%E8%A7%92%E8%86%9C%E5%8F%8D%E5%B0%84%E6%B3%95%E3%80%82"><span class="nav-number">3.1.</span> <span class="nav-text">非侵入式眼动技术主要采用的追踪方法主要有1.巩膜一虹膜边缘法、2.瞳孔追踪方法、3.瞳孔一角膜反射法。</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%B7%A9%E8%86%9C%E4%B8%80%E8%99%B9%E8%86%9C%E8%BE%B9%E7%BC%98%E6%B3%95"><span class="nav-number">3.1.1.</span> <span class="nav-text">1.巩膜一虹膜边缘法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%9E%B3%E5%AD%94%E8%BF%BD%E8%B8%AA%E6%96%B9%E6%B3%95"><span class="nav-number">3.1.2.</span> <span class="nav-text">2.瞳孔追踪方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E7%9E%B3%E5%AD%94-%E8%A7%92%E8%86%9C%E5%8F%8D%E5%B0%84%E8%BF%BD%E8%B8%AA%E6%96%B9%E6%B3%95"><span class="nav-number">3.1.3.</span> <span class="nav-text">3.瞳孔-角膜反射追踪方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B1%80%E9%99%90%E6%80%A7"><span class="nav-number">3.2.</span> <span class="nav-text">局限性</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Vision-based-Gaze-Estimation-A-Review"><span class="nav-number">4.</span> <span class="nav-text">Vision-based Gaze Estimation: A Review</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8C%91%E6%88%98"><span class="nav-number">4.1.</span> <span class="nav-text">挑战</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%81%9A%E7%B1%BB"><span class="nav-number">4.2.</span> <span class="nav-text">聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Y-Sugano-Y-Matsushita-and-Y-Sato-%E2%80%9CLearning-bysynthesis-for-appearance-based-3d-gaze-estimation-%E2%80%9D-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-CVPR-2014-pp-1821%E2%80%93-1828"><span class="nav-number">4.2.1.</span> <span class="nav-text">[5] Y. Sugano, Y. Matsushita, and Y. Sato, “Learning-bysynthesis for appearance-based 3d gaze estimation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 1821– 1828.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E7%B1%BB%E4%BC%BC%E7%9A%84%E8%81%9A%E7%B1%BB%E6%96%B9%E6%A1%88%E4%B9%9F%E7%94%A8%E4%BA%8E%E5%A4%84%E7%90%86%E5%A4%B4%E9%83%A8%E5%A7%BF%E6%80%81%E7%9A%84%E6%96%B9%E5%B7%AE%E3%80%82"><span class="nav-number">4.2.2.</span> <span class="nav-text">[6]类似的聚类方案也用于处理头部姿态的方差。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-R-Ranjan-S-D-Mello-and-J-Kautz-%E2%80%9CLight-weight-head-pose-invariant-gaze-tracking-%E2%80%9D-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-Workshops-CVPRW-Los-Alamitos-CA-USA-IEEE-Computer-Society-jun-2018-pp-2237%E2%80%93-22-378"><span class="nav-number">4.2.3.</span> <span class="nav-text">[7] R. Ranjan, S. D. Mello, and J. Kautz, “Light-weight head pose invariant gaze tracking,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Los Alamitos, CA, USA: IEEE Computer Society, jun 2018, pp. 2237– 22 378.</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%BA%E8%84%B8%E6%AD%A3%E9%9D%A2%E5%8C%96"><span class="nav-number">4.3.</span> <span class="nav-text">人脸正面化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-Q-Shi-S-Nobuhara-and-T-Matsuyama-%E2%80%9C3d-face-reconstruction-and-gaze-estimation-from-multi-view-video-using-symmetry-prior-%E2%80%9D-Ipsj-Transactions-on-Computer-Vision-and-Applications-vol-4-pp-149%E2%80%93160-2012"><span class="nav-number">4.3.1.</span> <span class="nav-text">[8] Q. Shi, S. Nobuhara, and T. Matsuyama, “3d face reconstruction and gaze estimation from multi-view video using symmetry prior,” Ipsj Transactions on Computer Vision and Applications, vol. 4, pp. 149–160, 2012.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-K-A-Funes-Mora-and-J-Odobez-%E2%80%9CGaze-estimation-from-multimodal-kinect-data-%E2%80%9D-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-Workshops-CVPRW-2012-pp-25%E2%80%9330-10-K-A-Funes-Mora-and-J-M-Odobez-%E2%80%9CGaze-estimation-in-the-3d-space-using-rgb-d-sensors-%E2%80%9D-International-Journal-of-Computer-Vision-vol-118-no-2-pp-194%E2%80%93-216-2016"><span class="nav-number">4.3.2.</span> <span class="nav-text">[9] K. A. Funes Mora and J. Odobez, “Gaze estimation from multimodal kinect data,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2012, pp. 25–30. [10] K. A. Funes-Mora and J. M. Odobez, “Gaze estimation in the 3d space using rgb-d sensors,” International Journal of Computer Vision, vol. 118, no. 2, pp. 194– 216, 2016.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-R-S-Ghiass-and-O-Arandjelovic-%E2%80%9CHighly-accurate-gaze-estimation-using-a-consumer-rgb-d-sensor-%E2%80%9D-in-Proceedings-of-the-Twenty-Fifth-International-Joint-Conference-on-Artificial-Intelligence-ser-IJCAI16-AAAI-Press-2016-p-33683374"><span class="nav-number">4.3.3.</span> <span class="nav-text">[11] R. S. Ghiass and O. Arandjelovic, “Highly accurate gaze estimation using a consumer rgb-d sensor,” in Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, ser. IJCAI16. AAAI Press, 2016, p. 33683374.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-L-A-Jeni-and-J-F-Cohn-%E2%80%9CPerson-independent-3d-gaze-estimation-using-face-frontalization-%E2%80%9D-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-Workshops-CVPRW-Los-Alamitos-CA-USA-IEEE-Computer-Society-jul-2016-pp-792%E2%80%93800"><span class="nav-number">4.3.4.</span> <span class="nav-text">[12] L. A. Jeni and J. F. Cohn, “Person-independent 3d gaze estimation using face frontalization,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Los Alamitos, CA, USA: IEEE Computer Society, jul 2016, pp. 792–800.</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88"><span class="nav-number">4.4.</span> <span class="nav-text">特征融合</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#13-R-Jafari-and-D-Ziou-%E2%80%9CGaze-estimation-using-kinect-ptz-camera-%E2%80%9D-in-2012-IEEE-International-Symposium-on-Robotic-and-Sensors-Environments-Proceedings-2012-pp-13%E2%80%9318-14-R-Jafari-and-D-Ziou-%E2%80%9CEye-gaze-estimation-under-various-head-positions-and-iris-states-%E2%80%9D-Expert-Systems-with-Applications-vol-42-no-1-pp-510%E2%80%93518-2015"><span class="nav-number">4.4.1.</span> <span class="nav-text">[13] R. Jafari and D. Ziou, “Gaze estimation using kinect&#x2F;ptz camera,” in 2012 IEEE International Symposium on Robotic and Sensors Environments Proceedings, 2012, pp. 13–18. [14] R. Jafari and D. Ziou, “Eye-gaze estimation under various head positions and iris states,” Expert Systems with Applications, vol. 42, no. 1, pp. 510–518, 2015</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-X-Zhang-Y-Sugano-M-Fritz-and-A-Bulling-%E2%80%9CAppearance-based-gaze-estimation-in-the-wild-%E2%80%9D-in-Proceedings-of-the-IEEE-Conference-on-Computer-Vision-and-Pattern-Recognition-CVPR-2015-pp-4511%E2%80%934520"><span class="nav-number">4.4.2.</span> <span class="nav-text">[15] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Appearance-based gaze estimation in the wild,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 4511–4520</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-X-Zhang-Y-Sugano-M-Fritz-and-A-Bulling-%E2%80%9CMpiigaze-Real-world-dataset-and-deep-appearance-based-gaze-estimation-%E2%80%9D-IEEE-Trans-Pattern-Anal-Mach-Intell-vol-PP-no-99-pp-1%E2%80%931-2017"><span class="nav-number">4.4.3.</span> <span class="nav-text">[16] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Mpiigaze: Real-world dataset and deep appearance-based gaze estimation,” IEEE Trans Pattern Anal Mach Intell, vol. PP, no. 99, pp. 1–1, 2017.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-J-Lemley-A-Kar-A-Drimbarean-and-P-Corcoran-%E2%80%9CConvolutional-neural-network-implementation-for-eyegaze-estimation-on-low-quality-consumer-imaging-systems-%E2%80%9D-IEEE-Transactions-on-Consumer-Electronics-vol-65-no-2-pp-179%E2%80%93187-2019"><span class="nav-number">4.4.4.</span> <span class="nav-text">[18] J. Lemley, A. Kar, A. Drimbarean, and P. Corcoran, “Convolutional neural network implementation for eyegaze estimation on low-quality consumer imaging systems,” IEEE Transactions on Consumer Electronics, vol. 65, no. 2, pp. 179–187, 2019.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-H-Sun-C-Yang-and-S-Lai-%E2%80%9CA-deep-learning-approach-to-appearance-based-gaze-estimation-under-head-pose-variations-%E2%80%9D-in-2017-4th-IAPR-Asian-Conference-on-Pattern-Recognition-ACPR-2017-pp-935%E2%80%93940"><span class="nav-number">4.4.5.</span> <span class="nav-text">[17] H. Sun, C. Yang, and S. Lai, “A deep learning approach to appearance-based gaze estimation under head pose variations,” in 2017 4th IAPR Asian Conference on Pattern Recognition (ACPR), 2017, pp. 935–940.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#19-D-Lian-Z-Zhang-W-Luo-L-Hu-M-Wu-Z-Li-J-Yu-and-S-Gao-%E2%80%9CRgbd-based-gaze-estimation-via-multitask-cnn-%E2%80%9D-in-Proceedings-of-the-AAAI-Conference-on-Artificial-Intelligence-vol-33-no-01-2019-pp-2488%E2%80%93-2495"><span class="nav-number">4.4.6.</span> <span class="nav-text">[19] D. Lian, Z. Zhang, W. Luo, L. Hu, M. Wu, Z. Li, J. Yu, and S. Gao, “Rgbd based gaze estimation via multitask cnn,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 2488– 2495.</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%87%A0%E4%BD%95%E7%89%B9%E6%80%A7%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">4.5.</span> <span class="nav-text">几何特性的方法</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">iszff</span>

  
 </div>



  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>





        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.json";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.4"></script>



  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
